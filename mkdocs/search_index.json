{
    "docs": [
        {
            "location": "/", 
            "text": "A real-time query engine for very large data streams\n\n\n\n\n\n\nNO persistence layer\n\n\n\n\n\n\nLight-weight, cheap and fast\n\n\n\n\n\n\nMulti-tenant\n\n\n\n\n\n\nPluggable to any data source\n\n\n\n\n\n\nProvides a UI and Web Service\n\n\n\n\n\n\nFilter raw data or aggregate data\n\n\n\n\n\n\nCan be run on Storm or Spark Streaming\n\n\n\n\n\n\nA look-forward query system - operates on data that arrive after the query is submitted\n\n\n\n\n\n\nBig-data scale-tested - used in production at Yahoo and tested running 500+ queries simultaneously on up to 2,000,000 rps\n\n\n\n\n\n\nHow is Bullet useful\n\n\nHow Bullet is used is largely determined by the data source it consumes. Depending on what kind of data you put Bullet on, the types of queries you run on it and your use-cases will change. As a look-forward query system with no persistence, you will not be able to repeat your queries on the same data. The next time you run your query, it will operate on the different data that arrives after that submission. If this usage pattern is what you need and you are looking for a light-weight system that can tap into your streaming data, then Bullet is for you!\n\n\nExample: How Bullet is used at Yahoo\n\n\nBullet is used in production internally at Yahoo by having it sit on a subset of raw user engagement events from Yahoo sites and apps. This lets Yahoo developers automatically validate their instrumentation code \nend-to-end\n in their Continuous Delivery pipelines. Validating instrumentation is critical since it powers pretty much all decisions and products including machine learning, corporate KPIs, analytics, personalization, targeting.\n\n\nThis instance of Bullet also powers other use-cases such as letting analysts validate assumptions about data, product managers verify launches instantly, debug issues and outages, or simply explore and play around with the data.\n\n\n\n\nBlog post\n\n\nHere is a \nlink to our blog post\n condensing most of this information if you want to take a look.\n\n\n\n\n\n\nQuick Start\n\n\nSee \nQuick Start\n to set up Bullet locally using Spark Streaming. You will generate some synthetic streaming data that you can then query with Bullet.\n\n\nSetup Bullet on your streaming data\n\n\nTo set up Bullet on a real data stream, you need:\n\n\n\n\nTo setup the Bullet Backend on a stream processing framework. Currently, we support \nBullet on Storm\n and \nBullet on Spark\n.\n\n\nPlug in your source of data. See \nData Ingestion\n or the \nDSL\n for details\n\n\nConsume your data stream\n\n\n\n\n\n\nThe \nWeb Service\n set up to convey queries and return results back from the backend\n\n\nTo choose a \nPubSub implementation\n that connects the Web Service and the Backend. We currently support \nKafka\n and a \nREST PubSub\n on any Backend and \nStorm DRPC\n for the Storm Backend.\n\n\nThe optional \nUI\n set up to talk to your Web Service. You can skip the UI if all your access is programmatic\n\n\n\n\n\n\nSchema in the UI\n\n\nThe UI also needs an endpoint that provides your data schema to help with query building. The Web Service you set up provides a simple file based schema endpoint that you can point the UI to if that is sufficient for your needs.\n\n\n\n\n\n\nQuerying in Bullet\n\n\nBullet queries allow you to filter, project and aggregate data. You can also specify a window to get incremental results. We support basically first-order SQL (no joins or nested queries). Bullet lets you fetch raw (the individual data records) as well as aggregated data.\n\n\n\n\n\n\nSee the \nUI Usage section\n for using the UI to build Bullet queries. This is the same UI you will build in the Quick Starts.\n\n\n\n\n\n\nSee the API section (\nAPI\n for building Bullet API queries\n\n\n\n\n\n\nFor examples using the API, see \nExamples\n. These are actual albeit cleansed queries sourced from the instance at Yahoo.\n\n\n\n\n\n\nTermination conditions\n\n\nA Bullet query terminates and returns whatever has been collected so far when:\n\n\n\n\nA maximum duration is reached. In other words, a query runs for a defined time window (which can be infinite).\n\n\nA maximum number of records is reached (only applicable for queries that are fetching raw data records and not aggregating).\n\n\n\n\nWindowing\n\n\nWindows in a Bullet query allow you to specify how often you'd like Bullet to return results.\n\n\nFor example, you could launch a query for 2 minutes, and have Bullet return a COUNT DISTINCT on a particular field every 3 seconds:\n\n\n\n\nSee documentation on the Web Service API for more info.\n\n\nResults\n\n\nThe Bullet Web Service returns your query result as well as associated metadata information in a structured JSON format. The UI can display the results in different formats.\n\n\n\n\nApproximate computation\n\n\nIt is often intractable to perform aggregations on an unbounded stream of data and still support arbitrary queries. However, it is possible if an exact answer is not required and the approximate answer's error is exactly quantifiable. There are stochastic algorithms and data structures that let us do this. We use \nData Sketches\n to perform aggregations such as counting uniques, and will be using Sketches to implement some future aggregations.\n\n\nSketches let us be exact in our computation up to configured thresholds and approximate after. The error is very controllable and quantifiable. All Bullet queries that use Sketches return the error bounds with Standard Deviations as part of the results so you can quantify the error exactly. Using Sketches lets us address otherwise hard to solve problems in sub-linear space. We uses Sketches to compute \nCOUNT DISTINCT\n, \nGROUP\n, \nDISTRIBUTION\n and \nTOP K\n queries.\n\n\nWe also use Sketches as a way to control high cardinality grouping (group by a natural key column or related) and rely on the Sketching data structure to drop excess groups. It is up to you setting up Bullet to determine to set Sketch sizes large or small enough for to satisfy the queries that will be performed on that instance of Bullet.\n\n\nArchitecture\n\n\nEnd-to-End Architecture\n\n\n\n\nThe image above shows how the various pieces of the Bullet interact at a high-level. All these layers are modular and pluggable. Some, like the persistence layer for queries, are optional. You can choose an implementation for the Backend and the PubSub (or create your own). The core of Bullet is abstracted into a \nlibrary\n that can be reused to implement the Backend, Web Service and PubSub layers in a platform agnostic manner.\n\n\n\n\nBackend\n\n\n\n\nThe Bullet Backend can be split into three main conceptual sub-systems:\n\n\n\n\nRequest Processor - receives queries, adds metadata and sends it to the rest of the system\n\n\nData Processor - reads data from a input stream, converts it to an unified data format and matches it against queries\n\n\nCombiner - combines results for different queries, performs final aggregations and returns results\n\n\n\n\nThe core of Bullet querying is not tied to the Backend and lives in a core library. This allows you implement the flow shown above in any stream processor you like.\n\n\nImplementations of \nBullet on Storm\n and \nBullet on Spark\n are currently supported.\n\n\nPubSub\n\n\nThe PubSub is responsible for transmitting queries from the API to the Backend and returning results back from the Backend to the clients. It decouples whatever particular Backend you are using with the API.\nWe currently support four different PubSub implementations:\n\n\n\n\nKafka\n\n\nREST\n\n\nPulsar\n\n\nStorm DRPC\n (only for non-windowed queries)\n\n\n\n\nYou can also very easily \nimplement your own\n by defining a few interfaces that we provide.\n\n\nWeb Service and UI\n\n\nThe rest of the pieces are just the standard other two pieces in a full-stack application:\n\n\n\n\nA Web Service that talks to the backend using the PubSub layer\n\n\nA UI that talks to this Web Service\n\n\n\n\nThe \nBullet Web Service\n is built using \nSpring Boot\n in Java and the \nUI\n is built in \nEmber\n.\n\n\nThe Web Service can be deployed as a standalone Java application (a JAR file) or easily rebuilt as a WAR to deploy your favorite servlet container like \nJetty\n. The UI is a client-side application that can be served using \nNode.js\n\n\n\n\nWant to know more?\n\n\nIn practice, the backend is implemented using the basic components that the Stream processing framework provides. See \nStorm Architecture\n and \nSpark Architecture\n for details.", 
            "title": "Home"
        }, 
        {
            "location": "/#how-is-bullet-useful", 
            "text": "How Bullet is used is largely determined by the data source it consumes. Depending on what kind of data you put Bullet on, the types of queries you run on it and your use-cases will change. As a look-forward query system with no persistence, you will not be able to repeat your queries on the same data. The next time you run your query, it will operate on the different data that arrives after that submission. If this usage pattern is what you need and you are looking for a light-weight system that can tap into your streaming data, then Bullet is for you!", 
            "title": "How is Bullet useful"
        }, 
        {
            "location": "/#example-how-bullet-is-used-at-yahoo", 
            "text": "Bullet is used in production internally at Yahoo by having it sit on a subset of raw user engagement events from Yahoo sites and apps. This lets Yahoo developers automatically validate their instrumentation code  end-to-end  in their Continuous Delivery pipelines. Validating instrumentation is critical since it powers pretty much all decisions and products including machine learning, corporate KPIs, analytics, personalization, targeting.  This instance of Bullet also powers other use-cases such as letting analysts validate assumptions about data, product managers verify launches instantly, debug issues and outages, or simply explore and play around with the data.   Blog post  Here is a  link to our blog post  condensing most of this information if you want to take a look.", 
            "title": "Example: How Bullet is used at Yahoo"
        }, 
        {
            "location": "/#quick-start", 
            "text": "See  Quick Start  to set up Bullet locally using Spark Streaming. You will generate some synthetic streaming data that you can then query with Bullet.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/#setup-bullet-on-your-streaming-data", 
            "text": "To set up Bullet on a real data stream, you need:   To setup the Bullet Backend on a stream processing framework. Currently, we support  Bullet on Storm  and  Bullet on Spark .  Plug in your source of data. See  Data Ingestion  or the  DSL  for details  Consume your data stream    The  Web Service  set up to convey queries and return results back from the backend  To choose a  PubSub implementation  that connects the Web Service and the Backend. We currently support  Kafka  and a  REST PubSub  on any Backend and  Storm DRPC  for the Storm Backend.  The optional  UI  set up to talk to your Web Service. You can skip the UI if all your access is programmatic    Schema in the UI  The UI also needs an endpoint that provides your data schema to help with query building. The Web Service you set up provides a simple file based schema endpoint that you can point the UI to if that is sufficient for your needs.", 
            "title": "Setup Bullet on your streaming data"
        }, 
        {
            "location": "/#querying-in-bullet", 
            "text": "Bullet queries allow you to filter, project and aggregate data. You can also specify a window to get incremental results. We support basically first-order SQL (no joins or nested queries). Bullet lets you fetch raw (the individual data records) as well as aggregated data.    See the  UI Usage section  for using the UI to build Bullet queries. This is the same UI you will build in the Quick Starts.    See the API section ( API  for building Bullet API queries    For examples using the API, see  Examples . These are actual albeit cleansed queries sourced from the instance at Yahoo.", 
            "title": "Querying in Bullet"
        }, 
        {
            "location": "/#termination-conditions", 
            "text": "A Bullet query terminates and returns whatever has been collected so far when:   A maximum duration is reached. In other words, a query runs for a defined time window (which can be infinite).  A maximum number of records is reached (only applicable for queries that are fetching raw data records and not aggregating).", 
            "title": "Termination conditions"
        }, 
        {
            "location": "/#windowing", 
            "text": "Windows in a Bullet query allow you to specify how often you'd like Bullet to return results.  For example, you could launch a query for 2 minutes, and have Bullet return a COUNT DISTINCT on a particular field every 3 seconds:   See documentation on the Web Service API for more info.", 
            "title": "Windowing"
        }, 
        {
            "location": "/#results", 
            "text": "The Bullet Web Service returns your query result as well as associated metadata information in a structured JSON format. The UI can display the results in different formats.", 
            "title": "Results"
        }, 
        {
            "location": "/#approximate-computation", 
            "text": "It is often intractable to perform aggregations on an unbounded stream of data and still support arbitrary queries. However, it is possible if an exact answer is not required and the approximate answer's error is exactly quantifiable. There are stochastic algorithms and data structures that let us do this. We use  Data Sketches  to perform aggregations such as counting uniques, and will be using Sketches to implement some future aggregations.  Sketches let us be exact in our computation up to configured thresholds and approximate after. The error is very controllable and quantifiable. All Bullet queries that use Sketches return the error bounds with Standard Deviations as part of the results so you can quantify the error exactly. Using Sketches lets us address otherwise hard to solve problems in sub-linear space. We uses Sketches to compute  COUNT DISTINCT ,  GROUP ,  DISTRIBUTION  and  TOP K  queries.  We also use Sketches as a way to control high cardinality grouping (group by a natural key column or related) and rely on the Sketching data structure to drop excess groups. It is up to you setting up Bullet to determine to set Sketch sizes large or small enough for to satisfy the queries that will be performed on that instance of Bullet.", 
            "title": "Approximate computation"
        }, 
        {
            "location": "/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/#end-to-end-architecture", 
            "text": "The image above shows how the various pieces of the Bullet interact at a high-level. All these layers are modular and pluggable. Some, like the persistence layer for queries, are optional. You can choose an implementation for the Backend and the PubSub (or create your own). The core of Bullet is abstracted into a  library  that can be reused to implement the Backend, Web Service and PubSub layers in a platform agnostic manner.", 
            "title": "End-to-End Architecture"
        }, 
        {
            "location": "/#backend", 
            "text": "The Bullet Backend can be split into three main conceptual sub-systems:   Request Processor - receives queries, adds metadata and sends it to the rest of the system  Data Processor - reads data from a input stream, converts it to an unified data format and matches it against queries  Combiner - combines results for different queries, performs final aggregations and returns results   The core of Bullet querying is not tied to the Backend and lives in a core library. This allows you implement the flow shown above in any stream processor you like.  Implementations of  Bullet on Storm  and  Bullet on Spark  are currently supported.", 
            "title": "Backend"
        }, 
        {
            "location": "/#pubsub", 
            "text": "The PubSub is responsible for transmitting queries from the API to the Backend and returning results back from the Backend to the clients. It decouples whatever particular Backend you are using with the API.\nWe currently support four different PubSub implementations:   Kafka  REST  Pulsar  Storm DRPC  (only for non-windowed queries)   You can also very easily  implement your own  by defining a few interfaces that we provide.", 
            "title": "PubSub"
        }, 
        {
            "location": "/#web-service-and-ui", 
            "text": "The rest of the pieces are just the standard other two pieces in a full-stack application:   A Web Service that talks to the backend using the PubSub layer  A UI that talks to this Web Service   The  Bullet Web Service  is built using  Spring Boot  in Java and the  UI  is built in  Ember .  The Web Service can be deployed as a standalone Java application (a JAR file) or easily rebuilt as a WAR to deploy your favorite servlet container like  Jetty . The UI is a client-side application that can be served using  Node.js   Want to know more?  In practice, the backend is implemented using the basic components that the Stream processing framework provides. See  Storm Architecture  and  Spark Architecture  for details.", 
            "title": "Web Service and UI"
        }, 
        {
            "location": "/quick-start/spark/", 
            "text": "Quick Start on Spark\n\n\nIn this section we will setup a mock instance of Bullet to play around with. We will use \nBullet Spark\n to run the backend of Bullet on the \nSpark\n framework. And we will use the \nBullet Kafka PubSub\n.\n\n\nAt the end of this section, you will have:\n\n\n\n\nLaunched the Bullet backend on Spark\n\n\nSetup the \nWeb Service\n\n\nSetup the \nUI\n to talk to the Web Service\n\n\n\n\nPrerequisites\n\n\n\n\nYou will need to be on an Unix-based system (Mac OS X, Ubuntu ...) with \ncurl\n installed\n\n\nYou will need \nJDK 8\n installed\n\n\n\n\nInstall Script\n\n\nSimply run:\n\n\ncurl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-spark.sh | bash\n\n\n\n\nThis will setup a local Spark and Kafka cluster, a Bullet running on it, the Bullet Web Service and a Bullet UI for you. Once everything has launched, you should be able to go to the Bullet UI running locally at \nhttp://localhost:8800\n. You can then \ncontinue this guide from here\n.\n\n\n\n\nWant to DIY?\n\n\nIf you want to manually run all the commands or if the script died while doing something above (might want to perform the \nteardown\n first), you can continue below.\n\n\n\n\nManual Installation\n\n\nStep 1: Setup directories and examples\n\n\nexport BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/spark\nmkdir -p $BULLET_HOME/pubsub\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME\ncurl -LO https://github.com/bullet-db/bullet-db.github.io/releases/download/v1.0.0/examples_artifacts.tar.gz\ntar -xzf examples_artifacts.tar.gz\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-examples\n\n\n\n\nSetup Kafka\n\n\nFor this instance of Bullet we will use the Kafka PubSub implementation found in \nbullet-spark\n. So we will first download and run Kafka, and setup a couple Kafka topics.\n\n\nStep 2: Download and Install Kafka\n\n\ncd $BULLET_HOME/pubsub\ncurl -Lo bullet-kafka.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-kafka/1.0.1/bullet-kafka-1.0.1-fat.jar\ncurl -LO https://archive.apache.org/dist/kafka/2.3.1/kafka_2.12-2.3.1.tgz\ntar -xzf kafka_2.12-2.3.1.tgz\nexport KAFKA_DIR=$BULLET_HOME/pubsub/kafka_2.12-2.3.1\n\n\n\n\nStep 3: Start Zookeeper\n\n\n$KAFKA_DIR/bin/zookeeper-server-start.sh $KAFKA_DIR/config/zookeeper.properties \n\n\n\n\n\nStep 4: Start Kafka\n\n\nGive Zookeeper ~5-10 seconds to start up, then start Kafka:\n\n\n$KAFKA_DIR/bin/kafka-server-start.sh $KAFKA_DIR/config/server.properties \n\n\n\n\n\nStep 5: Create Kafka Topics\n\n\nThe Bullet Kafka PubSub uses two topics. One to send messages from the Web Service to the Backend, and one to send messages from the Backend to the Web Service. So we will create a Kafka topic called \"bullet.requests\" and another called \"bullet.responses\".\n\n\n$KAFKA_DIR/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic bullet.requests\n$KAFKA_DIR/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic bullet.responses\n\n\n\n\nSetup Bullet Backend on Spark\n\n\nWe will run the bullet-spark backend using \nSpark 3.0.1\n.\n\n\nStep 6: Install Spark 3.0.1\n\n\nexport BULLET_SPARK=$BULLET_HOME/backend/spark\ncd $BULLET_SPARK\ncurl -O https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\ntar -xzf spark-3.0.1-bin-hadoop2.7.tgz\n\n\n\n\nStep 7: Setup Bullet-Spark and Example Data Producer\n\n\ncp $BULLET_HOME/bullet-examples/backend/spark/* $BULLET_SPARK\ncurl -Lo bullet-spark.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-spark/1.0.0/bullet-spark-1.0.0-standalone.jar\n\n\n\n\nStep 8: Launch the Bullet Spark Backend\n\n\nRun this multi-line command (new lines are escaped):\n\n\n$BULLET_SPARK/spark-3.0.1-bin-hadoop2.7/bin/spark-submit \\\n    --master local[10]  \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --jars $BULLET_HOME/pubsub/bullet-kafka.jar,$BULLET_SPARK/bullet-spark-example.jar \\\n    $BULLET_SPARK/bullet-spark.jar \\\n    --bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml \n log.txt \n\n\n\n\n\n\nThe Backend will usually be up and running usually within 5-10 seconds. Once it is running you can get information about the Spark job in the Spark UI, which can be seen in your browser at \nhttp://localhost:4040\n by default. The Web Service will now be hooked up through the Kafka PubSub to the Spark backend. To test it you can now run a Bullet query by hitting the Web Service directly:\n\n\nSetup Web Service\n\n\nStep 9: Install the Bullet Web Service\n\n\ncd $BULLET_HOME/service\ncurl -Lo bullet-service.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/1.0.0/bullet-service-1.0.0-embedded.jar\ncp $BULLET_EXAMPLES/web-service/example_kafka_pubsub_config.yaml $BULLET_HOME/service/\ncp $BULLET_EXAMPLES/web-service/example_query_config.yaml $BULLET_HOME/service/\ncp $BULLET_EXAMPLES/web-service/example_columns.json $BULLET_HOME/service/\n\n\n\n\nStep 10: Launch the Web Service\n\n\nRun this multi-line command (new lines are escaped):\n\n\njava -Dloader.path=$BULLET_HOME/pubsub/bullet-kafka.jar -jar bullet-service.jar \\\n    --bullet.pubsub.config=$BULLET_HOME/service/example_kafka_pubsub_config.yaml \\\n    --bullet.query.config=${BULLET_HOME}/service/example_query_config.yaml \\\n    --bullet.schema.file=$BULLET_HOME/service/example_columns.json \\\n    --server.port=9999  \\\n    --logging.path=. \\\n    --logging.file=log.txt \n log.txt \n\n\n\n\n\nStep 11: Test the Web Service (optional)\n\n\nWe can check that the Web Service is up and running by getting the example columns through the API:\n\n\ncurl -s http://localhost:9999/api/bullet/columns\n\n\n\n\ncurl -s -H 'Content-Type: text/plain' -X POST -d 'SELECT * FROM STREAM(2000, TIME) LIMIT 1;' http://localhost:9999/api/bullet/queries/sse-query\n\n\n\n\nThis query will return a result JSON containing a \"records\" field containing a single record, and a \"meta\" field with some meta information.\n\n\n\n\nWhat is this data?\n\n\nThis data is randomly generated by the \ncustom Spark Streaming Receiver\n that generates toy data to demo Bullet. In practice, your producer would read from an actual data source such as Kafka etc.\n\n\n\n\nYou can also check the status of the Web Service by looking at the Web Service log: $BULLET_HOME/service/log.txt\n\n\nSetting up the Bullet UI\n\n\nStep 12: Install Node\n\n\ncd $BULLET_HOME/ui\ncurl -s https://raw.githubusercontent.com/creationix/nvm/v0.37.2/install.sh | bash\nsource ~/.bashrc\nnvm install v10.20.1\nnvm use v10.20.1\n\n\n\n\nStep 13: Install the Bullet UI\n\n\ncurl -LO https://github.com/bullet-db/bullet-ui/releases/download/v1.0.1/bullet-ui-v1.0.1.tar.gz\ntar -xzf bullet-ui-v1.0.1.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/\n\n\n\n\nStep 14: Launch the UI\n\n\nPORT=8800 node express-server.js \n\n\n\n\n\nVisit \nhttp://localhost:8800\n to query your topology with the UI. See \nUI usage\n for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.\n\n\n\n\nRunning it remotely?\n\n\nIf you access the UI from another machine than where your UI is actually running, you will need to edit \nconfig/env-settings.json\n. Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change \nlocalhost\n in your env-settings.json to point to the host name where you will hosting the UI. This will be the same as the UI host you use in the browser. You can also do a local port forward on the machine accessing the UI by running: \nssh -N -L 8800:localhost:8800 -L 9999:localhost:9999 hostname-of-the-quickstart-components 2\n1\n.\n\n\n\n\nPlaying around with the instance\n\n\nCheck out and follow along with the \nUI Usage\n page as it shows you some queries you can run using this UI.\n\n\nTeardown\n\n\nWhen you are done trying out Bullet, you can stop the processes and cleanup using the instructions below.\n\n\nIf you were using the \nInstall Script\n or if you don't want to manually bring down everything, you can run:\n\n\ncurl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-spark.sh | bash -s cleanup\n\n\n\n\nIf you were performing the steps yourself, you can also manually cleanup \nall the components and all the downloads\n using:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUI\n\n\npkill -f [e]xpress-server.js\n\n\n\n\n\n\nWeb Service\n\n\npkill -f [e]xample_kafka_pubsub_config.yaml\n\n\n\n\n\n\nSpark\n\n\npkill -f [b]ullet-spark\n\n\n\n\n\n\nKafka\n\n\n${KAFKA_DIR}/bin/kafka-server-stop.sh\n\n\n\n\n\n\nZookeeper\n\n\n${KAFKA_DIR}/bin/zookeeper-server-stop.sh\n\n\n\n\n\n\nFile System\n\n\nrm -rf $BULLET_HOME /tmp/zookeeper /tmp/kafka-logs/ /tmp/spark-checkpoint\n\n\n\n\n\n\n\n\nNote: This does \nnot\n delete \n$HOME/.nvm\n.\n\n\nWhat did we do?\n\n\nThis section will go over the various custom pieces this example plugged into Bullet, so you can better understand what we did.\n\n\nSpark Streaming Job\n\n\nThe Spark Streaming application we ran was Bullet plugged in with a custom Receiver in our implementation of the Bullet Spark DataProducer trait. This Receiver and DataProducer are implemented in this \nexample project\n and was already built for you when you \ndownloaded the examples\n. It does not read from any data source and just produces random, structured data. It also produces only up to a maximum number of records in a given period. Both this maximum and the length of a period are configured in the Receiver (at most 100 every 1 second).\n\n\n$BULLET_SPARK/spark-3.0.1-bin-hadoop2.7/bin/spark-submit \\\n    --master local[10]  \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --jars $BULLET_HOME/pubsub/bullet-kafka.jar,$BULLET_SPARK/bullet-spark-example.jar \\\n    $BULLET_SPARK/bullet-spark.jar \\\n    --bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml \n log.txt \n\n\n\n\n\n\nWe launched the bullet-spark jar (an uber or \"fat\" jar) containing Bullet Spark and all its dependencies. We added our Pubsub (see below) implementation and our jar containing our custom Receiver to the Spark job's additional jars.\n\n\nThe settings defined by \n--bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml\n and the arguments here run all components in the Spark Streaming job.\n\n\n\n\nI thought you said hundreds of thousands of records...\n\n\n100 records per second is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components with CPU and memory configurations to accommodate for your data volume and querying needs.\n\n\n\n\nLet's look at the \ncustom Receiver code\n that generates the data.\n\n\n    private def receive(): Unit = {\n      nextIntervalStart = System.currentTimeMillis()\n      while (!isStopped) {\n        val timeNow = System.currentTimeMillis()\n        // Only emit if we are still in the interval and haven't gone over our per period max\n        if (timeNow \n= nextIntervalStart \n generatedThisPeriod \n maxPerPeriod) {\n          store(generateRecord())\n          generatedThisPeriod += 1\n        }\n        if (timeNow \n nextIntervalStart) {\n          logger.info(\nGenerated {} tuples out of {}\n, generatedThisPeriod, maxPerPeriod)\n          nextIntervalStart = timeNow + period\n          generatedThisPeriod = 0\n          periodCount += 1\n        }\n        // It is courteous to sleep for a short time.\n        try {\n          Thread.sleep(1)\n        } catch {\n          case e: InterruptedException =\n logger.error(\nError: \n, e)\n        }\n      }\n    }\n\n\n\n\nThis method above emits the data. This method is wrapped in a thread that is called by the Spark framework. This function only emits at most the given maximum tuples per period.\n\n\n    private def makeRandomMap: Map[java.lang.String, java.lang.String] = {\n      val randomMap = new HashMap[java.lang.String, java.lang.String](2)\n      randomMap.put(RandomReceiver.RANDOM_MAP_KEY_A, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n      randomMap.put(RandomReceiver.RANDOM_MAP_KEY_B, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n      randomMap\n    }\n\n    private def generateRecord(): BulletRecord[_ \n: java.io.Serializable] = {\n      val record = new TypedSimpleBulletRecord()\n      val uuid = UUID.randomUUID().toString\n      record.setString(RandomReceiver.STRING, uuid)\n      record.setLong(RandomReceiver.LONG, generatedThisPeriod)\n      record.setDouble(RandomReceiver.DOUBLE, Random.nextDouble())\n      record.setDouble(RandomReceiver.GAUSSIAN, Random.nextGaussian())\n      record.setString(RandomReceiver.TYPE, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n      record.setLong(RandomReceiver.DURATION, System.nanoTime() % RandomReceiver.INTEGER_POOL(Random.nextInt(RandomReceiver.INTEGER_POOL.length)))\n\n      // Don't use Scala Map and convert it by asJava when calling setxxxMap method in BulletRecord.\n      // It converts Scala Map to scala.collection.convert.Wrappers$MapWrapper which is not serializable in scala 2.11.x (https://issues.scala-lang.org/browse/SI-8911).\n\n      record.setStringMap(RandomReceiver.SUBTYPES_MAP, makeRandomMap);\n\n      val booleanMap = new HashMap[java.lang.String, java.lang.Boolean](4)\n      booleanMap.put(uuid.substring(0, 8), Random.nextBoolean())\n      booleanMap.put(uuid.substring(9, 13), Random.nextBoolean())\n      booleanMap.put(uuid.substring(14, 18), Random.nextBoolean())\n      booleanMap.put(uuid.substring(19, 23), Random.nextBoolean())\n      record.setBooleanMap(RandomReceiver.BOOLEAN_MAP, booleanMap)\n\n\n      val statsMap = new HashMap[java.lang.String, java.lang.Long](4)\n      statsMap.put(RandomReceiver.PERIOD_COUNT, periodCount)\n      statsMap.put(RandomReceiver.RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod)\n      statsMap.put(RandomReceiver.NANO_TIME, System.nanoTime())\n      statsMap.put(RandomReceiver.TIMESTAMP, System.nanoTime())\n      record.setLongMap(RandomReceiver.STATS_MAP, statsMap)\n\n      record.setListOfStringMap(RandomReceiver.LIST, asList(makeRandomMap, makeRandomMap))\n      record\n    }\n\n\n\n\nThis \ngenerateRecord\n method generates some fields randomly and inserts them into a BulletRecord (simple). Note that the BulletRecord is typed and all data must be inserted with the proper types.\n\n\nThis whole receiver is plugged into an implementation of the Spark DataProducer trait that Bullet Spark requires to plug in your data (as a Spark DStream) into it. You can find this class implemented \nhere\n and reproduced below.\n\n\npackage com.yahoo.bullet.spark.examples\n\nimport com.yahoo.bullet.record.BulletRecord\nimport com.yahoo.bullet.spark.DataProducer\nimport com.yahoo.bullet.spark.examples.receiver.RandomReceiver\nimport com.yahoo.bullet.spark.utils.BulletSparkConfig\nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.dstream.DStream\n\nclass RandomProducer extends DataProducer {\n  override def getBulletRecordStream(ssc: StreamingContext, config: BulletSparkConfig): DStream[BulletRecord] = {\n    // Bullet record input stream.\n    val bulletReceiver = new RandomReceiver(config)\n    ssc.receiverStream(bulletReceiver).asInstanceOf[DStream[BulletRecord]]\n  }\n}\n\n\n\n\nIf you put Bullet on your data, you will need to write a DataProducer (or a full on Spark DAG if your reading is complex), that reads from your data source and emits a DStream of BulletRecords with the fields you wish to be query-able similar to this example, or you can use \nBullet DSL to configure and plug in\n a DSL based receiver that uses the Bullet DSL's Connector -\n Serializer -\n Converter system to read and convert your dataset without having to write code!\n\n\nPubSub\n\n\nWe used the \nKafka PubSub\n. We configured the Backend to use this PubSub by adding these settings to the YAML file that we passed to our Spark Streaming job. Notice that we set the context to \nQUERY_PROCESSING\n since this is the Backend.\n\n\nbullet.pubsub.context.name: \nQUERY_PROCESSING\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.kafka.KafkaPubSub\n\nbullet.pubsub.kafka.bootstrap.servers: \nlocalhost:9092\n\nbullet.pubsub.kafka.request.topic.name: \nbullet.requests\n\nbullet.pubsub.kafka.response.topic.name: \nbullet.responses\n\n\n\n\n\nFor the Web Service, we passed in a YAML file that pointed to the same Kafka topics. Notice that we set the context to \nQUERY_SUBMISSION\n since this is the Web Service.\n\n\nbullet.pubsub.context.name: \nQUERY_SUBMISSION\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.kafka.KafkaPubSub\n\nbullet.pubsub.kafka.bootstrap.servers: \nlocalhost:9092\n\nbullet.pubsub.kafka.request.topic.name: \nbullet.requests\n\nbullet.pubsub.kafka.response.topic.name: \nbullet.responses\n\n\n\n\n\nWeb Service\n\n\nWe launched the Web Service using two custom files - a PubSub configuration YAML file and JSON schema file.\n\n\nThe JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.\n\n\nThe following is a snippet from the \nJSON file\n. Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using \nenumerations\n.\n\n\n[\n    {\n        \nname\n: \nprobability\n,\n        \ntype\n: \nDOUBLE\n,\n        \ndescription\n: \nGenerated from Random#nextDouble\n\n    },\n    ...\n    {\n        \nname\n: \nstats\n,\n        \ntype\n: \nMAP\n,\n        \nsubtype\n: \nLONG\n,\n        \ndescription\n: \nThis map contains some numeric information such as the current number of periods etc.\n,\n        \nenumerations\n: [\n            ...\n            {\nname\n: \nnano_time\n, \ndescription\n: \nThe ns time when this record was generated\n}\n        ]\n    },\n    {\n        \nname\n: \nclassifiers\n,\n        \ntype\n: \nLIST\n,\n        \nsubtype\n: \nMAP\n,\n        \ndescription\n: \nThis contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf\n\n    }\n]\n\n\n\n\nThe contents of the \nPubSub configuration file\n was discussed in the \nPubSub section above\n.\n\n\nUI\n\n\nFinally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.\n\n\n{\n  \ndefault\n: {\n    \nqueryHost\n: \nhttp://localhost:9999\n,\n    \nqueryNamespace\n: \napi/bullet/queries\n,\n    \nqueryPath\n: \nws-query\n,\n    \nvalidationPath\n: \nvalidate-query\n,\n    \nqueryStompRequestChannel\n: \n/server/request\n,\n    \nqueryStompResponseChannel\n: \n/client/response\n,\n    \nschemaHost\n: \nhttp://localhost:9999\n,\n    \nschemaNamespace\n: \napi/bullet\n,\n    \nhelpLinks\n: [\n      {\n        \nname\n: \nTutorials\n,\n        \nlink\n: \nhttps://bullet-db.github.io/ui/usage\n\n      }\n    ],\n    \nbugLink\n: \nhttps://github.com/bullet-db/bullet-ui/issues\n,\n    \nmodelVersion\n: 4,\n    \nmigrations\n: {\n      \ndeletions\n: \nquery\n\n    },\n    \ndefaultValues\n: {\n      \naggregationMaxSize\n: 1024,\n      \nrawMaxSize\n: 500,\n      \ndurationMaxSecs\n: 9007199254740,\n      \ndistributionNumberOfPoints\n: 11,\n      \ndistributionQuantilePoints\n: \n0, 0.25, 0.5, 0.75, 0.9, 1\n,\n      \ndistributionQuantileStart\n: 0,\n      \ndistributionQuantileEnd\n: 1,\n      \ndistributionQuantileIncrement\n: 0.1,\n      \nwindowEmitFrequencyMinSecs\n: 1,\n      \neveryForRecordBasedWindow\n: 1,\n      \neveryForTimeBasedWindow\n: 2000,\n      \nsketches\n: {\n        \ncountDistinctMaxEntries\n: 16384,\n        \ngroupByMaxEntries\n: 512,\n        \ndistributionMaxEntries\n: 1024,\n        \ndistributionMaxNumberOfPoints\n: 200,\n        \ntopKMaxEntries\n: 1024,\n        \ntopKErrorType\n: \nNo False Negatives\n\n      },\n      \nmetadataKeyMapping\n: {\n        \nquerySection\n: \nQuery\n,\n        \nwindowSection\n: \nWindow\n,\n        \nsketchSection\n: \nSketch\n,\n        \ntheta\n: \nTheta\n,\n        \nuniquesEstimate\n: \nUniques Estimate\n,\n        \nqueryCreationTime\n: \nReceive Time\n,\n        \nqueryTerminationTime\n: \nFinish Time\n,\n        \nestimatedResult\n: \nWas Estimated\n,\n        \nstandardDeviations\n: \nStandard Deviations\n,\n        \nnormalizedRankError\n: \nNormalized Rank Error\n,\n        \nmaximumCountError\n: \nMaximum Count Error\n,\n        \nitemsSeen\n: \nItems Seen\n,\n        \nminimumValue\n: \nMinimum Value\n,\n        \nmaximumValue\n: \nMaximum Value\n,\n        \nwindowNumber\n: \nNumber\n,\n        \nwindowSize\n: \nSize\n,\n        \nwindowEmitTime\n: \nEmit Time\n,\n        \nexpectedEmitTime\n: \nExpected Emit Time\n\n      }\n    }\n  }\n}\n\n\n\n\nSince we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no \nschemaPath\n because it must be the constant string \ncolumns\n. If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to \nschemaHost/schemaNamespace/columns\n.", 
            "title": "Spark"
        }, 
        {
            "location": "/quick-start/spark/#quick-start-on-spark", 
            "text": "In this section we will setup a mock instance of Bullet to play around with. We will use  Bullet Spark  to run the backend of Bullet on the  Spark  framework. And we will use the  Bullet Kafka PubSub .  At the end of this section, you will have:   Launched the Bullet backend on Spark  Setup the  Web Service  Setup the  UI  to talk to the Web Service   Prerequisites   You will need to be on an Unix-based system (Mac OS X, Ubuntu ...) with  curl  installed  You will need  JDK 8  installed", 
            "title": "Quick Start on Spark"
        }, 
        {
            "location": "/quick-start/spark/#install-script", 
            "text": "Simply run:  curl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-spark.sh | bash  This will setup a local Spark and Kafka cluster, a Bullet running on it, the Bullet Web Service and a Bullet UI for you. Once everything has launched, you should be able to go to the Bullet UI running locally at  http://localhost:8800 . You can then  continue this guide from here .   Want to DIY?  If you want to manually run all the commands or if the script died while doing something above (might want to perform the  teardown  first), you can continue below.", 
            "title": "Install Script"
        }, 
        {
            "location": "/quick-start/spark/#manual-installation", 
            "text": "", 
            "title": "Manual Installation"
        }, 
        {
            "location": "/quick-start/spark/#step-1-setup-directories-and-examples", 
            "text": "export BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/spark\nmkdir -p $BULLET_HOME/pubsub\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME\ncurl -LO https://github.com/bullet-db/bullet-db.github.io/releases/download/v1.0.0/examples_artifacts.tar.gz\ntar -xzf examples_artifacts.tar.gz\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-examples", 
            "title": "Step 1: Setup directories and examples"
        }, 
        {
            "location": "/quick-start/spark/#setup-kafka", 
            "text": "For this instance of Bullet we will use the Kafka PubSub implementation found in  bullet-spark . So we will first download and run Kafka, and setup a couple Kafka topics.", 
            "title": "Setup Kafka"
        }, 
        {
            "location": "/quick-start/spark/#step-2-download-and-install-kafka", 
            "text": "cd $BULLET_HOME/pubsub\ncurl -Lo bullet-kafka.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-kafka/1.0.1/bullet-kafka-1.0.1-fat.jar\ncurl -LO https://archive.apache.org/dist/kafka/2.3.1/kafka_2.12-2.3.1.tgz\ntar -xzf kafka_2.12-2.3.1.tgz\nexport KAFKA_DIR=$BULLET_HOME/pubsub/kafka_2.12-2.3.1", 
            "title": "Step 2: Download and Install Kafka"
        }, 
        {
            "location": "/quick-start/spark/#step-3-start-zookeeper", 
            "text": "$KAFKA_DIR/bin/zookeeper-server-start.sh $KAFKA_DIR/config/zookeeper.properties", 
            "title": "Step 3: Start Zookeeper"
        }, 
        {
            "location": "/quick-start/spark/#step-4-start-kafka", 
            "text": "Give Zookeeper ~5-10 seconds to start up, then start Kafka:  $KAFKA_DIR/bin/kafka-server-start.sh $KAFKA_DIR/config/server.properties", 
            "title": "Step 4: Start Kafka"
        }, 
        {
            "location": "/quick-start/spark/#step-5-create-kafka-topics", 
            "text": "The Bullet Kafka PubSub uses two topics. One to send messages from the Web Service to the Backend, and one to send messages from the Backend to the Web Service. So we will create a Kafka topic called \"bullet.requests\" and another called \"bullet.responses\".  $KAFKA_DIR/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic bullet.requests\n$KAFKA_DIR/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic bullet.responses", 
            "title": "Step 5: Create Kafka Topics"
        }, 
        {
            "location": "/quick-start/spark/#setup-bullet-backend-on-spark", 
            "text": "We will run the bullet-spark backend using  Spark 3.0.1 .", 
            "title": "Setup Bullet Backend on Spark"
        }, 
        {
            "location": "/quick-start/spark/#step-6-install-spark-301", 
            "text": "export BULLET_SPARK=$BULLET_HOME/backend/spark\ncd $BULLET_SPARK\ncurl -O https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\ntar -xzf spark-3.0.1-bin-hadoop2.7.tgz", 
            "title": "Step 6: Install Spark 3.0.1"
        }, 
        {
            "location": "/quick-start/spark/#step-7-setup-bullet-spark-and-example-data-producer", 
            "text": "cp $BULLET_HOME/bullet-examples/backend/spark/* $BULLET_SPARK\ncurl -Lo bullet-spark.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-spark/1.0.0/bullet-spark-1.0.0-standalone.jar", 
            "title": "Step 7: Setup Bullet-Spark and Example Data Producer"
        }, 
        {
            "location": "/quick-start/spark/#step-8-launch-the-bullet-spark-backend", 
            "text": "Run this multi-line command (new lines are escaped):  $BULLET_SPARK/spark-3.0.1-bin-hadoop2.7/bin/spark-submit \\\n    --master local[10]  \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --jars $BULLET_HOME/pubsub/bullet-kafka.jar,$BULLET_SPARK/bullet-spark-example.jar \\\n    $BULLET_SPARK/bullet-spark.jar \\\n    --bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml   log.txt    The Backend will usually be up and running usually within 5-10 seconds. Once it is running you can get information about the Spark job in the Spark UI, which can be seen in your browser at  http://localhost:4040  by default. The Web Service will now be hooked up through the Kafka PubSub to the Spark backend. To test it you can now run a Bullet query by hitting the Web Service directly:", 
            "title": "Step 8: Launch the Bullet Spark Backend"
        }, 
        {
            "location": "/quick-start/spark/#setup-web-service", 
            "text": "", 
            "title": "Setup Web Service"
        }, 
        {
            "location": "/quick-start/spark/#step-9-install-the-bullet-web-service", 
            "text": "cd $BULLET_HOME/service\ncurl -Lo bullet-service.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/1.0.0/bullet-service-1.0.0-embedded.jar\ncp $BULLET_EXAMPLES/web-service/example_kafka_pubsub_config.yaml $BULLET_HOME/service/\ncp $BULLET_EXAMPLES/web-service/example_query_config.yaml $BULLET_HOME/service/\ncp $BULLET_EXAMPLES/web-service/example_columns.json $BULLET_HOME/service/", 
            "title": "Step 9: Install the Bullet Web Service"
        }, 
        {
            "location": "/quick-start/spark/#step-10-launch-the-web-service", 
            "text": "Run this multi-line command (new lines are escaped):  java -Dloader.path=$BULLET_HOME/pubsub/bullet-kafka.jar -jar bullet-service.jar \\\n    --bullet.pubsub.config=$BULLET_HOME/service/example_kafka_pubsub_config.yaml \\\n    --bullet.query.config=${BULLET_HOME}/service/example_query_config.yaml \\\n    --bullet.schema.file=$BULLET_HOME/service/example_columns.json \\\n    --server.port=9999  \\\n    --logging.path=. \\\n    --logging.file=log.txt   log.txt", 
            "title": "Step 10: Launch the Web Service"
        }, 
        {
            "location": "/quick-start/spark/#step-11-test-the-web-service-optional", 
            "text": "We can check that the Web Service is up and running by getting the example columns through the API:  curl -s http://localhost:9999/api/bullet/columns  curl -s -H 'Content-Type: text/plain' -X POST -d 'SELECT * FROM STREAM(2000, TIME) LIMIT 1;' http://localhost:9999/api/bullet/queries/sse-query  This query will return a result JSON containing a \"records\" field containing a single record, and a \"meta\" field with some meta information.   What is this data?  This data is randomly generated by the  custom Spark Streaming Receiver  that generates toy data to demo Bullet. In practice, your producer would read from an actual data source such as Kafka etc.   You can also check the status of the Web Service by looking at the Web Service log: $BULLET_HOME/service/log.txt", 
            "title": "Step 11: Test the Web Service (optional)"
        }, 
        {
            "location": "/quick-start/spark/#setting-up-the-bullet-ui", 
            "text": "", 
            "title": "Setting up the Bullet UI"
        }, 
        {
            "location": "/quick-start/spark/#step-12-install-node", 
            "text": "cd $BULLET_HOME/ui\ncurl -s https://raw.githubusercontent.com/creationix/nvm/v0.37.2/install.sh | bash\nsource ~/.bashrc\nnvm install v10.20.1\nnvm use v10.20.1", 
            "title": "Step 12: Install Node"
        }, 
        {
            "location": "/quick-start/spark/#step-13-install-the-bullet-ui", 
            "text": "curl -LO https://github.com/bullet-db/bullet-ui/releases/download/v1.0.1/bullet-ui-v1.0.1.tar.gz\ntar -xzf bullet-ui-v1.0.1.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/", 
            "title": "Step 13: Install the Bullet UI"
        }, 
        {
            "location": "/quick-start/spark/#step-14-launch-the-ui", 
            "text": "PORT=8800 node express-server.js    Visit  http://localhost:8800  to query your topology with the UI. See  UI usage  for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.   Running it remotely?  If you access the UI from another machine than where your UI is actually running, you will need to edit  config/env-settings.json . Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change  localhost  in your env-settings.json to point to the host name where you will hosting the UI. This will be the same as the UI host you use in the browser. You can also do a local port forward on the machine accessing the UI by running:  ssh -N -L 8800:localhost:8800 -L 9999:localhost:9999 hostname-of-the-quickstart-components 2 1 .", 
            "title": "Step 14: Launch the UI"
        }, 
        {
            "location": "/quick-start/spark/#playing-around-with-the-instance", 
            "text": "Check out and follow along with the  UI Usage  page as it shows you some queries you can run using this UI.", 
            "title": "Playing around with the instance"
        }, 
        {
            "location": "/quick-start/spark/#teardown", 
            "text": "When you are done trying out Bullet, you can stop the processes and cleanup using the instructions below.  If you were using the  Install Script  or if you don't want to manually bring down everything, you can run:  curl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-spark.sh | bash -s cleanup  If you were performing the steps yourself, you can also manually cleanup  all the components and all the downloads  using:           UI  pkill -f [e]xpress-server.js    Web Service  pkill -f [e]xample_kafka_pubsub_config.yaml    Spark  pkill -f [b]ullet-spark    Kafka  ${KAFKA_DIR}/bin/kafka-server-stop.sh    Zookeeper  ${KAFKA_DIR}/bin/zookeeper-server-stop.sh    File System  rm -rf $BULLET_HOME /tmp/zookeeper /tmp/kafka-logs/ /tmp/spark-checkpoint     Note: This does  not  delete  $HOME/.nvm .", 
            "title": "Teardown"
        }, 
        {
            "location": "/quick-start/spark/#what-did-we-do", 
            "text": "This section will go over the various custom pieces this example plugged into Bullet, so you can better understand what we did.", 
            "title": "What did we do?"
        }, 
        {
            "location": "/quick-start/spark/#spark-streaming-job", 
            "text": "The Spark Streaming application we ran was Bullet plugged in with a custom Receiver in our implementation of the Bullet Spark DataProducer trait. This Receiver and DataProducer are implemented in this  example project  and was already built for you when you  downloaded the examples . It does not read from any data source and just produces random, structured data. It also produces only up to a maximum number of records in a given period. Both this maximum and the length of a period are configured in the Receiver (at most 100 every 1 second).  $BULLET_SPARK/spark-3.0.1-bin-hadoop2.7/bin/spark-submit \\\n    --master local[10]  \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --jars $BULLET_HOME/pubsub/bullet-kafka.jar,$BULLET_SPARK/bullet-spark-example.jar \\\n    $BULLET_SPARK/bullet-spark.jar \\\n    --bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml   log.txt    We launched the bullet-spark jar (an uber or \"fat\" jar) containing Bullet Spark and all its dependencies. We added our Pubsub (see below) implementation and our jar containing our custom Receiver to the Spark job's additional jars.  The settings defined by  --bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml  and the arguments here run all components in the Spark Streaming job.   I thought you said hundreds of thousands of records...  100 records per second is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components with CPU and memory configurations to accommodate for your data volume and querying needs.   Let's look at the  custom Receiver code  that generates the data.      private def receive(): Unit = {\n      nextIntervalStart = System.currentTimeMillis()\n      while (!isStopped) {\n        val timeNow = System.currentTimeMillis()\n        // Only emit if we are still in the interval and haven't gone over our per period max\n        if (timeNow  = nextIntervalStart   generatedThisPeriod   maxPerPeriod) {\n          store(generateRecord())\n          generatedThisPeriod += 1\n        }\n        if (timeNow   nextIntervalStart) {\n          logger.info( Generated {} tuples out of {} , generatedThisPeriod, maxPerPeriod)\n          nextIntervalStart = timeNow + period\n          generatedThisPeriod = 0\n          periodCount += 1\n        }\n        // It is courteous to sleep for a short time.\n        try {\n          Thread.sleep(1)\n        } catch {\n          case e: InterruptedException =  logger.error( Error:  , e)\n        }\n      }\n    }  This method above emits the data. This method is wrapped in a thread that is called by the Spark framework. This function only emits at most the given maximum tuples per period.      private def makeRandomMap: Map[java.lang.String, java.lang.String] = {\n      val randomMap = new HashMap[java.lang.String, java.lang.String](2)\n      randomMap.put(RandomReceiver.RANDOM_MAP_KEY_A, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n      randomMap.put(RandomReceiver.RANDOM_MAP_KEY_B, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n      randomMap\n    }\n\n    private def generateRecord(): BulletRecord[_  : java.io.Serializable] = {\n      val record = new TypedSimpleBulletRecord()\n      val uuid = UUID.randomUUID().toString\n      record.setString(RandomReceiver.STRING, uuid)\n      record.setLong(RandomReceiver.LONG, generatedThisPeriod)\n      record.setDouble(RandomReceiver.DOUBLE, Random.nextDouble())\n      record.setDouble(RandomReceiver.GAUSSIAN, Random.nextGaussian())\n      record.setString(RandomReceiver.TYPE, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n      record.setLong(RandomReceiver.DURATION, System.nanoTime() % RandomReceiver.INTEGER_POOL(Random.nextInt(RandomReceiver.INTEGER_POOL.length)))\n\n      // Don't use Scala Map and convert it by asJava when calling setxxxMap method in BulletRecord.\n      // It converts Scala Map to scala.collection.convert.Wrappers$MapWrapper which is not serializable in scala 2.11.x (https://issues.scala-lang.org/browse/SI-8911).\n\n      record.setStringMap(RandomReceiver.SUBTYPES_MAP, makeRandomMap);\n\n      val booleanMap = new HashMap[java.lang.String, java.lang.Boolean](4)\n      booleanMap.put(uuid.substring(0, 8), Random.nextBoolean())\n      booleanMap.put(uuid.substring(9, 13), Random.nextBoolean())\n      booleanMap.put(uuid.substring(14, 18), Random.nextBoolean())\n      booleanMap.put(uuid.substring(19, 23), Random.nextBoolean())\n      record.setBooleanMap(RandomReceiver.BOOLEAN_MAP, booleanMap)\n\n\n      val statsMap = new HashMap[java.lang.String, java.lang.Long](4)\n      statsMap.put(RandomReceiver.PERIOD_COUNT, periodCount)\n      statsMap.put(RandomReceiver.RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod)\n      statsMap.put(RandomReceiver.NANO_TIME, System.nanoTime())\n      statsMap.put(RandomReceiver.TIMESTAMP, System.nanoTime())\n      record.setLongMap(RandomReceiver.STATS_MAP, statsMap)\n\n      record.setListOfStringMap(RandomReceiver.LIST, asList(makeRandomMap, makeRandomMap))\n      record\n    }  This  generateRecord  method generates some fields randomly and inserts them into a BulletRecord (simple). Note that the BulletRecord is typed and all data must be inserted with the proper types.  This whole receiver is plugged into an implementation of the Spark DataProducer trait that Bullet Spark requires to plug in your data (as a Spark DStream) into it. You can find this class implemented  here  and reproduced below.  package com.yahoo.bullet.spark.examples\n\nimport com.yahoo.bullet.record.BulletRecord\nimport com.yahoo.bullet.spark.DataProducer\nimport com.yahoo.bullet.spark.examples.receiver.RandomReceiver\nimport com.yahoo.bullet.spark.utils.BulletSparkConfig\nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.dstream.DStream\n\nclass RandomProducer extends DataProducer {\n  override def getBulletRecordStream(ssc: StreamingContext, config: BulletSparkConfig): DStream[BulletRecord] = {\n    // Bullet record input stream.\n    val bulletReceiver = new RandomReceiver(config)\n    ssc.receiverStream(bulletReceiver).asInstanceOf[DStream[BulletRecord]]\n  }\n}  If you put Bullet on your data, you will need to write a DataProducer (or a full on Spark DAG if your reading is complex), that reads from your data source and emits a DStream of BulletRecords with the fields you wish to be query-able similar to this example, or you can use  Bullet DSL to configure and plug in  a DSL based receiver that uses the Bullet DSL's Connector -  Serializer -  Converter system to read and convert your dataset without having to write code!", 
            "title": "Spark Streaming Job"
        }, 
        {
            "location": "/quick-start/spark/#pubsub", 
            "text": "We used the  Kafka PubSub . We configured the Backend to use this PubSub by adding these settings to the YAML file that we passed to our Spark Streaming job. Notice that we set the context to  QUERY_PROCESSING  since this is the Backend.  bullet.pubsub.context.name:  QUERY_PROCESSING \nbullet.pubsub.class.name:  com.yahoo.bullet.kafka.KafkaPubSub \nbullet.pubsub.kafka.bootstrap.servers:  localhost:9092 \nbullet.pubsub.kafka.request.topic.name:  bullet.requests \nbullet.pubsub.kafka.response.topic.name:  bullet.responses   For the Web Service, we passed in a YAML file that pointed to the same Kafka topics. Notice that we set the context to  QUERY_SUBMISSION  since this is the Web Service.  bullet.pubsub.context.name:  QUERY_SUBMISSION \nbullet.pubsub.class.name:  com.yahoo.bullet.kafka.KafkaPubSub \nbullet.pubsub.kafka.bootstrap.servers:  localhost:9092 \nbullet.pubsub.kafka.request.topic.name:  bullet.requests \nbullet.pubsub.kafka.response.topic.name:  bullet.responses", 
            "title": "PubSub"
        }, 
        {
            "location": "/quick-start/spark/#web-service", 
            "text": "We launched the Web Service using two custom files - a PubSub configuration YAML file and JSON schema file.  The JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.  The following is a snippet from the  JSON file . Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using  enumerations .  [\n    {\n         name :  probability ,\n         type :  DOUBLE ,\n         description :  Generated from Random#nextDouble \n    },\n    ...\n    {\n         name :  stats ,\n         type :  MAP ,\n         subtype :  LONG ,\n         description :  This map contains some numeric information such as the current number of periods etc. ,\n         enumerations : [\n            ...\n            { name :  nano_time ,  description :  The ns time when this record was generated }\n        ]\n    },\n    {\n         name :  classifiers ,\n         type :  LIST ,\n         subtype :  MAP ,\n         description :  This contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf \n    }\n]  The contents of the  PubSub configuration file  was discussed in the  PubSub section above .", 
            "title": "Web Service"
        }, 
        {
            "location": "/quick-start/spark/#ui", 
            "text": "Finally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.  {\n   default : {\n     queryHost :  http://localhost:9999 ,\n     queryNamespace :  api/bullet/queries ,\n     queryPath :  ws-query ,\n     validationPath :  validate-query ,\n     queryStompRequestChannel :  /server/request ,\n     queryStompResponseChannel :  /client/response ,\n     schemaHost :  http://localhost:9999 ,\n     schemaNamespace :  api/bullet ,\n     helpLinks : [\n      {\n         name :  Tutorials ,\n         link :  https://bullet-db.github.io/ui/usage \n      }\n    ],\n     bugLink :  https://github.com/bullet-db/bullet-ui/issues ,\n     modelVersion : 4,\n     migrations : {\n       deletions :  query \n    },\n     defaultValues : {\n       aggregationMaxSize : 1024,\n       rawMaxSize : 500,\n       durationMaxSecs : 9007199254740,\n       distributionNumberOfPoints : 11,\n       distributionQuantilePoints :  0, 0.25, 0.5, 0.75, 0.9, 1 ,\n       distributionQuantileStart : 0,\n       distributionQuantileEnd : 1,\n       distributionQuantileIncrement : 0.1,\n       windowEmitFrequencyMinSecs : 1,\n       everyForRecordBasedWindow : 1,\n       everyForTimeBasedWindow : 2000,\n       sketches : {\n         countDistinctMaxEntries : 16384,\n         groupByMaxEntries : 512,\n         distributionMaxEntries : 1024,\n         distributionMaxNumberOfPoints : 200,\n         topKMaxEntries : 1024,\n         topKErrorType :  No False Negatives \n      },\n       metadataKeyMapping : {\n         querySection :  Query ,\n         windowSection :  Window ,\n         sketchSection :  Sketch ,\n         theta :  Theta ,\n         uniquesEstimate :  Uniques Estimate ,\n         queryCreationTime :  Receive Time ,\n         queryTerminationTime :  Finish Time ,\n         estimatedResult :  Was Estimated ,\n         standardDeviations :  Standard Deviations ,\n         normalizedRankError :  Normalized Rank Error ,\n         maximumCountError :  Maximum Count Error ,\n         itemsSeen :  Items Seen ,\n         minimumValue :  Minimum Value ,\n         maximumValue :  Maximum Value ,\n         windowNumber :  Number ,\n         windowSize :  Size ,\n         windowEmitTime :  Emit Time ,\n         expectedEmitTime :  Expected Emit Time \n      }\n    }\n  }\n}  Since we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no  schemaPath  because it must be the constant string  columns . If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to  schemaHost/schemaNamespace/columns .", 
            "title": "UI"
        }, 
        {
            "location": "/quick-start/storm/", 
            "text": "Quick Start on Storm\n\n\nThis section gets you running a mock instance of Bullet to play around with. The instance will run using \nBullet on Storm\n and use the \nREST Pubsub\n. Since we do not have an actual data source, we will produce some fake data and convert it into \nBullet Records\n in a \ncustom Storm spout\n. If you want to use Bullet for your data, you will need to do read and convert your data to Bullet Records in a similar manner.\n\n\nAt the end of this section, you will have:\n\n\n\n\nSetup the Bullet topology using a custom spout on \nbullet-storm-1.0.0\n\n\nSetup the \nWeb Service\n talking to the topology and serving a schema for your UI using \nbullet-service-1.0.0\n\n\nSetup the \nREST PubSub\n talking to the topology and Web Service using \nbullet-core-1.2.0\n.\n\n\nSetup the \nUI\n talking to the Web Service using \nbullet-ui-1.0.1\n\n\n\n\nPrerequisites\n\n\n\n\nYou will need to be on an Unix-based system (Mac OS X, Ubuntu ...) with \ncurl\n installed\n\n\nYou will need \nJDK 8+\n installed\n\n\nYou will need enough CPU and RAM on your machine to run about 8-10 JVMs in \nserver\n mode. You should have at least 2 GB free space on your disk. We will be setting up a Storm cluster with multiple components, an embedded Tomcat server and a Node server.\n\n\n\n\nInstall Script\n\n\nSimply run:\n\n\ncurl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-storm.sh | bash\n\n\n\n\nThis will setup a local Storm cluster, a Bullet running on it, the Bullet Web Service and a Bullet UI for you. Once everything has launched, you should be able to go to the Bullet UI running locally at \nhttp://localhost:8800\n. You can then \ncontinue this guide from here\n.\n\n\n\n\nWant to DIY?\n\n\nIf you want to manually run all the commands or if the script died while doing something above (might want to perform the \nteardown\n first), you can continue below.\n\n\n\n\nManual Installation\n\n\nSetting up Storm\n\n\nTo set up a clean working environment, let's start with creating some directories.\n\n\nStep 1: Setup directories and examples\n\n\nexport BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/storm\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME\ncurl -LO https://github.com/bullet-db/bullet-db.github.io/releases/download/v1.0.0/examples_artifacts.tar.gz\ntar -xzf examples_artifacts.tar.gz\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-examples\n\n\n\n\nStep 2: Install Storm 2.2\n\n\ncd $BULLET_HOME/backend\ncurl -LO https://downloads.apache.org/storm/apache-storm-2.2.0/apache-storm-2.2.0.zip\nunzip apache-storm-2.2.0.zip\nexport PATH=$(pwd)/apache-storm-2.2.0/bin/:$PATH\n\n\n\n\nStep 3: Launch Storm components\n\n\nLaunch each of the following components, in order and wait for the commands to go through. You may have to do these one at a time. You will see a JVM being launched for each one and connection messages as the components communicate through Zookeeper.\n\n\nstorm dev-zookeeper \n\nstorm nimbus \n\nstorm ui \n\nstorm logviewer \n\nstorm supervisor \n\n\n\n\n\nIt may take 30-60 seconds for all the components to launch.\n\n\nOnce everything is up without errors, visit \nhttp://localhost:8080\n and see if the Storm UI loads.\n\n\n\n\nLocal mode cleanup\n\n\nIf you notice any problems while setting up storm or while relaunching a topology, it may be because some state is corrupted. When running Storm in this fashion, states and serializations are stored in \nstorm-local\n and \n/tmp/\n. You may want to \nrm -rf storm-local/* /tmp/dev-storm-zookeeper\n to clean up this state before relaunching Storm components. See the \ntear down section\n on how to kill any running instances.\n\n\n\n\nSetting up the example Bullet topology\n\n\nNow that Storm is up and running, we can put Bullet on it. We will use an example spout that runs on Bullet 1.2.0 on our Storm cluster. The source is available \nhere\n. This was part of the artifact that you installed in Step 1.\n\n\nStep 4: Setup the Storm example\n\n\ncp $BULLET_EXAMPLES/backend/storm/* $BULLET_HOME/backend/storm\n\n\n\n\n\n\nSettings\n\n\nTake a look at bullet_settings.yaml for the settings that are being overridden for this example. You can add or change settings as you like by referring to \ncore Bullet settings in bullet_defaults.yaml\n and \nStorm settings in bullet_storm_defaults.yaml\n. In particular, we have \ncustomized these settings\n that affect the Bullet queries you can run:\n\n\nbullet.query.aggregation.raw.max.size: 500\n The max \nRAW\n records you can fetch is 500.\n\n\nbullet.query.aggregation.count.distinct.sketch.entries: 16384\n We can count 16384 unique values exactly. Approximates after.\n\n\nbullet.query.aggregation.group.sketch.entries: 1024\n The max unique groups can be 1024. Uniform sample after.\n\n\nbullet.query.aggregation.distribution.sketch.entries: 1024\n Determines the normalized rank error for distributions.\n\n\nbullet.query.aggregation.top.k.sketch.entries: 1024\n 0.75 times this number is the number of unique items for which counts can be done exactly. Approximates after.\n\n\nbullet.query.aggregation.distribution.max.points: 200\n The maximum number of points you can generate, use or provide for a Distribution aggregation.\n\n\n\n\n\n\nWant to tweak the example topology code?\n\n\nYou will need to clone the \nexamples repository\n and customize it. To build the examples, you'll need to install \nMaven 3\n.\n\n\ncd $BULLET_HOME \n git clone git@github.com:bullet-db/bullet-db.github.io.git\n\n\ncd bullet-db.github.io/examples/storm \n mvn package\n\n\nYou will find the \nbullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar\n in \n$BULLET_HOME/bullet-db.github.io/examples/storm/target/\n\n\nYou can also make the \nexamples_artifacts.tar.gz\n file with all the settings that is placed in \n$BULLET_EXAMPLES\n by just running \nmake\n in the \nbullet-db.github.io/examples/\n folder.\n\n\n\n\nStep 5: Launch the topology\n\n\ncd $BULLET_HOME/backend/storm \n ./launch.sh\n\n\n\n\nVisit the UI and see if the topology is up. You should see the \nDataSource\n spout begin emitting records.\n\n\n\n\nWhere is this data coming from?\n\n\nThis data is randomly generated by the \ncustom Storm spout\n that is in the example topology you just launched. In practice, your spout would read from an actual data source such as Kafka etc. See \nbelow\n for more details about this random data spout.\n\n\n\n\nSetting up the Bullet Web Service\n\n\nStep 6: Install the Bullet Web Service\n\n\ncd $BULLET_HOME/service\ncurl -Lo bullet-service.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/1.0.0/bullet-service-1.0.0-embedded.jar\ncp $BULLET_EXAMPLES/web-service/example* $BULLET_HOME/service/\n\n\n\n\nStep 7: Launch the Web Service\n\n\ncd $BULLET_HOME/service\njava -jar bullet-service.jar --bullet.pubsub.config=example_rest_pubsub_config.yaml --bullet.schema.file=example_columns.json --bullet.pubsub.builtin.rest.enabled=true --server.port=9999  --logging.path=. --logging.file=log.txt \n log.txt \n\n\n\n\n\nNote that we turned on the built-in REST pubsub in the Web Service when launching it. The REST PubSub is bundled into the Bullet API by default, so no additional jars are needed.\n\n\nYou can verify that it is up by running a Bullet query or getting the example columns through the API:\n\n\ncurl -s -H 'Content-Type: text/plain' -X POST -d 'SELECT * FROM STREAM(10000, TIME) LIMIT 1' http://localhost:9999/api/bullet/queries/sse-query\ncurl -s http://localhost:9999/api/bullet/columns\n\n\n\n\n\n\nSettings\n\n\nTake a look at example_query_settings.yaml for the settings that are being overridden for this example. You can add or change the query settings (used by BQL when creating the query) by referring to \ncore Bullet settings in bullet_defaults.yaml\n. We have \ncustomized these settings\n:\n\n\nbullet.query.aggregation.max.size: 1024\n The max records you can fetch for any query is 1024.\n\n\n\n\nSetting up the Bullet UI\n\n\nStep 8: Install Node\n\n\ncurl -s https://raw.githubusercontent.com/creationix/nvm/v0.37.2/install.sh | bash\nsource ~/.bashrc\nnvm install v10.20.1\nnvm use v10.20.1\n\n\n\n\nStep 9: Install the Bullet UI\n\n\ncd $BULLET_HOME/ui\ncurl -LO https://github.com/bullet-db/bullet-ui/releases/download/v1.0.1/bullet-ui-v1.0.1.tar.gz\ntar -xzf bullet-ui-v1.0.1.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/\n\n\n\n\nStep 10: Launch the UI\n\n\nPORT=8800 node express-server.js \n\n\n\n\n\nVisit \nhttp://localhost:8800\n to query your topology with the UI. See \nUI usage\n for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.\n\n\n\n\nRunning it remotely?\n\n\nIf you access the UI from another machine than where your UI is actually running, you will need to edit \nconfig/env-settings.json\n. Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change \nlocalhost\n in your env-settings.json to point to the host name where you will hosting the UI. This will be the same as the UI host you use in the browser. You can also do a local port forward on the machine accessing the UI by running: \nssh -N -L 8800:localhost:8800 -L 9999:localhost:9999 hostname-of-the-quickstart-components 2\n1\n\n\n\n\nPlaying around with the instance\n\n\nCheck out and follow along with the \nUI Usage\n page as it shows you some queries you can run using this UI.\n\n\nTeardown\n\n\nIf you were using the \nInstall Script\n or if you don't want to manually bring down everything, you can run:\n\n\ncurl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-storm.sh | bash -s cleanup\n\n\n\n\nIf you were performing the steps yourself, you can also manually cleanup \nall the components and all the downloads\n using:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUI\n\n\npkill -f [e]xpress-server.js\n\n\n\n\n\n\nWeb Service\n\n\npkill -f [e]xample_rest_pubsub_config.yaml\n\n\n\n\n\n\nStorm\n\n\npkill -f [a]pache-storm-2.2.0\n\n\n\n\n\n\nFile System\n\n\nrm -rf $BULLET_HOME /tmp/dev-storm-zookeeper\n\n\n\n\n\n\n\n\nThis does \nnot\n delete \n$HOME/.nvm\n and some extra lines nvm may have added to your \n$HOME/{.profile, .bash_profile, .zshrc, .bashrc}\n.\n\n\nWhat did we do?\n\n\nThis section will go over the various custom pieces this example plugged into Bullet, so you can better understand what we did.\n\n\nStorm topology\n\n\nThe topology was the Bullet topology plugged in with a custom spout. This spout is implemented in this \nexample project\n and was already built for you when you \ndownloaded the examples\n. It does not read from any data source and just produces random, structured data. It also produces only up to a maximum number of records in a given period. Both this maximum and the length of a period are configurable. If you examine $BULLET_HOME/backend/storm/launch.sh, you'll see the following:\n\n\nstorm jar bullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf ./bullet_settings.yaml \\\n          --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout \\\n          --bullet-spout-parallelism 1 \\\n          ...\n          --bullet-spout-arg 20 \\\n          --bullet-spout-arg 101 \\\n          ...\n\n\n\n\nThis command launches the jar (an uber or \"fat\" jar) containing the custom spout code and all dependencies you copied in Step 5. We pass the name of your spout class with \n--bullet-spout com.yahoo.bullet.storm.examples.RandomSpout\n to the Bullet main class \ncom.yahoo.bullet.Topology\n with two arguments \n--bullet-spout-arg 20\n and \n--bullet-spout-arg 101\n. The first argument tells the spout to generate at most 20 tuples (records) in a period and the second argument says a period is 101 ms long.\n\n\nThe settings defined by \n--bullet-conf ./bullet_settings.yaml\n and the arguments here run all components in the topology with a parallelism of 1. So there will be one spout that is producing ~200 rps.\n\n\n\n\nI thought you said hundreds of thousands of records...\n\n\n200 records is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components with CPU and memory configurations to accommodate for your data volume and querying needs.\n\n\n\n\nLet's look at the \ncustom spout code\n that generates the data.\n\n\n    @Override\n    public void nextTuple() {\n        long timeNow = System.nanoTime();\n        // Only emit if we are still in the interval and haven't gone over our per period max\n        if (timeNow \n= nextIntervalStart \n generatedThisPeriod \n maxPerPeriod) {\n            outputCollector.emit(new Values(generateRecord()), DUMMY_ID);\n            generatedThisPeriod++;\n            return;\n        }\n        if (timeNow \n nextIntervalStart) {\n            log.info(\nGenerated {} tuples out of {}\n, generatedThisPeriod, maxPerPeriod);\n            nextIntervalStart = timeNow + period;\n            generatedThisPeriod = 0;\n            periodCount++;\n        }\n        // It is courteous to sleep for a short time if you're not emitting anything...\n        try {\n            Thread.sleep(1);\n        } catch (InterruptedException e) {\n            log.error(\nError: \n, e);\n        }\n    }\n\n\n\n\nThis method above emits the tuples. The Storm framework calls this method. This function only emits at most the given maximum tuples per period.\n\n\n\n\nWhy a DUMMY_ID?\n\n\nWhen the spout emits the randomly generated tuple, it attaches a \nDUMMY_ID\n to it. In Storm terms, this is a message ID. By adding a message ID, this tuple can be made to flow reliably. The Bullet component that receives this tuple (Filter bolt) acknowledges or \"acks\" this tuple. If the tuple did not make it to Filter bolt within a configured timeout window, Storm will call a \nfail(Object messageId)\n method on the spout. This particular spout does not define one and hence the usage of a \nDUMMY_ID\n. If your source of data can identify records uniquely and you can re-emit them on a fail, you should attach that actual ID in place of the \nDUMMY_ID\n.\n\n\n\n\nprivate Map\nString, String\n makeRandomMap() {\n    Map\nString, String\n randomMap = new HashMap\n(2);\n    randomMap.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    randomMap.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    return randomMap;\n}\n\nprivate BulletRecord generateRecord() {\n    BulletRecord record = new AvroBulletRecord();\n    String uuid = UUID.randomUUID().toString();\n\n    record.setString(STRING, uuid);\n    record.setLong(LONG, (long) generatedThisPeriod);\n    record.setDouble(DOUBLE, random.nextDouble());\n    record.setDouble(GAUSSIAN, random.nextGaussian());\n    record.setString(TYPE, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    record.setLong(DURATION, System.currentTimeMillis() % INTEGER_POOL[random.nextInt(INTEGER_POOL.length)]);\n\n    record.setStringMap(SUBTYPES_MAP, makeRandomMap());\n\n    Map\nString, Boolean\n booleanMap = new HashMap\n(4);\n    booleanMap.put(uuid.substring(0, 8), random.nextBoolean());\n    booleanMap.put(uuid.substring(9, 13), random.nextBoolean());\n    booleanMap.put(uuid.substring(14, 18), random.nextBoolean());\n    booleanMap.put(uuid.substring(19, 23), random.nextBoolean());\n    record.setBooleanMap(BOOLEAN_MAP, booleanMap);\n\n    Map\nString, Long\n statsMap = new HashMap\n(4);\n    statsMap.put(PERIOD_COUNT, periodCount);\n    statsMap.put(RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod);\n    statsMap.put(NANO_TIME, System.nanoTime());\n    statsMap.put(TIMESTAMP, System.currentTimeMillis());\n    record.setLongMap(STATS_MAP, statsMap);\n\n    record.setListOfStringMap(LIST, asList(makeRandomMap(), makeRandomMap()));\n\n    return record;\n}\n\n\n\n\nThis \ngenerateRecord\n method generates some fields randomly and inserts them into a BulletRecord. Note that the BulletRecord is typed and all data must be inserted with the proper types.\n\n\nIf you put Bullet on your data, you will need to write a spout (or a topology if your reading is complex), that reads from your data source and emits BulletRecords with the fields you wish to be query-able placed into a BulletRecord similar to this example.\n\n\nPubSub\n\n\nWe used the \nREST PubSub\n. Note that even though we support a DRPC PubSub, it doesn't actually support windowing so we have not used it for this example. We configured the Backend to use this PubSub by adding these settings to the YAML file that we passed to our Storm topology. Notice that we set the context to \nQUERY_PROCESSING\n since this is the Backend. We do not set \nbullet.pubsub.rest.result.url\n because each query sent to the topology has this information so that the results could be returned back to it.\n\n\nbullet.pubsub.context.name: \nQUERY_PROCESSING\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.pubsub.rest.RESTPubSub\n\nbullet.pubsub.rest.query.urls:\n    - \nhttp://localhost:9999/api/bullet/pubsub/query\n\n\n\n\n\nFor the Web Service, we passed in a YAML file that pointed to itself for the REST endpoints that serve as the PubSub interface. Notice that we set the context to \nQUERY_SUBMISSION\n since this is the Web Service.\n\n\nbullet.pubsub.context.name: \nQUERY_SUBMISSION\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.pubsub.rest.RESTPubSub\n\nbullet.pubsub.rest.query.urls:\n    - \nhttp://localhost:9999/api/bullet/pubsub/query\n\nbullet.pubsub.rest.result.url: \nhttp://localhost:9999/api/bullet/pubsub/result\n\nbullet.pubsub.rest.subscriber.connect.timeout.ms: 5000\nbullet.pubsub.rest.publisher.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.result.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10\n\n\n\n\nWeb Service\n\n\nWe launched the Web Service using two custom files - a PubSub configuration YAML file and JSON schema file.\n\n\nThe JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.\n\n\nThe following is a snippet from the \nJSON file\n. Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using \nenumerations\n.\n\n\n[\n    {\n        \nname\n: \nprobability\n,\n        \ntype\n: \nDOUBLE\n,\n        \ndescription\n: \nGenerated from Random#nextDouble\n\n    },\n    ...\n    {\n        \nname\n: \nstats_map\n,\n        \ntype\n: \nMAP\n,\n        \nsubtype\n: \nLONG\n,\n        \ndescription\n: \nThis map contains some numeric information such as the current number of periods etc.\n,\n        \nenumerations\n: [\n            ...\n            {\nname\n: \nnano_time\n, \ndescription\n: \nThe ns time when this record was generated\n}\n        ]\n    },\n    {\n        \nname\n: \nclassifiers\n,\n        \ntype\n: \nLIST\n,\n        \nsubtype\n: \nMAP\n,\n        \ndescription\n: \nThis contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf\n\n    }\n]\n\n\n\n\nThe contents of the \nPubSub configuration file\n was discussed in the \nPubSub section above\n.\n\n\nUI\n\n\nFinally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.\n\n\n{\n  \ndefault\n: {\n    \nqueryHost\n: \nhttp://localhost:9999\n,\n    \nqueryNamespace\n: \napi/bullet/queries\n,\n    \nqueryPath\n: \nws-query\n,\n    \nvalidationPath\n: \nvalidate-query\n,\n    \nqueryStompRequestChannel\n: \n/server/request\n,\n    \nqueryStompResponseChannel\n: \n/client/response\n,\n    \nschemaHost\n: \nhttp://localhost:9999\n,\n    \nschemaNamespace\n: \napi/bullet\n,\n    \nhelpLinks\n: [\n      {\n        \nname\n: \nTutorials\n,\n        \nlink\n: \nhttps://bullet-db.github.io/ui/usage\n\n      }\n    ],\n    \nbugLink\n: \nhttps://github.com/bullet-db/bullet-ui/issues\n,\n    \nmodelVersion\n: 4,\n    \nmigrations\n: {\n      \ndeletions\n: \nquery\n\n    },\n    \ndefaultValues\n: {\n      \naggregationMaxSize\n: 1024,\n      \nrawMaxSize\n: 500,\n      \ndurationMaxSecs\n: 9007199254740,\n      \ndistributionNumberOfPoints\n: 11,\n      \ndistributionQuantilePoints\n: \n0, 0.25, 0.5, 0.75, 0.9, 1\n,\n      \ndistributionQuantileStart\n: 0,\n      \ndistributionQuantileEnd\n: 1,\n      \ndistributionQuantileIncrement\n: 0.1,\n      \nwindowEmitFrequencyMinSecs\n: 1,\n      \neveryForRecordBasedWindow\n: 1,\n      \neveryForTimeBasedWindow\n: 2000,\n      \nsketches\n: {\n        \ncountDistinctMaxEntries\n: 16384,\n        \ngroupByMaxEntries\n: 512,\n        \ndistributionMaxEntries\n: 1024,\n        \ndistributionMaxNumberOfPoints\n: 200,\n        \ntopKMaxEntries\n: 1024,\n        \ntopKErrorType\n: \nNo False Negatives\n\n      },\n      \nmetadataKeyMapping\n: {\n        \nquerySection\n: \nQuery\n,\n        \nwindowSection\n: \nWindow\n,\n        \nsketchSection\n: \nSketch\n,\n        \ntheta\n: \nTheta\n,\n        \nuniquesEstimate\n: \nUniques Estimate\n,\n        \nqueryCreationTime\n: \nReceive Time\n,\n        \nqueryTerminationTime\n: \nFinish Time\n,\n        \nestimatedResult\n: \nWas Estimated\n,\n        \nstandardDeviations\n: \nStandard Deviations\n,\n        \nnormalizedRankError\n: \nNormalized Rank Error\n,\n        \nmaximumCountError\n: \nMaximum Count Error\n,\n        \nitemsSeen\n: \nItems Seen\n,\n        \nminimumValue\n: \nMinimum Value\n,\n        \nmaximumValue\n: \nMaximum Value\n,\n        \nwindowNumber\n: \nNumber\n,\n        \nwindowSize\n: \nSize\n,\n        \nwindowEmitTime\n: \nEmit Time\n,\n        \nexpectedEmitTime\n: \nExpected Emit Time\n\n      }\n    }\n  }\n}\n\n\n\n\nSince we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no \nschemaPath\n because it must be the constant string \ncolumns\n. If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to \nschemaHost/schemaNamespace/columns\n.", 
            "title": "Storm"
        }, 
        {
            "location": "/quick-start/storm/#quick-start-on-storm", 
            "text": "This section gets you running a mock instance of Bullet to play around with. The instance will run using  Bullet on Storm  and use the  REST Pubsub . Since we do not have an actual data source, we will produce some fake data and convert it into  Bullet Records  in a  custom Storm spout . If you want to use Bullet for your data, you will need to do read and convert your data to Bullet Records in a similar manner.  At the end of this section, you will have:   Setup the Bullet topology using a custom spout on  bullet-storm-1.0.0  Setup the  Web Service  talking to the topology and serving a schema for your UI using  bullet-service-1.0.0  Setup the  REST PubSub  talking to the topology and Web Service using  bullet-core-1.2.0 .  Setup the  UI  talking to the Web Service using  bullet-ui-1.0.1   Prerequisites   You will need to be on an Unix-based system (Mac OS X, Ubuntu ...) with  curl  installed  You will need  JDK 8+  installed  You will need enough CPU and RAM on your machine to run about 8-10 JVMs in  server  mode. You should have at least 2 GB free space on your disk. We will be setting up a Storm cluster with multiple components, an embedded Tomcat server and a Node server.", 
            "title": "Quick Start on Storm"
        }, 
        {
            "location": "/quick-start/storm/#install-script", 
            "text": "Simply run:  curl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-storm.sh | bash  This will setup a local Storm cluster, a Bullet running on it, the Bullet Web Service and a Bullet UI for you. Once everything has launched, you should be able to go to the Bullet UI running locally at  http://localhost:8800 . You can then  continue this guide from here .   Want to DIY?  If you want to manually run all the commands or if the script died while doing something above (might want to perform the  teardown  first), you can continue below.", 
            "title": "Install Script"
        }, 
        {
            "location": "/quick-start/storm/#manual-installation", 
            "text": "", 
            "title": "Manual Installation"
        }, 
        {
            "location": "/quick-start/storm/#setting-up-storm", 
            "text": "To set up a clean working environment, let's start with creating some directories.", 
            "title": "Setting up Storm"
        }, 
        {
            "location": "/quick-start/storm/#step-1-setup-directories-and-examples", 
            "text": "export BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/storm\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME\ncurl -LO https://github.com/bullet-db/bullet-db.github.io/releases/download/v1.0.0/examples_artifacts.tar.gz\ntar -xzf examples_artifacts.tar.gz\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-examples", 
            "title": "Step 1: Setup directories and examples"
        }, 
        {
            "location": "/quick-start/storm/#step-2-install-storm-22", 
            "text": "cd $BULLET_HOME/backend\ncurl -LO https://downloads.apache.org/storm/apache-storm-2.2.0/apache-storm-2.2.0.zip\nunzip apache-storm-2.2.0.zip\nexport PATH=$(pwd)/apache-storm-2.2.0/bin/:$PATH", 
            "title": "Step 2: Install Storm 2.2"
        }, 
        {
            "location": "/quick-start/storm/#step-3-launch-storm-components", 
            "text": "Launch each of the following components, in order and wait for the commands to go through. You may have to do these one at a time. You will see a JVM being launched for each one and connection messages as the components communicate through Zookeeper.  storm dev-zookeeper  \nstorm nimbus  \nstorm ui  \nstorm logviewer  \nstorm supervisor    It may take 30-60 seconds for all the components to launch.  Once everything is up without errors, visit  http://localhost:8080  and see if the Storm UI loads.   Local mode cleanup  If you notice any problems while setting up storm or while relaunching a topology, it may be because some state is corrupted. When running Storm in this fashion, states and serializations are stored in  storm-local  and  /tmp/ . You may want to  rm -rf storm-local/* /tmp/dev-storm-zookeeper  to clean up this state before relaunching Storm components. See the  tear down section  on how to kill any running instances.", 
            "title": "Step 3: Launch Storm components"
        }, 
        {
            "location": "/quick-start/storm/#setting-up-the-example-bullet-topology", 
            "text": "Now that Storm is up and running, we can put Bullet on it. We will use an example spout that runs on Bullet 1.2.0 on our Storm cluster. The source is available  here . This was part of the artifact that you installed in Step 1.", 
            "title": "Setting up the example Bullet topology"
        }, 
        {
            "location": "/quick-start/storm/#step-4-setup-the-storm-example", 
            "text": "cp $BULLET_EXAMPLES/backend/storm/* $BULLET_HOME/backend/storm   Settings  Take a look at bullet_settings.yaml for the settings that are being overridden for this example. You can add or change settings as you like by referring to  core Bullet settings in bullet_defaults.yaml  and  Storm settings in bullet_storm_defaults.yaml . In particular, we have  customized these settings  that affect the Bullet queries you can run:  bullet.query.aggregation.raw.max.size: 500  The max  RAW  records you can fetch is 500.  bullet.query.aggregation.count.distinct.sketch.entries: 16384  We can count 16384 unique values exactly. Approximates after.  bullet.query.aggregation.group.sketch.entries: 1024  The max unique groups can be 1024. Uniform sample after.  bullet.query.aggregation.distribution.sketch.entries: 1024  Determines the normalized rank error for distributions.  bullet.query.aggregation.top.k.sketch.entries: 1024  0.75 times this number is the number of unique items for which counts can be done exactly. Approximates after.  bullet.query.aggregation.distribution.max.points: 200  The maximum number of points you can generate, use or provide for a Distribution aggregation.    Want to tweak the example topology code?  You will need to clone the  examples repository  and customize it. To build the examples, you'll need to install  Maven 3 .  cd $BULLET_HOME   git clone git@github.com:bullet-db/bullet-db.github.io.git  cd bullet-db.github.io/examples/storm   mvn package  You will find the  bullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar  in  $BULLET_HOME/bullet-db.github.io/examples/storm/target/  You can also make the  examples_artifacts.tar.gz  file with all the settings that is placed in  $BULLET_EXAMPLES  by just running  make  in the  bullet-db.github.io/examples/  folder.", 
            "title": "Step 4: Setup the Storm example"
        }, 
        {
            "location": "/quick-start/storm/#step-5-launch-the-topology", 
            "text": "cd $BULLET_HOME/backend/storm   ./launch.sh  Visit the UI and see if the topology is up. You should see the  DataSource  spout begin emitting records.   Where is this data coming from?  This data is randomly generated by the  custom Storm spout  that is in the example topology you just launched. In practice, your spout would read from an actual data source such as Kafka etc. See  below  for more details about this random data spout.", 
            "title": "Step 5: Launch the topology"
        }, 
        {
            "location": "/quick-start/storm/#setting-up-the-bullet-web-service", 
            "text": "", 
            "title": "Setting up the Bullet Web Service"
        }, 
        {
            "location": "/quick-start/storm/#step-6-install-the-bullet-web-service", 
            "text": "cd $BULLET_HOME/service\ncurl -Lo bullet-service.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/1.0.0/bullet-service-1.0.0-embedded.jar\ncp $BULLET_EXAMPLES/web-service/example* $BULLET_HOME/service/", 
            "title": "Step 6: Install the Bullet Web Service"
        }, 
        {
            "location": "/quick-start/storm/#step-7-launch-the-web-service", 
            "text": "cd $BULLET_HOME/service\njava -jar bullet-service.jar --bullet.pubsub.config=example_rest_pubsub_config.yaml --bullet.schema.file=example_columns.json --bullet.pubsub.builtin.rest.enabled=true --server.port=9999  --logging.path=. --logging.file=log.txt   log.txt    Note that we turned on the built-in REST pubsub in the Web Service when launching it. The REST PubSub is bundled into the Bullet API by default, so no additional jars are needed.  You can verify that it is up by running a Bullet query or getting the example columns through the API:  curl -s -H 'Content-Type: text/plain' -X POST -d 'SELECT * FROM STREAM(10000, TIME) LIMIT 1' http://localhost:9999/api/bullet/queries/sse-query\ncurl -s http://localhost:9999/api/bullet/columns   Settings  Take a look at example_query_settings.yaml for the settings that are being overridden for this example. You can add or change the query settings (used by BQL when creating the query) by referring to  core Bullet settings in bullet_defaults.yaml . We have  customized these settings :  bullet.query.aggregation.max.size: 1024  The max records you can fetch for any query is 1024.", 
            "title": "Step 7: Launch the Web Service"
        }, 
        {
            "location": "/quick-start/storm/#setting-up-the-bullet-ui", 
            "text": "", 
            "title": "Setting up the Bullet UI"
        }, 
        {
            "location": "/quick-start/storm/#step-8-install-node", 
            "text": "curl -s https://raw.githubusercontent.com/creationix/nvm/v0.37.2/install.sh | bash\nsource ~/.bashrc\nnvm install v10.20.1\nnvm use v10.20.1", 
            "title": "Step 8: Install Node"
        }, 
        {
            "location": "/quick-start/storm/#step-9-install-the-bullet-ui", 
            "text": "cd $BULLET_HOME/ui\ncurl -LO https://github.com/bullet-db/bullet-ui/releases/download/v1.0.1/bullet-ui-v1.0.1.tar.gz\ntar -xzf bullet-ui-v1.0.1.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/", 
            "title": "Step 9: Install the Bullet UI"
        }, 
        {
            "location": "/quick-start/storm/#step-10-launch-the-ui", 
            "text": "PORT=8800 node express-server.js    Visit  http://localhost:8800  to query your topology with the UI. See  UI usage  for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.   Running it remotely?  If you access the UI from another machine than where your UI is actually running, you will need to edit  config/env-settings.json . Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change  localhost  in your env-settings.json to point to the host name where you will hosting the UI. This will be the same as the UI host you use in the browser. You can also do a local port forward on the machine accessing the UI by running:  ssh -N -L 8800:localhost:8800 -L 9999:localhost:9999 hostname-of-the-quickstart-components 2 1", 
            "title": "Step 10: Launch the UI"
        }, 
        {
            "location": "/quick-start/storm/#playing-around-with-the-instance", 
            "text": "Check out and follow along with the  UI Usage  page as it shows you some queries you can run using this UI.", 
            "title": "Playing around with the instance"
        }, 
        {
            "location": "/quick-start/storm/#teardown", 
            "text": "If you were using the  Install Script  or if you don't want to manually bring down everything, you can run:  curl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-storm.sh | bash -s cleanup  If you were performing the steps yourself, you can also manually cleanup  all the components and all the downloads  using:           UI  pkill -f [e]xpress-server.js    Web Service  pkill -f [e]xample_rest_pubsub_config.yaml    Storm  pkill -f [a]pache-storm-2.2.0    File System  rm -rf $BULLET_HOME /tmp/dev-storm-zookeeper     This does  not  delete  $HOME/.nvm  and some extra lines nvm may have added to your  $HOME/{.profile, .bash_profile, .zshrc, .bashrc} .", 
            "title": "Teardown"
        }, 
        {
            "location": "/quick-start/storm/#what-did-we-do", 
            "text": "This section will go over the various custom pieces this example plugged into Bullet, so you can better understand what we did.", 
            "title": "What did we do?"
        }, 
        {
            "location": "/quick-start/storm/#storm-topology", 
            "text": "The topology was the Bullet topology plugged in with a custom spout. This spout is implemented in this  example project  and was already built for you when you  downloaded the examples . It does not read from any data source and just produces random, structured data. It also produces only up to a maximum number of records in a given period. Both this maximum and the length of a period are configurable. If you examine $BULLET_HOME/backend/storm/launch.sh, you'll see the following:  storm jar bullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf ./bullet_settings.yaml \\\n          --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout \\\n          --bullet-spout-parallelism 1 \\\n          ...\n          --bullet-spout-arg 20 \\\n          --bullet-spout-arg 101 \\\n          ...  This command launches the jar (an uber or \"fat\" jar) containing the custom spout code and all dependencies you copied in Step 5. We pass the name of your spout class with  --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout  to the Bullet main class  com.yahoo.bullet.Topology  with two arguments  --bullet-spout-arg 20  and  --bullet-spout-arg 101 . The first argument tells the spout to generate at most 20 tuples (records) in a period and the second argument says a period is 101 ms long.  The settings defined by  --bullet-conf ./bullet_settings.yaml  and the arguments here run all components in the topology with a parallelism of 1. So there will be one spout that is producing ~200 rps.   I thought you said hundreds of thousands of records...  200 records is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components with CPU and memory configurations to accommodate for your data volume and querying needs.   Let's look at the  custom spout code  that generates the data.      @Override\n    public void nextTuple() {\n        long timeNow = System.nanoTime();\n        // Only emit if we are still in the interval and haven't gone over our per period max\n        if (timeNow  = nextIntervalStart   generatedThisPeriod   maxPerPeriod) {\n            outputCollector.emit(new Values(generateRecord()), DUMMY_ID);\n            generatedThisPeriod++;\n            return;\n        }\n        if (timeNow   nextIntervalStart) {\n            log.info( Generated {} tuples out of {} , generatedThisPeriod, maxPerPeriod);\n            nextIntervalStart = timeNow + period;\n            generatedThisPeriod = 0;\n            periodCount++;\n        }\n        // It is courteous to sleep for a short time if you're not emitting anything...\n        try {\n            Thread.sleep(1);\n        } catch (InterruptedException e) {\n            log.error( Error:  , e);\n        }\n    }  This method above emits the tuples. The Storm framework calls this method. This function only emits at most the given maximum tuples per period.   Why a DUMMY_ID?  When the spout emits the randomly generated tuple, it attaches a  DUMMY_ID  to it. In Storm terms, this is a message ID. By adding a message ID, this tuple can be made to flow reliably. The Bullet component that receives this tuple (Filter bolt) acknowledges or \"acks\" this tuple. If the tuple did not make it to Filter bolt within a configured timeout window, Storm will call a  fail(Object messageId)  method on the spout. This particular spout does not define one and hence the usage of a  DUMMY_ID . If your source of data can identify records uniquely and you can re-emit them on a fail, you should attach that actual ID in place of the  DUMMY_ID .   private Map String, String  makeRandomMap() {\n    Map String, String  randomMap = new HashMap (2);\n    randomMap.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    randomMap.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    return randomMap;\n}\n\nprivate BulletRecord generateRecord() {\n    BulletRecord record = new AvroBulletRecord();\n    String uuid = UUID.randomUUID().toString();\n\n    record.setString(STRING, uuid);\n    record.setLong(LONG, (long) generatedThisPeriod);\n    record.setDouble(DOUBLE, random.nextDouble());\n    record.setDouble(GAUSSIAN, random.nextGaussian());\n    record.setString(TYPE, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    record.setLong(DURATION, System.currentTimeMillis() % INTEGER_POOL[random.nextInt(INTEGER_POOL.length)]);\n\n    record.setStringMap(SUBTYPES_MAP, makeRandomMap());\n\n    Map String, Boolean  booleanMap = new HashMap (4);\n    booleanMap.put(uuid.substring(0, 8), random.nextBoolean());\n    booleanMap.put(uuid.substring(9, 13), random.nextBoolean());\n    booleanMap.put(uuid.substring(14, 18), random.nextBoolean());\n    booleanMap.put(uuid.substring(19, 23), random.nextBoolean());\n    record.setBooleanMap(BOOLEAN_MAP, booleanMap);\n\n    Map String, Long  statsMap = new HashMap (4);\n    statsMap.put(PERIOD_COUNT, periodCount);\n    statsMap.put(RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod);\n    statsMap.put(NANO_TIME, System.nanoTime());\n    statsMap.put(TIMESTAMP, System.currentTimeMillis());\n    record.setLongMap(STATS_MAP, statsMap);\n\n    record.setListOfStringMap(LIST, asList(makeRandomMap(), makeRandomMap()));\n\n    return record;\n}  This  generateRecord  method generates some fields randomly and inserts them into a BulletRecord. Note that the BulletRecord is typed and all data must be inserted with the proper types.  If you put Bullet on your data, you will need to write a spout (or a topology if your reading is complex), that reads from your data source and emits BulletRecords with the fields you wish to be query-able placed into a BulletRecord similar to this example.", 
            "title": "Storm topology"
        }, 
        {
            "location": "/quick-start/storm/#pubsub", 
            "text": "We used the  REST PubSub . Note that even though we support a DRPC PubSub, it doesn't actually support windowing so we have not used it for this example. We configured the Backend to use this PubSub by adding these settings to the YAML file that we passed to our Storm topology. Notice that we set the context to  QUERY_PROCESSING  since this is the Backend. We do not set  bullet.pubsub.rest.result.url  because each query sent to the topology has this information so that the results could be returned back to it.  bullet.pubsub.context.name:  QUERY_PROCESSING \nbullet.pubsub.class.name:  com.yahoo.bullet.pubsub.rest.RESTPubSub \nbullet.pubsub.rest.query.urls:\n    -  http://localhost:9999/api/bullet/pubsub/query   For the Web Service, we passed in a YAML file that pointed to itself for the REST endpoints that serve as the PubSub interface. Notice that we set the context to  QUERY_SUBMISSION  since this is the Web Service.  bullet.pubsub.context.name:  QUERY_SUBMISSION \nbullet.pubsub.class.name:  com.yahoo.bullet.pubsub.rest.RESTPubSub \nbullet.pubsub.rest.query.urls:\n    -  http://localhost:9999/api/bullet/pubsub/query \nbullet.pubsub.rest.result.url:  http://localhost:9999/api/bullet/pubsub/result \nbullet.pubsub.rest.subscriber.connect.timeout.ms: 5000\nbullet.pubsub.rest.publisher.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.result.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10", 
            "title": "PubSub"
        }, 
        {
            "location": "/quick-start/storm/#web-service", 
            "text": "We launched the Web Service using two custom files - a PubSub configuration YAML file and JSON schema file.  The JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.  The following is a snippet from the  JSON file . Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using  enumerations .  [\n    {\n         name :  probability ,\n         type :  DOUBLE ,\n         description :  Generated from Random#nextDouble \n    },\n    ...\n    {\n         name :  stats_map ,\n         type :  MAP ,\n         subtype :  LONG ,\n         description :  This map contains some numeric information such as the current number of periods etc. ,\n         enumerations : [\n            ...\n            { name :  nano_time ,  description :  The ns time when this record was generated }\n        ]\n    },\n    {\n         name :  classifiers ,\n         type :  LIST ,\n         subtype :  MAP ,\n         description :  This contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf \n    }\n]  The contents of the  PubSub configuration file  was discussed in the  PubSub section above .", 
            "title": "Web Service"
        }, 
        {
            "location": "/quick-start/storm/#ui", 
            "text": "Finally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.  {\n   default : {\n     queryHost :  http://localhost:9999 ,\n     queryNamespace :  api/bullet/queries ,\n     queryPath :  ws-query ,\n     validationPath :  validate-query ,\n     queryStompRequestChannel :  /server/request ,\n     queryStompResponseChannel :  /client/response ,\n     schemaHost :  http://localhost:9999 ,\n     schemaNamespace :  api/bullet ,\n     helpLinks : [\n      {\n         name :  Tutorials ,\n         link :  https://bullet-db.github.io/ui/usage \n      }\n    ],\n     bugLink :  https://github.com/bullet-db/bullet-ui/issues ,\n     modelVersion : 4,\n     migrations : {\n       deletions :  query \n    },\n     defaultValues : {\n       aggregationMaxSize : 1024,\n       rawMaxSize : 500,\n       durationMaxSecs : 9007199254740,\n       distributionNumberOfPoints : 11,\n       distributionQuantilePoints :  0, 0.25, 0.5, 0.75, 0.9, 1 ,\n       distributionQuantileStart : 0,\n       distributionQuantileEnd : 1,\n       distributionQuantileIncrement : 0.1,\n       windowEmitFrequencyMinSecs : 1,\n       everyForRecordBasedWindow : 1,\n       everyForTimeBasedWindow : 2000,\n       sketches : {\n         countDistinctMaxEntries : 16384,\n         groupByMaxEntries : 512,\n         distributionMaxEntries : 1024,\n         distributionMaxNumberOfPoints : 200,\n         topKMaxEntries : 1024,\n         topKErrorType :  No False Negatives \n      },\n       metadataKeyMapping : {\n         querySection :  Query ,\n         windowSection :  Window ,\n         sketchSection :  Sketch ,\n         theta :  Theta ,\n         uniquesEstimate :  Uniques Estimate ,\n         queryCreationTime :  Receive Time ,\n         queryTerminationTime :  Finish Time ,\n         estimatedResult :  Was Estimated ,\n         standardDeviations :  Standard Deviations ,\n         normalizedRankError :  Normalized Rank Error ,\n         maximumCountError :  Maximum Count Error ,\n         itemsSeen :  Items Seen ,\n         minimumValue :  Minimum Value ,\n         maximumValue :  Maximum Value ,\n         windowNumber :  Number ,\n         windowSize :  Size ,\n         windowEmitTime :  Emit Time ,\n         expectedEmitTime :  Expected Emit Time \n      }\n    }\n  }\n}  Since we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no  schemaPath  because it must be the constant string  columns . If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to  schemaHost/schemaNamespace/columns .", 
            "title": "UI"
        }, 
        {
            "location": "/backend/ingestion/", 
            "text": "Data Ingestion\n\n\nBullet operates on a generic data container that it understands. In order to get Bullet to operate on your data, you need to convert your data records into this format. This conversion is usually done when you plug in your data source into Bullet. Bullet does not make any assumptions on where you get this data from. It could be \nKafka\n, \nRabbitMQ\n, or something else.\n\n\n\n\nIf you are trying to set up Bullet...\n\n\nThe rest of this page gives more information about the Record container and how to depend on it in code directly. If you are setting up Bullet, the Record is already included by default with the Bullet artifact. You can head on over to \nsetting up the Storm topology\n to build the piece that gets your data into the Record container.\n\n\n\n\nBullet Record\n\n\nThe Bullet backend processes data that must be stored in a \nBullet Record\n which is an abstract Java class that can\nbe implemented as to be optimized for different backends or use-cases.\n\n\nThere are currently two concrete implementations of BulletRecord:\n\n\n\n\nSimpleBulletRecord\n which is based on a simple Java HashMap\n\n\nAvroBulletRecord\n which uses \nAvro\n for serialization\n\n\n\n\nTypes\n\n\nData placed into a Bullet Record is strongly typed. We support these types currently:\n\n\nPrimitives\n\n\n\n\nBoolean\n\n\nInteger\n\n\nLong\n\n\nFloat\n\n\nDouble\n\n\nString\n\n\n\n\nComplex\n\n\n\n\nMap of Strings to any of the \nPrimitives\n\n\nMap of Strings to any Map in 1\n\n\nList of any of the \nPrimitives\n\n\nList of any Map in 1\n\n\n\n\nWith these types, it is unlikely you would have data that cannot be represented as Bullet Record but if you do, please let us know and we are more than willing to accommodate.\n\n\nInstalling the Record directly\n\n\nGenerally, you depend on the Bullet Core artifact for your Stream Processor when you plug in the piece that gets your data into the Stream processor. The Bullet Core artifact already brings in the Bullet Record containers as well. See the usage for the \nStorm\n for an example.\n\n\nHowever, if you need it, the artifacts are available through JCenter to depend on them in code directly. You will need to add the repository. Below is a Maven example:\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-record\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nIf you just need the jar artifact, you can download it directly from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the sources or the javadoc.", 
            "title": "Record Container"
        }, 
        {
            "location": "/backend/ingestion/#data-ingestion", 
            "text": "Bullet operates on a generic data container that it understands. In order to get Bullet to operate on your data, you need to convert your data records into this format. This conversion is usually done when you plug in your data source into Bullet. Bullet does not make any assumptions on where you get this data from. It could be  Kafka ,  RabbitMQ , or something else.   If you are trying to set up Bullet...  The rest of this page gives more information about the Record container and how to depend on it in code directly. If you are setting up Bullet, the Record is already included by default with the Bullet artifact. You can head on over to  setting up the Storm topology  to build the piece that gets your data into the Record container.", 
            "title": "Data Ingestion"
        }, 
        {
            "location": "/backend/ingestion/#bullet-record", 
            "text": "The Bullet backend processes data that must be stored in a  Bullet Record  which is an abstract Java class that can\nbe implemented as to be optimized for different backends or use-cases.  There are currently two concrete implementations of BulletRecord:   SimpleBulletRecord  which is based on a simple Java HashMap  AvroBulletRecord  which uses  Avro  for serialization", 
            "title": "Bullet Record"
        }, 
        {
            "location": "/backend/ingestion/#types", 
            "text": "Data placed into a Bullet Record is strongly typed. We support these types currently:", 
            "title": "Types"
        }, 
        {
            "location": "/backend/ingestion/#primitives", 
            "text": "Boolean  Integer  Long  Float  Double  String", 
            "title": "Primitives"
        }, 
        {
            "location": "/backend/ingestion/#complex", 
            "text": "Map of Strings to any of the  Primitives  Map of Strings to any Map in 1  List of any of the  Primitives  List of any Map in 1   With these types, it is unlikely you would have data that cannot be represented as Bullet Record but if you do, please let us know and we are more than willing to accommodate.", 
            "title": "Complex"
        }, 
        {
            "location": "/backend/ingestion/#installing-the-record-directly", 
            "text": "Generally, you depend on the Bullet Core artifact for your Stream Processor when you plug in the piece that gets your data into the Stream processor. The Bullet Core artifact already brings in the Bullet Record containers as well. See the usage for the  Storm  for an example.  However, if you need it, the artifacts are available through JCenter to depend on them in code directly. You will need to add the repository. Below is a Maven example:  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-record /artifactId \n   version ${bullet.version} /version  /dependency   If you just need the jar artifact, you can download it directly from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the sources or the javadoc.", 
            "title": "Installing the Record directly"
        }, 
        {
            "location": "/backend/dsl/", 
            "text": "Bullet DSL\n\n\nBullet DSL is a configuration-based DSL that allows users to plug their data into the Bullet Backend. Instead of having users write their own code to set up the Backend on their data, users can now accomplish the same thing by simply providing the appropriate configuration to Bullet.\n\n\nTo support this, Bullet DSL provides two major components. The first is for reading data from a pluggable data source (the \nconnectors\n for talking to various data sources), and the second is for converting data (the \nconverters\n for understanding your data formats) into \nBulletRecords\n.\nBy enabling Bullet DSL in the Backend and configuring Bullet DSL, your backend will use the two components to read from the configured data source and convert the data into BulletRecords, without you having to write any code.\n\n\nThere is also an optional minor component that acts as the glue between the connectors and the converters. These are the \ndeserializers\n. They exist if the data coming out of connector is of a format that cannot be understood by a converter. Typically, this happens for serialized data that needs to be deserialized first before a converter can understand it.\n\n\nThe four interfaces that the DSL uses are:\n\n\n\n\nThe \nBulletConnector\n : Bullet DSL's reading component\n\n\nThe \nBulletDeserializer\n : Bullet DSL's optional deserializing component\n\n\nThe \nBulletRecordConverter\n : Bullet DSL's converting component\n\n\nThe \nBullet Backend\n : The implementation of Bullet on a Stream Processor\n\n\n\n\nBulletConnector\n\n\nBulletConnector is an abstract Java class that can be implemented to read data from different pluggable data sources. As with all our components, we provide and maintain implementations while providing an interface to add new ones. Currently, we support two BulletConnector implementations:\n\n\n\n\nKafkaConnector\n for connecting to \nApache Kafka\n\n\nPulsarConnector\n for connecting to \nApache Pulsar\n\n\n\n\nWhen using Bullet DSL, you will need to specify the particular BulletConnector to use. For example, if you wanted to use the KafkaConnector, you would add the following to your configuration file:\n\n\n# The classpath to the BulletConnector to use (need this for Bullet DSL!)\nbullet.dsl.connector.class.name: \ncom.yahoo.bullet.dsl.connector.KafkaConnector\n\n\n# The read timeout duration in ms (defaults to 0)\nbullet.dsl.connector.read.timeout.ms: 0\n\n# Whether or not to asynchronously commit messages (defaults to true)\nbullet.dsl.connector.async.commit.enable: true\n\n\n\n\nAll BulletConnector configuration can be found in the \ndefault configuration file\n along with specific default configuration for both implementations.\n\n\n\n\nNote\n\n\nIf you have an unsupported data source and you want to use Bullet DSL, you will have to implement your own BulletConnector. If you do, please do consider contributing it back! Check out the\nBulletConnector interface \nhere\n.\n\n\n\n\nKafkaConnector\n\n\nThe KafkaConnector configuration requires a few settings that are necessary to read from Kafka, including the bootstrap servers, group id, and key/value deserializers, and the topics to subscribe to.\n\n\n# The list of Kafka topics to subscribe to (required)\nbullet.dsl.connector.kafka.topics:\n  - \n\n\n# Whether or not the KafkaConsumer should seek to the end of its subscribed topics at initialization (defaults to false)\nbullet.dsl.connector.kafka.start.at.end.enable: false\n\n# Required consumer properties\nbullet.dsl.connector.kafka.bootstrap.servers: \nlocalhost:9092\n\nbullet.dsl.connector.kafka.group.id:\nbullet.dsl.connector.kafka.key.deserializer: \norg.apache.kafka.common.serialization.StringDeserializer\n\nbullet.dsl.connector.kafka.value.deserializer:\n\n\n\n\nYou can also pass additional Kafka properties to the KafkaConnector by prefixing them with \nbullet.dsl.connector.kafka.\n For a complete list of properties, see the \nKafka Consumer configs\n.\n\n\nPulsarConnector\n\n\nThe PulsarConnector configuration requires a few settings that are necessary to read from Pulsar. It also provides additional options to enable authentication and/or TLS.\n\n\n# The list of Pulsar topics to subscribe to (required)\nbullet.dsl.connector.pulsar.topics:\n- \n\n\n# The Pulsar Schema to use (required)\nbullet.dsl.connector.pulsar.schema.type: \nBYTES\n\n\n# The classpath to the Pulsar Schema to use (required only if using JSON, AVRO, PROTOBUF, or CUSTOM schema)\nbullet.dsl.connector.pulsar.schema.class.name:\n\n# Required client property\nbullet.dsl.connector.pulsar.client.serviceUrl: \npulsar://localhost:6650\n\n\n# Authentication properties (disabled by default)\nbullet.dsl.connector.pulsar.auth.enable: false\nbullet.dsl.connector.pulsar.auth.plugin.class.name:\nbullet.dsl.connector.pulsar.auth.plugin.params.string:\n\n# Required consumer properties\nbullet.dsl.connector.pulsar.consumer.subscriptionName: \n\nbullet.dsl.connector.pulsar.consumer.subscriptionType: \nShared\n\n\n\n\n\nMost important to note is that the connector requires a \nPulsar schema\n whose type can be either BYTES, STRING, JSON, AVRO, PROTOBUF, or CUSTOM (defaults to BYTES). If the schema is any type except CUSTOM, the connector will load the schema natively supported by Pulsar. For JSON, AVRO, and PROTOBUF, the POJO class to wrap must be specified. For a CUSTOM schema, the schema class must be specified instead.\n\n\nYou can also pass additional Pulsar Client and Consumer properties to the PulsarConnector by prefixing them with \nbullet.dsl.connector.pulsar.client\n and \nbullet.dsl.connector.pulsar.consumer\n For both lists of properties, see Pulsar \nClientConfigurationData\n and \nConsumerConfigurationData\n.\n\n\nBulletRecordConverter\n\n\nBulletRecordConverter is an abstract Java class that can be implemented to convert different types of data formats into BulletRecords.\n\n\nCurrently, we support three BulletRecordConverter implementations:\n\n\n\n\nPOJOBulletRecordConverter\n to convert \nPOJOs\n\n\nMapBulletRecordConverter\n for \nJava Maps\n of Objects\n\n\nAvroBulletRecordConverter\n for \nApache Avro\n\n\n\n\nWhen using Bullet DSL, you will need to specify the appropriate BulletRecordConverter to use. The converters also support taking in an optional schema (see the [Schema section][#schema] for more details and the benefits to using one). For example, to use AvroBulletRecordConverter, you would add the following to your configuration file:\n\n\n# The classpath to the BulletRecordConverter to use\nbullet.dsl.converter.class.name: \ncom.yahoo.bullet.dsl.converter.AvroBulletRecordConverter\n\n\n# The path to the schema file to use\nbullet.dsl.converter.schema.file: \nyour-schema-file.json\n\n\n\n\n\n\n\nSupported Converters\n\n\nAt this moment, these are the converters that we maintain. If you do need a specific converter though that is not yet available, you can write your own (and hopefully contribute it back!). Check out the BulletRecordConverter\ninterface \nhere\n.\n\n\n\n\nPOJOBulletRecordConverter\n\n\nThe POJOBulletRecordConverter uses Java Reflection to convert POJOs into BulletRecords. In the configuration, you need to specify the POJO class you want to convert, and when the converter is created, it will inspect the POJO with Reflection. Without a \nschema\n, the converter will look through all fields and accept only the fields that have valid types. With a schema, the converter will only look for the fields referenced, but it will also accept getter methods. It is recommended to specify getters as references where possible.\n\n\nbullet.dsl.converter.pojo.class.name: \ncom.your.package.YourPOJO\n\n\n\n\n\nMapBulletRecordConverter\n\n\nThe MapBulletRecordConverter is used to convert Java Maps of Objects into BulletRecords. Without a schema, it simply inserts every entry in the Map into a BulletRecord without any type-checking. If the Map contains objects that are not types supported by the BulletRecord, you might have issues when serializing the record.\n\n\nJSONBulletRecordConverter\n\n\nThe JSONBulletRecordConverter is used to convert String JSON representations of records into BulletRecords. Without a schema, it simply inserts every entry in the JSON object into a BulletRecord without any type-checking and it only uses the Double type for all numeric values (since it is unable to guess whether records might need a wider type). You should use a schema and mention the appropriate types if you want more specific numeric types for the fields in your record. If the JSON contains objects that are not types supported by the BulletRecord, you might have issues when serializing the record.\n\n\nAvroBulletRecordConverter\n\n\nThe AvroBulletRecordConverter is used to convert Avro records into BulletRecords. Without a schema, it inserts every field into a BulletRecord without any type-checking. With a schema, you get type-checking, and you can also specify a RECORD field, and the converter will accept Avro Records in addition to Maps, flattening them into the BulletRecord.\n\n\nSchema\n\n\nThe schema consists of a list of fields each described by a name, reference, type, and subtype.\n\n\n\n\nname\n :  The name of the field in the BulletRecord\n\n\nreference\n : The field to extract from the to-be-converted object\n\n\ntype\n : The type of the field\n\n\n\n\nWhen using the schema:\n\n\n\n\nThe \nname\n of the field in the schema will be the name of the field in the BulletRecord.\n\n\nThe \nreference\n of the field in the schema is the field/value to be extracted from an object when it is converted to a BulletRecord.\n\n\nIf the \nreference\n is null, it is assumed that the \nname\n and the \nreference\n are the same.\n\n\nThe \ntype\n must be specified and can be used for type-checking. If you provide a schema and set the \nbullet.dsl.converter.schema.type.check.enable\n setting, then the converter will validate that the types in the source data matches the given type here. Otherwise, the type provided will be assumed. This is useful when initially using the DSL and you are not sure of the types.\n\n\n\n\nTypes\n\n\n\n\nBOOLEAN\n\n\nINTEGER\n\n\nLONG\n\n\nFLOAT\n\n\nDOUBLE\n\n\nSTRING\n\n\nBOOLEAN_MAP\n\n\nINTEGER_MAP\n\n\nLONG_MAP\n\n\nFLOAT_MAP\n\n\nDOUBLE_MAP\n\n\nSTRING_MAP\n\n\nBOOLEAN_MAP_MAP\n\n\nINTEGER_MAP_MAP\n\n\nLONG_MAP_MAP\n\n\nFLOAT_MAP_MAP\n\n\nDOUBLE_MAP_MAP\n\n\nSTRING_MAP_MAP\n\n\nBOOLEAN_LIST\n\n\nINTEGER_LIST\n\n\nLONG_LIST\n\n\nFLOAT_LIST\n\n\nDOUBLE_LIST\n\n\nSTRING_LIST\n\n\nBOOLEAN_MAP_LIST\n\n\nINTEGER_MAP_LIST\n\n\nLONG_MAP_LIST\n\n\nFLOAT_MAP_LIST\n\n\nDOUBLE_MAP_LIST\n\n\nSTRING_MAP_LIST\n\n\n\n\n\n\nSpecial Type for a RECORD\n\n\nThere is a special case where if you omit the \ntype\n and the \nname\n for an entry in the schema, the reference is assumed to be a map containing arbitrary fields with types in the list above. You can use this if you have a map field that contains various objects with one or more types in the list above and want to flatten that map out into the target record using the respective types of each field in the map. The names of the fields in the map will be used as the top-level names in the resulting record.\n\n\n\n\nExample Schema\n\n\n{\n  \nfields\n: [\n    {\n      \nname\n: \nmyBool\n,\n      \ntype\n: \nBOOLEAN\n\n    },\n    {\n      \nname\n: \nmyBoolMap\n,\n      \ntype\n: \nBOOLEAN_MAP\n\n    },\n    {\n      \nname\n: \nmyLongMapMap\n,\n      \ntype\n: \nLONG_MAP_MAP\n\n    },\n    {\n      \nname\n: \nmyIntFromSomeMap\n,\n      \nreference\n: \nsomeMap.myInt\n,\n      \ntype\n: \nINTEGER\n\n    },\n    {\n      \nname\n: \nmyIntFromSomeIntList\n,\n      \nreference\n: \nsomeIntList.0\n,\n      \ntype\n: \nINTEGER\n\n    },\n    {\n      \nname\n: \nmyIntFromSomeNestedMapsAndLists\n,\n      \nreference\n: \nsomeMap.nestedMap.nestedList.0\n,\n      \ntype\n: \nINTEGER\n\n    },    \n    {\n      \nreference\n : \nsomeMap\n\n    }\n  ]\n}\n\n\n\n\nBulletDeserializer\n\n\nBulletDeserializer is an abstract Java class that can be implemented to deserialize/transform output from BulletConnector to input for BulletRecordConverter. It is an \noptional\n component and whether it's necessary or not depends on the output of your data sources. If one is not needed, the \nIdentityDeserializer\n can be used. For example, if your KafkaConnector outputs byte arrays that are actually Java-serialized Maps, and you're using a MapBulletRecordConverter, you would use the JavaDeserializer, which would deserialize byte arrays into Java Maps for the converter.\n\n\nCurrently, we support two BulletDeserializer implementations:\n\n\n\n\nJavaDeserializer\n\n\nAvroDeserializer\n\n\n\n\nFor Bullet DSL, if you wanted to use AvroDeserializer, you would add the following to your configuration file:\n\n\n# The classpath to the BulletDeserializer to use\nbullet.dsl.deserializer.class.name: \ncom.yahoo.bullet.dsl.deserializer.AvroDeserializer\n\n\n\n\n\nJavaDeserializer\n\n\nThe JavaDeserializer uses Java Serialization to deserialize (Java-serialized) byte arrays into objects.\n\n\nAvroDeserializer\n\n\nThe AvroDeserializer uses Avro to deserialize (Avro-serialized) byte arrays into Avro GenericRecords.\n\n\nThe deserializer must be given the Avro schema for the Avro records you want to deserialize. In the configuration, you can either provide the Avro schema file (note the \nfile://\n prefix) or the Avro class itself (the class must be in the classpath).\n\n\n# The path to the Avro schema file to use prefixed by \nfile://\n\nbullet.dsl.deserializer.avro.schema.file: \nfile://example.avsc\n\n\n# The class name of the Avro record class to deserialize\nbullet.dsl.deserializer.avro.class.name: \ncom.your.package.YourAvro", 
            "title": "DSL"
        }, 
        {
            "location": "/backend/dsl/#bullet-dsl", 
            "text": "Bullet DSL is a configuration-based DSL that allows users to plug their data into the Bullet Backend. Instead of having users write their own code to set up the Backend on their data, users can now accomplish the same thing by simply providing the appropriate configuration to Bullet.  To support this, Bullet DSL provides two major components. The first is for reading data from a pluggable data source (the  connectors  for talking to various data sources), and the second is for converting data (the  converters  for understanding your data formats) into  BulletRecords .\nBy enabling Bullet DSL in the Backend and configuring Bullet DSL, your backend will use the two components to read from the configured data source and convert the data into BulletRecords, without you having to write any code.  There is also an optional minor component that acts as the glue between the connectors and the converters. These are the  deserializers . They exist if the data coming out of connector is of a format that cannot be understood by a converter. Typically, this happens for serialized data that needs to be deserialized first before a converter can understand it.  The four interfaces that the DSL uses are:   The  BulletConnector  : Bullet DSL's reading component  The  BulletDeserializer  : Bullet DSL's optional deserializing component  The  BulletRecordConverter  : Bullet DSL's converting component  The  Bullet Backend  : The implementation of Bullet on a Stream Processor", 
            "title": "Bullet DSL"
        }, 
        {
            "location": "/backend/dsl/#bulletconnector", 
            "text": "BulletConnector is an abstract Java class that can be implemented to read data from different pluggable data sources. As with all our components, we provide and maintain implementations while providing an interface to add new ones. Currently, we support two BulletConnector implementations:   KafkaConnector  for connecting to  Apache Kafka  PulsarConnector  for connecting to  Apache Pulsar   When using Bullet DSL, you will need to specify the particular BulletConnector to use. For example, if you wanted to use the KafkaConnector, you would add the following to your configuration file:  # The classpath to the BulletConnector to use (need this for Bullet DSL!)\nbullet.dsl.connector.class.name:  com.yahoo.bullet.dsl.connector.KafkaConnector \n\n# The read timeout duration in ms (defaults to 0)\nbullet.dsl.connector.read.timeout.ms: 0\n\n# Whether or not to asynchronously commit messages (defaults to true)\nbullet.dsl.connector.async.commit.enable: true  All BulletConnector configuration can be found in the  default configuration file  along with specific default configuration for both implementations.   Note  If you have an unsupported data source and you want to use Bullet DSL, you will have to implement your own BulletConnector. If you do, please do consider contributing it back! Check out the\nBulletConnector interface  here .", 
            "title": "BulletConnector"
        }, 
        {
            "location": "/backend/dsl/#kafkaconnector", 
            "text": "The KafkaConnector configuration requires a few settings that are necessary to read from Kafka, including the bootstrap servers, group id, and key/value deserializers, and the topics to subscribe to.  # The list of Kafka topics to subscribe to (required)\nbullet.dsl.connector.kafka.topics:\n  -  \n\n# Whether or not the KafkaConsumer should seek to the end of its subscribed topics at initialization (defaults to false)\nbullet.dsl.connector.kafka.start.at.end.enable: false\n\n# Required consumer properties\nbullet.dsl.connector.kafka.bootstrap.servers:  localhost:9092 \nbullet.dsl.connector.kafka.group.id:\nbullet.dsl.connector.kafka.key.deserializer:  org.apache.kafka.common.serialization.StringDeserializer \nbullet.dsl.connector.kafka.value.deserializer:  You can also pass additional Kafka properties to the KafkaConnector by prefixing them with  bullet.dsl.connector.kafka.  For a complete list of properties, see the  Kafka Consumer configs .", 
            "title": "KafkaConnector"
        }, 
        {
            "location": "/backend/dsl/#pulsarconnector", 
            "text": "The PulsarConnector configuration requires a few settings that are necessary to read from Pulsar. It also provides additional options to enable authentication and/or TLS.  # The list of Pulsar topics to subscribe to (required)\nbullet.dsl.connector.pulsar.topics:\n-  \n\n# The Pulsar Schema to use (required)\nbullet.dsl.connector.pulsar.schema.type:  BYTES \n\n# The classpath to the Pulsar Schema to use (required only if using JSON, AVRO, PROTOBUF, or CUSTOM schema)\nbullet.dsl.connector.pulsar.schema.class.name:\n\n# Required client property\nbullet.dsl.connector.pulsar.client.serviceUrl:  pulsar://localhost:6650 \n\n# Authentication properties (disabled by default)\nbullet.dsl.connector.pulsar.auth.enable: false\nbullet.dsl.connector.pulsar.auth.plugin.class.name:\nbullet.dsl.connector.pulsar.auth.plugin.params.string:\n\n# Required consumer properties\nbullet.dsl.connector.pulsar.consumer.subscriptionName:  \nbullet.dsl.connector.pulsar.consumer.subscriptionType:  Shared   Most important to note is that the connector requires a  Pulsar schema  whose type can be either BYTES, STRING, JSON, AVRO, PROTOBUF, or CUSTOM (defaults to BYTES). If the schema is any type except CUSTOM, the connector will load the schema natively supported by Pulsar. For JSON, AVRO, and PROTOBUF, the POJO class to wrap must be specified. For a CUSTOM schema, the schema class must be specified instead.  You can also pass additional Pulsar Client and Consumer properties to the PulsarConnector by prefixing them with  bullet.dsl.connector.pulsar.client  and  bullet.dsl.connector.pulsar.consumer  For both lists of properties, see Pulsar  ClientConfigurationData  and  ConsumerConfigurationData .", 
            "title": "PulsarConnector"
        }, 
        {
            "location": "/backend/dsl/#bulletrecordconverter", 
            "text": "BulletRecordConverter is an abstract Java class that can be implemented to convert different types of data formats into BulletRecords.  Currently, we support three BulletRecordConverter implementations:   POJOBulletRecordConverter  to convert  POJOs  MapBulletRecordConverter  for  Java Maps  of Objects  AvroBulletRecordConverter  for  Apache Avro   When using Bullet DSL, you will need to specify the appropriate BulletRecordConverter to use. The converters also support taking in an optional schema (see the [Schema section][#schema] for more details and the benefits to using one). For example, to use AvroBulletRecordConverter, you would add the following to your configuration file:  # The classpath to the BulletRecordConverter to use\nbullet.dsl.converter.class.name:  com.yahoo.bullet.dsl.converter.AvroBulletRecordConverter \n\n# The path to the schema file to use\nbullet.dsl.converter.schema.file:  your-schema-file.json    Supported Converters  At this moment, these are the converters that we maintain. If you do need a specific converter though that is not yet available, you can write your own (and hopefully contribute it back!). Check out the BulletRecordConverter\ninterface  here .", 
            "title": "BulletRecordConverter"
        }, 
        {
            "location": "/backend/dsl/#pojobulletrecordconverter", 
            "text": "The POJOBulletRecordConverter uses Java Reflection to convert POJOs into BulletRecords. In the configuration, you need to specify the POJO class you want to convert, and when the converter is created, it will inspect the POJO with Reflection. Without a  schema , the converter will look through all fields and accept only the fields that have valid types. With a schema, the converter will only look for the fields referenced, but it will also accept getter methods. It is recommended to specify getters as references where possible.  bullet.dsl.converter.pojo.class.name:  com.your.package.YourPOJO", 
            "title": "POJOBulletRecordConverter"
        }, 
        {
            "location": "/backend/dsl/#mapbulletrecordconverter", 
            "text": "The MapBulletRecordConverter is used to convert Java Maps of Objects into BulletRecords. Without a schema, it simply inserts every entry in the Map into a BulletRecord without any type-checking. If the Map contains objects that are not types supported by the BulletRecord, you might have issues when serializing the record.", 
            "title": "MapBulletRecordConverter"
        }, 
        {
            "location": "/backend/dsl/#jsonbulletrecordconverter", 
            "text": "The JSONBulletRecordConverter is used to convert String JSON representations of records into BulletRecords. Without a schema, it simply inserts every entry in the JSON object into a BulletRecord without any type-checking and it only uses the Double type for all numeric values (since it is unable to guess whether records might need a wider type). You should use a schema and mention the appropriate types if you want more specific numeric types for the fields in your record. If the JSON contains objects that are not types supported by the BulletRecord, you might have issues when serializing the record.", 
            "title": "JSONBulletRecordConverter"
        }, 
        {
            "location": "/backend/dsl/#avrobulletrecordconverter", 
            "text": "The AvroBulletRecordConverter is used to convert Avro records into BulletRecords. Without a schema, it inserts every field into a BulletRecord without any type-checking. With a schema, you get type-checking, and you can also specify a RECORD field, and the converter will accept Avro Records in addition to Maps, flattening them into the BulletRecord.", 
            "title": "AvroBulletRecordConverter"
        }, 
        {
            "location": "/backend/dsl/#schema", 
            "text": "The schema consists of a list of fields each described by a name, reference, type, and subtype.   name  :  The name of the field in the BulletRecord  reference  : The field to extract from the to-be-converted object  type  : The type of the field   When using the schema:   The  name  of the field in the schema will be the name of the field in the BulletRecord.  The  reference  of the field in the schema is the field/value to be extracted from an object when it is converted to a BulletRecord.  If the  reference  is null, it is assumed that the  name  and the  reference  are the same.  The  type  must be specified and can be used for type-checking. If you provide a schema and set the  bullet.dsl.converter.schema.type.check.enable  setting, then the converter will validate that the types in the source data matches the given type here. Otherwise, the type provided will be assumed. This is useful when initially using the DSL and you are not sure of the types.", 
            "title": "Schema"
        }, 
        {
            "location": "/backend/dsl/#types", 
            "text": "BOOLEAN  INTEGER  LONG  FLOAT  DOUBLE  STRING  BOOLEAN_MAP  INTEGER_MAP  LONG_MAP  FLOAT_MAP  DOUBLE_MAP  STRING_MAP  BOOLEAN_MAP_MAP  INTEGER_MAP_MAP  LONG_MAP_MAP  FLOAT_MAP_MAP  DOUBLE_MAP_MAP  STRING_MAP_MAP  BOOLEAN_LIST  INTEGER_LIST  LONG_LIST  FLOAT_LIST  DOUBLE_LIST  STRING_LIST  BOOLEAN_MAP_LIST  INTEGER_MAP_LIST  LONG_MAP_LIST  FLOAT_MAP_LIST  DOUBLE_MAP_LIST  STRING_MAP_LIST    Special Type for a RECORD  There is a special case where if you omit the  type  and the  name  for an entry in the schema, the reference is assumed to be a map containing arbitrary fields with types in the list above. You can use this if you have a map field that contains various objects with one or more types in the list above and want to flatten that map out into the target record using the respective types of each field in the map. The names of the fields in the map will be used as the top-level names in the resulting record.", 
            "title": "Types"
        }, 
        {
            "location": "/backend/dsl/#example-schema", 
            "text": "{\n   fields : [\n    {\n       name :  myBool ,\n       type :  BOOLEAN \n    },\n    {\n       name :  myBoolMap ,\n       type :  BOOLEAN_MAP \n    },\n    {\n       name :  myLongMapMap ,\n       type :  LONG_MAP_MAP \n    },\n    {\n       name :  myIntFromSomeMap ,\n       reference :  someMap.myInt ,\n       type :  INTEGER \n    },\n    {\n       name :  myIntFromSomeIntList ,\n       reference :  someIntList.0 ,\n       type :  INTEGER \n    },\n    {\n       name :  myIntFromSomeNestedMapsAndLists ,\n       reference :  someMap.nestedMap.nestedList.0 ,\n       type :  INTEGER \n    },    \n    {\n       reference  :  someMap \n    }\n  ]\n}", 
            "title": "Example Schema"
        }, 
        {
            "location": "/backend/dsl/#bulletdeserializer", 
            "text": "BulletDeserializer is an abstract Java class that can be implemented to deserialize/transform output from BulletConnector to input for BulletRecordConverter. It is an  optional  component and whether it's necessary or not depends on the output of your data sources. If one is not needed, the  IdentityDeserializer  can be used. For example, if your KafkaConnector outputs byte arrays that are actually Java-serialized Maps, and you're using a MapBulletRecordConverter, you would use the JavaDeserializer, which would deserialize byte arrays into Java Maps for the converter.  Currently, we support two BulletDeserializer implementations:   JavaDeserializer  AvroDeserializer   For Bullet DSL, if you wanted to use AvroDeserializer, you would add the following to your configuration file:  # The classpath to the BulletDeserializer to use\nbullet.dsl.deserializer.class.name:  com.yahoo.bullet.dsl.deserializer.AvroDeserializer", 
            "title": "BulletDeserializer"
        }, 
        {
            "location": "/backend/dsl/#javadeserializer", 
            "text": "The JavaDeserializer uses Java Serialization to deserialize (Java-serialized) byte arrays into objects.", 
            "title": "JavaDeserializer"
        }, 
        {
            "location": "/backend/dsl/#avrodeserializer", 
            "text": "The AvroDeserializer uses Avro to deserialize (Avro-serialized) byte arrays into Avro GenericRecords.  The deserializer must be given the Avro schema for the Avro records you want to deserialize. In the configuration, you can either provide the Avro schema file (note the  file://  prefix) or the Avro class itself (the class must be in the classpath).  # The path to the Avro schema file to use prefixed by  file:// \nbullet.dsl.deserializer.avro.schema.file:  file://example.avsc \n\n# The class name of the Avro record class to deserialize\nbullet.dsl.deserializer.avro.class.name:  com.your.package.YourAvro", 
            "title": "AvroDeserializer"
        }, 
        {
            "location": "/backend/storm-architecture/", 
            "text": "Storm architecture\n\n\nThis section describes how the \nBackend architecture\n is implemented in Storm.\n\n\nTopology\n\n\nFor Bullet on Storm, the Storm topology implements the backend piece from the full \nArchitecture\n. The topology is implemented with the standard Storm spout and bolt components:\n\n\n\n\nThe components in \nArchitecture\n have direct counterparts here. The Query spouts reading from the PubSub layer using plugged-in PubSub consumers make up the Request Processor. The Filter bolts and your plugin for your source of data (shown here using plugged with both the DSL spouts and DSL bolts from \nBullet DSL\n) make up the Data Processor. The Join bolt, the Loop bolt and the Result bolt make up the Combiner. There are peripheral components such as the Loop bolts, Tick spouts or the Replay bolts to handle metadata management in the topology.\n\n\nThe red colored lines are the path for the queries that come in through the PubSub, the blue is for the data from your data source and the orange is for metadata and loop-back signals used internally by the backend. The purple lines highlight the most important components where queries and data intermix (the Filter and Join bolts).\n\n\nThe pattern on the lines denote how the data (Storm tuples) is moved to the next component. Dashed indicates a broadcast (sent to all instances of the component), dotted indicates a key grouping (sent to a particular instance based on hashing on a particular field), solid indicates a shuffle (randomly sent to an instance) and dashed-dotted indicates a custom grouping.\n\n\n\n\nWhat's a Replay?\n\n\nThe Replay and the \npluggable storage\n are \noptional components\n in Bullet on Storm. They exist to replay existing queries if you plugged in a storage layer into the \nAPI\n. You would use this if you have long running queries and are not tolerant to losing queries for your use-case. Currently, we do not support storage intermediate results in the storage though. For instance, if you restart the topology but have storage and replay configured, you will recreate all the queries on startup but you will lose all intermediate results that were held in memory so far. We plan to add intermediate result storage as well soon!\n\n\n\n\n\n\nWhat's a Tick spout?\n\n\nThe Tick spout component produces Storm tuples at predefined intervals to the Filter and Join bolts. These tuples, called tick tuples, behave like CPU clock cycles for Bullet. Bullet performs all its system related activities on a tick. This includes purging stale queries, emitting left over data for queries, etc. We could have gone the route of having asynchronous threads that do the same thing but this was a far simpler solution. The downside is that Bullet is as fast or as slow as its tick period, which can be configured on launch (defaults to \n100 ms\n). In practice, this means that your time-based windows need to be at least twice as long as your tick period.\n\n\nAs a practical example of how Bullet uses ticks: when the final data is emitted from the Filter bolts when the query has expired, the Join bolt receiving it waits for 3 (this is configurable) ticks after \nits query\n expires to collect all the last intermediate results from the Filter bolts. If the tick period is set as high as 5 s, this means that a query will take 3 * 15 or 15 s to get back after its expiry! Setting it to 1 s, makes it 1 * 3 s. Similarly, intermediate windows are buffered (for certain kinds of windowed queries) to collect all results for that window before sending it back to the user.\n\n\n\n\nData processing\n\n\nBullet can accept arbitrary sources of data as long as they can be read from Storm. You can either:\n\n\n\n\nWrite a Storm spout (or optionally topology) that reads your data from where ever it is (Kafka etc) and \nconverts it to Bullet Records\n. See \nQuick Start\n for an example.\n\n\nHook up an existing topology that is doing something else directly to Bullet. You will still write and hook up a component that converts your data into Bullet Records in your existing topology.\n\n\nUse \nBullet DSL\n to configure a DSL spout (and optionally a DSL bolt) to use the DSL interfaces to automatically understand your data source with its data format and convert it to the interface Bullet uses without code.\n\n\n\n\n\n\n\n\n\n\n\n\nOption 1\n\n\nOption 2\n\n\nOption 3\n\n\n\n\n\n\n\n\n\n\nWrite code to read from your data source and convert to Bullet records\n\n\nY\n\n\nY\n\n\nN\n\n\n\n\n\n\nWrite Storm spouts and/or bolts\n\n\nY\n\n\nY\n\n\nN\n\n\n\n\n\n\nSaves a persistence/pubsub layer\n\n\nN\n\n\nY\n\n\nN\n\n\n\n\n\n\nSeparate reading data from converting and allowing fan-out\n\n\nN\n\n\nY\n\n\nY\n\n\n\n\n\n\nFull control over how data is read, processed and converted to Bullet records\n\n\nY\n\n\nY\n\n\nN\n\n\n\n\n\n\n\n\nOption 3 is generally flexible and is recommended. Having a code-less way to plug into Bullet is the fastest way to get started. We are adding new data sources and formats to the DSL so that we can support more ways to get your data into Bullet. If a connector or converter is not supported in DSL for your specific data source, you can also implement your own. It will save you from having to write Storm spouts or bolts and lets you reuse the Bullet DSL spout and/or bolt.\n\n\nRegardless of how your data is read, it is then emitted to the Filter bolt. If you have no queries in your system, the Filter bolt will promptly drop all Bullet Records and do nothing. If there are queries in the Filter bolt, the record is checked against each query and if it matches, it is processed by the query. Each query type can choose when to emit its intermediate result based on what window is configured for it. Depending on this, the matched record could be immediately emitted (if it is a RAW query or the intermediate aggregate if anything else) or it could be buffered till a specific time is reached (or the query has expired).\n\n\nRequest processing\n\n\nThe Query spouts fetch Bullet queries through the PubSub layer using the Subscribers provided by the plugged in PubSub layer. The queries received through the PubSub also contain information about the query such as its unique identifier and potentially other metadata. The Query spouts broadcasts the query body to every Filter bolt. Since every Filter bolt has a copy of every query, the shuffled data from the source of data can be compared against the query no matter which particular Filter bolt it ends up at. Each Filter bolt has access to the unique query id and is able to key group by the id to the Join bolt with the intermediate results for the query.\n\n\nThe Query spout also key groups the query and additional query metadata to the Join bolts. This means that the query and the metadata will be end up at exactly one Join bolt.\n\n\n\n\nKey grouping\n\n\nTechnically, Bullet uses a Custom grouping in Storm instead of Key grouping. The Custom grouping does the same thing as what the Key grouping would do in Storm. The reason why we use the Custom grouping is for Replay so that we can deterministically control the Join bolt component that will receive a query and reuse that information for certain aspects of replaying. This is just defensive programming in case Storm changes their key group algorithm to function differently.\n\n\n\n\nCombining\n\n\nSince the data from the Query spout (query and metadata) and the data from all Filter bolts (intermediate results) is key grouped by the unique query id, only one particular Join bolt receives both the query and the intermediate results for a particular query. The Join bolt can then combine the intermediate results and produce a final result. This result is joined (hence the name) along with the metadata for the query and is shuffled to the Result bolts. This bolt then uses the particular Publisher from the plugged in PubSub layer and uses the metadata if it needs to and sends the results back through the PubSub layer to the requestor.\n\n\n\n\nCombining and operations\n\n\nIn order to be able to combine intermediate results and process data in any order, all aggregations that Bullet does need to be associative and have an identity. In other words, they need to be \nMonoids\n. Luckily for us, the \nDataSketches\n that we use are monoids when exact (\nCOUNT DISTINCT\n and \nGROUP BY\n actually are commutative monoids). Sketches can be unioned and thus all the aggregations we support - \nSUM\n, \nCOUNT\n, \nMIN\n, \nMAX\n, \nAVG\n, \nCOUNT DISTINCT\n, \nDISTINCT\n etc - are monoidal. (\nAVG\n is monoidal if you store a \nSUM\n and a \nCOUNT\n instead). When \nDISTRIBUTION\n and \nTOP K\n Sketches are approximating, they may end up not being associative since they depend on the distribution of the data but you can think of them this way if you include their defined error functions bounding the result of the operation.\n\n\n\n\n\n\nLoop back\n\n\nWe have not mentioned the loop components or the replay bolts. These are mainly used to perform house-keeping within the topology or if you have configured storage/replay. For instance, there is a Rate Limit concept in the Bullet core libraries that if violated in any instance of the query being executed, should cause the query to be killed. Wherever this error originates, it will trickle to the Loop bolt and be looped back through the PubSub, through the Query spout and sent to all components that know about the query. These components will then kill the query as well. We call this a loop because strictly speaking, the topology is a Directed Acyclic Graph and we violate it by making a loop. These are also used to deliver external signals such as killing a query etc from the API or the UI. If you disable windows entirely, the Loop bolt will not be wired up when you launch your Bullet topology.\n\n\n\n\nScalability\n\n\nThe topology set up this way scales horizontally and has some nice properties:\n\n\n\n\nIf you want to scale for processing more data but the same amount of queries, you only need to scale the components that read your data (the spout reading the data or your custom topology) and the Filter bolts.\n\n\nIf you want to scale for more queries but the same amount of data, you generally need to scale up the Filter bolts. If you need it, you can scale the Query spouts, Join bolts, Loop bolts and Result bolts. You should ensure that your PubSub layer (if you're using the Storm DRPC PubSub layer, then this is the number of DRPC servers in your Storm cluster) can handle the volume of queries and results being sent through it. These components generally have low parallelisms compared to your data processing components since the data volume is generally much higher than your query volume, so this is generally not needed.\n\n\n\n\nSee \nScaling for more Queries\n and \nScaling for more Data\n for more details.\n\n\n\n\nMore queries and Filter bolts\n\n\nIf you send more queries to the Filter bolt, it will be limited by at most how many queries a Filter bolt can store and still process data efficiently. Factors like CPU, memory allocations etc for the Filter bolts come in to the picture in addition to the parallelism. Generally, if you have allocated enough Filter bolts to process your data with enough head room, this should let you run hundreds of queries simultaneously before you run into these issues. In practice, since most usage is expected to be on large data volumes and 100s of queries simultaneously, you will need to scale the the Filter bolts out so that they are not slowed down by the large number of queries in each.", 
            "title": "Architecture"
        }, 
        {
            "location": "/backend/storm-architecture/#storm-architecture", 
            "text": "This section describes how the  Backend architecture  is implemented in Storm.", 
            "title": "Storm architecture"
        }, 
        {
            "location": "/backend/storm-architecture/#topology", 
            "text": "For Bullet on Storm, the Storm topology implements the backend piece from the full  Architecture . The topology is implemented with the standard Storm spout and bolt components:   The components in  Architecture  have direct counterparts here. The Query spouts reading from the PubSub layer using plugged-in PubSub consumers make up the Request Processor. The Filter bolts and your plugin for your source of data (shown here using plugged with both the DSL spouts and DSL bolts from  Bullet DSL ) make up the Data Processor. The Join bolt, the Loop bolt and the Result bolt make up the Combiner. There are peripheral components such as the Loop bolts, Tick spouts or the Replay bolts to handle metadata management in the topology.  The red colored lines are the path for the queries that come in through the PubSub, the blue is for the data from your data source and the orange is for metadata and loop-back signals used internally by the backend. The purple lines highlight the most important components where queries and data intermix (the Filter and Join bolts).  The pattern on the lines denote how the data (Storm tuples) is moved to the next component. Dashed indicates a broadcast (sent to all instances of the component), dotted indicates a key grouping (sent to a particular instance based on hashing on a particular field), solid indicates a shuffle (randomly sent to an instance) and dashed-dotted indicates a custom grouping.   What's a Replay?  The Replay and the  pluggable storage  are  optional components  in Bullet on Storm. They exist to replay existing queries if you plugged in a storage layer into the  API . You would use this if you have long running queries and are not tolerant to losing queries for your use-case. Currently, we do not support storage intermediate results in the storage though. For instance, if you restart the topology but have storage and replay configured, you will recreate all the queries on startup but you will lose all intermediate results that were held in memory so far. We plan to add intermediate result storage as well soon!    What's a Tick spout?  The Tick spout component produces Storm tuples at predefined intervals to the Filter and Join bolts. These tuples, called tick tuples, behave like CPU clock cycles for Bullet. Bullet performs all its system related activities on a tick. This includes purging stale queries, emitting left over data for queries, etc. We could have gone the route of having asynchronous threads that do the same thing but this was a far simpler solution. The downside is that Bullet is as fast or as slow as its tick period, which can be configured on launch (defaults to  100 ms ). In practice, this means that your time-based windows need to be at least twice as long as your tick period.  As a practical example of how Bullet uses ticks: when the final data is emitted from the Filter bolts when the query has expired, the Join bolt receiving it waits for 3 (this is configurable) ticks after  its query  expires to collect all the last intermediate results from the Filter bolts. If the tick period is set as high as 5 s, this means that a query will take 3 * 15 or 15 s to get back after its expiry! Setting it to 1 s, makes it 1 * 3 s. Similarly, intermediate windows are buffered (for certain kinds of windowed queries) to collect all results for that window before sending it back to the user.", 
            "title": "Topology"
        }, 
        {
            "location": "/backend/storm-architecture/#data-processing", 
            "text": "Bullet can accept arbitrary sources of data as long as they can be read from Storm. You can either:   Write a Storm spout (or optionally topology) that reads your data from where ever it is (Kafka etc) and  converts it to Bullet Records . See  Quick Start  for an example.  Hook up an existing topology that is doing something else directly to Bullet. You will still write and hook up a component that converts your data into Bullet Records in your existing topology.  Use  Bullet DSL  to configure a DSL spout (and optionally a DSL bolt) to use the DSL interfaces to automatically understand your data source with its data format and convert it to the interface Bullet uses without code.       Option 1  Option 2  Option 3      Write code to read from your data source and convert to Bullet records  Y  Y  N    Write Storm spouts and/or bolts  Y  Y  N    Saves a persistence/pubsub layer  N  Y  N    Separate reading data from converting and allowing fan-out  N  Y  Y    Full control over how data is read, processed and converted to Bullet records  Y  Y  N     Option 3 is generally flexible and is recommended. Having a code-less way to plug into Bullet is the fastest way to get started. We are adding new data sources and formats to the DSL so that we can support more ways to get your data into Bullet. If a connector or converter is not supported in DSL for your specific data source, you can also implement your own. It will save you from having to write Storm spouts or bolts and lets you reuse the Bullet DSL spout and/or bolt.  Regardless of how your data is read, it is then emitted to the Filter bolt. If you have no queries in your system, the Filter bolt will promptly drop all Bullet Records and do nothing. If there are queries in the Filter bolt, the record is checked against each query and if it matches, it is processed by the query. Each query type can choose when to emit its intermediate result based on what window is configured for it. Depending on this, the matched record could be immediately emitted (if it is a RAW query or the intermediate aggregate if anything else) or it could be buffered till a specific time is reached (or the query has expired).", 
            "title": "Data processing"
        }, 
        {
            "location": "/backend/storm-architecture/#request-processing", 
            "text": "The Query spouts fetch Bullet queries through the PubSub layer using the Subscribers provided by the plugged in PubSub layer. The queries received through the PubSub also contain information about the query such as its unique identifier and potentially other metadata. The Query spouts broadcasts the query body to every Filter bolt. Since every Filter bolt has a copy of every query, the shuffled data from the source of data can be compared against the query no matter which particular Filter bolt it ends up at. Each Filter bolt has access to the unique query id and is able to key group by the id to the Join bolt with the intermediate results for the query.  The Query spout also key groups the query and additional query metadata to the Join bolts. This means that the query and the metadata will be end up at exactly one Join bolt.   Key grouping  Technically, Bullet uses a Custom grouping in Storm instead of Key grouping. The Custom grouping does the same thing as what the Key grouping would do in Storm. The reason why we use the Custom grouping is for Replay so that we can deterministically control the Join bolt component that will receive a query and reuse that information for certain aspects of replaying. This is just defensive programming in case Storm changes their key group algorithm to function differently.", 
            "title": "Request processing"
        }, 
        {
            "location": "/backend/storm-architecture/#combining", 
            "text": "Since the data from the Query spout (query and metadata) and the data from all Filter bolts (intermediate results) is key grouped by the unique query id, only one particular Join bolt receives both the query and the intermediate results for a particular query. The Join bolt can then combine the intermediate results and produce a final result. This result is joined (hence the name) along with the metadata for the query and is shuffled to the Result bolts. This bolt then uses the particular Publisher from the plugged in PubSub layer and uses the metadata if it needs to and sends the results back through the PubSub layer to the requestor.   Combining and operations  In order to be able to combine intermediate results and process data in any order, all aggregations that Bullet does need to be associative and have an identity. In other words, they need to be  Monoids . Luckily for us, the  DataSketches  that we use are monoids when exact ( COUNT DISTINCT  and  GROUP BY  actually are commutative monoids). Sketches can be unioned and thus all the aggregations we support -  SUM ,  COUNT ,  MIN ,  MAX ,  AVG ,  COUNT DISTINCT ,  DISTINCT  etc - are monoidal. ( AVG  is monoidal if you store a  SUM  and a  COUNT  instead). When  DISTRIBUTION  and  TOP K  Sketches are approximating, they may end up not being associative since they depend on the distribution of the data but you can think of them this way if you include their defined error functions bounding the result of the operation.    Loop back  We have not mentioned the loop components or the replay bolts. These are mainly used to perform house-keeping within the topology or if you have configured storage/replay. For instance, there is a Rate Limit concept in the Bullet core libraries that if violated in any instance of the query being executed, should cause the query to be killed. Wherever this error originates, it will trickle to the Loop bolt and be looped back through the PubSub, through the Query spout and sent to all components that know about the query. These components will then kill the query as well. We call this a loop because strictly speaking, the topology is a Directed Acyclic Graph and we violate it by making a loop. These are also used to deliver external signals such as killing a query etc from the API or the UI. If you disable windows entirely, the Loop bolt will not be wired up when you launch your Bullet topology.", 
            "title": "Combining"
        }, 
        {
            "location": "/backend/storm-architecture/#scalability", 
            "text": "The topology set up this way scales horizontally and has some nice properties:   If you want to scale for processing more data but the same amount of queries, you only need to scale the components that read your data (the spout reading the data or your custom topology) and the Filter bolts.  If you want to scale for more queries but the same amount of data, you generally need to scale up the Filter bolts. If you need it, you can scale the Query spouts, Join bolts, Loop bolts and Result bolts. You should ensure that your PubSub layer (if you're using the Storm DRPC PubSub layer, then this is the number of DRPC servers in your Storm cluster) can handle the volume of queries and results being sent through it. These components generally have low parallelisms compared to your data processing components since the data volume is generally much higher than your query volume, so this is generally not needed.   See  Scaling for more Queries  and  Scaling for more Data  for more details.   More queries and Filter bolts  If you send more queries to the Filter bolt, it will be limited by at most how many queries a Filter bolt can store and still process data efficiently. Factors like CPU, memory allocations etc for the Filter bolts come in to the picture in addition to the parallelism. Generally, if you have allocated enough Filter bolts to process your data with enough head room, this should let you run hundreds of queries simultaneously before you run into these issues. In practice, since most usage is expected to be on large data volumes and 100s of queries simultaneously, you will need to scale the the Filter bolts out so that they are not slowed down by the large number of queries in each.", 
            "title": "Scalability"
        }, 
        {
            "location": "/backend/storm-setup/", 
            "text": "Bullet on Storm\n\n\nThis section explains how to set up and run Bullet on Storm. If you're using the Storm DRPC PubSub, refer to \nthis section\n for further details.\n\n\nConfiguration\n\n\nBullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in \nbullet_defaults.yaml\n. There are too many to list here. You can find out what these settings do in the comments listed in the defaults.\n\n\nInstallation\n\n\nTo use Bullet, you need to implement a way to read from your data source and convert your data into Bullet Records (bullet-record is a transitive dependency for Bullet and can be found \nin JCenter\n. You have a couple of options in how to get your data into Bullet:\n\n\n\n\nYou can implement a spout (or even a topology) that reads from your data source and emits Bullet Records. You then write a main class that submits the topology with your topology wired in \nusing our submit method\n.\n\n\nUse Bullet DSL to configure a spout (and optionally a bolt) that you provide in the settings to our main class. This will wire up your data source and data format to Bullet without you having to write code!\n\n\n\n\nYou can refer to the \nPros and Cons\n of the various approaches to determine what works best for you.\n\n\nYou need a JVM based project that implements one of the two options above. You include the Bullet artifact and Storm dependencies in your pom.xml or other dependency management system. The artifacts are available through JCenter, so you will need to add the repository.\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\norg.apache.storm\n/groupId\n\n  \nartifactId\nstorm-core\n/artifactId\n\n  \nversion\n${storm.version}\n/version\n\n  \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-storm\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nIf you just need the jar artifact directly, you can download it from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the sources or javadoc. We also package up our test code where we have some helper classes to deal with \nStorm components\n. If you wish to use these to help with testing your topology, you can add another dependency on bullet-storm with \ntype\ntest-jar\n/type\n.\n\n\nIf you are going to use the second option (directly pipe data into Bullet from your Storm topology), then you will need a main class that directly calls the submit method with your wired up topology and the name of the component that is going to emit Bullet Records in that wired up topology. The submit method can be found in \nTopology.java\n. The submit method submits the topology so it should be the last thing you do in your main.\n\n\nIf you are just implementing a spout, see the \nLaunch\n section below on how to use the main class in Bullet to create and submit your topology.\n\n\nStorm topologies are generally launched with \"fat\" jars (jar-with-dependencies), excluding storm itself:\n\n\nplugin\n\n    \ngroupId\norg.apache.maven.plugins\n/groupId\n\n    \nartifactId\nmaven-assembly-plugin\n/artifactId\n\n    \nversion\n2.4\n/version\n\n    \nexecutions\n\n        \nexecution\n\n            \nid\nassemble-all\n/id\n\n            \nphase\npackage\n/phase\n\n            \ngoals\n\n                \ngoal\nsingle\n/goal\n\n            \n/goals\n\n        \n/execution\n\n    \n/executions\n\n    \nconfiguration\n\n        \ndescriptorRefs\n\n            \ndescriptorRef\njar-with-dependencies\n/descriptorRef\n\n        \n/descriptorRefs\n\n    \n/configuration\n\n\n/plugin\n\n\n\n\n\nOlder Storm Versions\n\n\nSince package prefixes changed from \nbacktype.storm\n to \norg.apache.storm\n in Storm 1.0 and above, you will need to get the storm-0.10 version of Bullet if\nyour Storm cluster is still not at 1.0 or higher. You change your dependency to:\n\n\ndependency\n\n    \ngroupId\ncom.yahoo.bullet\n/groupId\n\n    \nartifactId\nbullet-storm-0.10\n/artifactId\n\n    \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nThe jar artifact can be downloaded directly from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the source or javadoc and \ntype\ntest-jar\n/type\n for the test classes as with bullet-storm.\n\n\nAlso, since storm-metrics and the Resource Aware Scheduler are not in Storm versions less than 1.0, there are changes in the Bullet settings. The settings that set the CPU and memory loads do not exist (so the config file does not specify them). The setting to enable the topology scheduler are no longer present (you can still override these settings if you run a custom version of Storm by passing it to the storm jar command. \nSee below\n.) You can take a look the settings file on the storm-0.10 branch in the Git repo.\n\n\nIf for some reason, you are running a version of Storm less than 1.0 that has the RAS back-ported to it and you wish to set the CPU and other settings, you will your own main class that mirrors the master branch of the main class but with backtype.storm packages instead.\n\n\nLaunch\n\n\nIf you have implemented your own main class (option 2 above), you just pass your main class to the storm executable as usual. If you are implementing a spout, here's an example of how you could launch the topology:\n\n\nstorm jar your-fat-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf path/to/the/bullet_settings.yaml \\\n          -c topology.acker.executors=64 \\\n          -c topology.max.spout.pending=10000\n\n\n\n\nAnd in your bullet_settings.yaml, you would have, for example:\n\n\nbullet.topology.bullet.spout.class.name: \nfull.package.prefix.to.your.spout.implementation\n\nbullet.topology.bullet.spout.args: [\narg-to-your-spout-class-for-example-a-path-to-a-config-file\n, \nanother-arg-to-your-spout-class\n]\nbullet.topology.bullet.spout.parallelism: 64\nbullet.topology.bullet.spout.cpu.load: 200.0\nbullet.topology.bullet.spout.memory.on.heap.load: 512.0\nbullet.topology.bullet.spout.memory.off.heap.load: 256.0\n\n\n\n\nYou can pass other arguments to Storm using the -c argument. The example above uses 64 ackers, for example and uses Storm's \nreliable message processing mechanisms\n. Certain components in the Bullet Storm topology cannot be reliable due to how Bullet operates currently. Hundreds of millions of Storm tuples could go into any query running in Bullet and it is intractable to \nanchor\n a single Bullet aggregation to those tuples, particularly when the results are approximate. However, you should enable acking to ensure at least once message deliveries for the hop from your topology (or spout) to the Filter bolts and for the Query spouts to the Filter and Join bolts. Ackers are lightweight so you need not have the same number of tasks as components that ack in your topology so you can tweak it accordingly. The example above also sets max spout pending to control how fast the spout emits. You could use the back-pressure mechanisms in Storm in addition or in lieu of as you choose. We have found that max spout pending gives a much more predictable way of throttling our spouts during catch up or data spikes.\n\n\n\n\nMain Class Arguments\n\n\nIf you run the main class without arguments or pass in the \n--help\n argument, you can see what these arguments mean and what others are supported.\n\n\n\n\nUsing Bullet DSL\n\n\nInstead of implementing your own spout or Topology, you can also use the provided DSL spout (and optionally, DSL bolt) with \nBullet DSL\n. To do so, add the following settings to your YAML configuration:\n\n\nbullet.topology.dsl.spout.enable: true\nbullet.topology.dsl.spout.parallelism:\nbullet.topology.dsl.spout.cpu.load:\nbullet.topology.dsl.spout.memory.on.heap.load:\nbullet.topology.dsl.spout.memory.off.heap.load:\n\nbullet.topology.dsl.bolt.enable: false\nbullet.topology.dsl.bolt.parallelism:\nbullet.topology.dsl.bolt.cpu.load:\nbullet.topology.dsl.bolt.memory.on.heap.load:\nbullet.topology.dsl.bolt.memory.off.heap.load:\n\nbullet.topology.dsl.deserializer.enable: false\n\n\n\n\nIf the DSL bolt is enabled in addition to the spout (the spout is always required!), Storm will read your data in the spout and convert it in the bolt. Without the bolt, reading and converting are done entirely in the spout. If you wish to separate the two by enabling the DSL bolt, you can lower per-worker latencies when data volume is large and scale them independently.\n\n\nThere is also a setting to enable \nBulletDeserializer\n, which is an optional component of Bullet DSL for deserializing data between reading and converting.  \n\n\nSetup\n\n\nThe Bullet Storm jar is not built with Bullet DSL or with other dependencies you may want such as Kafka and Pulsar. Instead, you will have to either add the dependencies (the DSL fat jar and your particular connector dependencies) to the Storm launcher and worker environments or build a fat jar with the dependencies. In Storm 1.2.2+, however, you also have the option of directly adding the following jars to the classpath in the \nstorm jar\n command.\n\n\nKafka\n\n\nKafka Clients 2.1.0\n\n\nPulsar\n\n\nPulsar Client 2.2.1\n\n\nPulsar Client Schema 2.2.1\n\n\nPulsar Protobuf Shaded 2.1.0-incubating\n\n\nExample\n\n\nThe following is an example for Pulsar in Storm 1.2.2+:\n\n\n\nstorm jar bullet-storm-0.9.1.jar \\\n          com.yahoo.bullet.storm.Topology \\\n          --bullet-conf ./bullet_settings.yaml \\\n          --jars \nbullet-dsl-0.1.2.jar,pulsar-client-2.2.1.jar,pulsar-client-schema-2.2.1.jar,protobuf-shaded-2.1.0-incubating.jar\n\n\n\n\n\nStorage and Replay\n\n\nIf you set up the \nStorage layer in the Web Service\n, you can turn on the replaying feature in Bullet on Storm. This wires up the Replay bolts to the topology. This component keeps track of the queries in the backend and replays queries from the Storage layer upon restart or component failure.\n\n\nCurrently, only queries are stored. In the future, the Storage module will also be used for storing intermediate results in addition to queries to accommodate for restarts or component failures without loss of data for executing queries.", 
            "title": "Setup"
        }, 
        {
            "location": "/backend/storm-setup/#bullet-on-storm", 
            "text": "This section explains how to set up and run Bullet on Storm. If you're using the Storm DRPC PubSub, refer to  this section  for further details.", 
            "title": "Bullet on Storm"
        }, 
        {
            "location": "/backend/storm-setup/#configuration", 
            "text": "Bullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in  bullet_defaults.yaml . There are too many to list here. You can find out what these settings do in the comments listed in the defaults.", 
            "title": "Configuration"
        }, 
        {
            "location": "/backend/storm-setup/#installation", 
            "text": "To use Bullet, you need to implement a way to read from your data source and convert your data into Bullet Records (bullet-record is a transitive dependency for Bullet and can be found  in JCenter . You have a couple of options in how to get your data into Bullet:   You can implement a spout (or even a topology) that reads from your data source and emits Bullet Records. You then write a main class that submits the topology with your topology wired in  using our submit method .  Use Bullet DSL to configure a spout (and optionally a bolt) that you provide in the settings to our main class. This will wire up your data source and data format to Bullet without you having to write code!   You can refer to the  Pros and Cons  of the various approaches to determine what works best for you.  You need a JVM based project that implements one of the two options above. You include the Bullet artifact and Storm dependencies in your pom.xml or other dependency management system. The artifacts are available through JCenter, so you will need to add the repository.  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId org.apache.storm /groupId \n   artifactId storm-core /artifactId \n   version ${storm.version} /version \n   scope provided /scope  /dependency  dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-storm /artifactId \n   version ${bullet.version} /version  /dependency   If you just need the jar artifact directly, you can download it from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the sources or javadoc. We also package up our test code where we have some helper classes to deal with  Storm components . If you wish to use these to help with testing your topology, you can add another dependency on bullet-storm with  type test-jar /type .  If you are going to use the second option (directly pipe data into Bullet from your Storm topology), then you will need a main class that directly calls the submit method with your wired up topology and the name of the component that is going to emit Bullet Records in that wired up topology. The submit method can be found in  Topology.java . The submit method submits the topology so it should be the last thing you do in your main.  If you are just implementing a spout, see the  Launch  section below on how to use the main class in Bullet to create and submit your topology.  Storm topologies are generally launched with \"fat\" jars (jar-with-dependencies), excluding storm itself:  plugin \n     groupId org.apache.maven.plugins /groupId \n     artifactId maven-assembly-plugin /artifactId \n     version 2.4 /version \n     executions \n         execution \n             id assemble-all /id \n             phase package /phase \n             goals \n                 goal single /goal \n             /goals \n         /execution \n     /executions \n     configuration \n         descriptorRefs \n             descriptorRef jar-with-dependencies /descriptorRef \n         /descriptorRefs \n     /configuration  /plugin", 
            "title": "Installation"
        }, 
        {
            "location": "/backend/storm-setup/#older-storm-versions", 
            "text": "Since package prefixes changed from  backtype.storm  to  org.apache.storm  in Storm 1.0 and above, you will need to get the storm-0.10 version of Bullet if\nyour Storm cluster is still not at 1.0 or higher. You change your dependency to:  dependency \n     groupId com.yahoo.bullet /groupId \n     artifactId bullet-storm-0.10 /artifactId \n     version ${bullet.version} /version  /dependency   The jar artifact can be downloaded directly from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the source or javadoc and  type test-jar /type  for the test classes as with bullet-storm.  Also, since storm-metrics and the Resource Aware Scheduler are not in Storm versions less than 1.0, there are changes in the Bullet settings. The settings that set the CPU and memory loads do not exist (so the config file does not specify them). The setting to enable the topology scheduler are no longer present (you can still override these settings if you run a custom version of Storm by passing it to the storm jar command.  See below .) You can take a look the settings file on the storm-0.10 branch in the Git repo.  If for some reason, you are running a version of Storm less than 1.0 that has the RAS back-ported to it and you wish to set the CPU and other settings, you will your own main class that mirrors the master branch of the main class but with backtype.storm packages instead.", 
            "title": "Older Storm Versions"
        }, 
        {
            "location": "/backend/storm-setup/#launch", 
            "text": "If you have implemented your own main class (option 2 above), you just pass your main class to the storm executable as usual. If you are implementing a spout, here's an example of how you could launch the topology:  storm jar your-fat-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf path/to/the/bullet_settings.yaml \\\n          -c topology.acker.executors=64 \\\n          -c topology.max.spout.pending=10000  And in your bullet_settings.yaml, you would have, for example:  bullet.topology.bullet.spout.class.name:  full.package.prefix.to.your.spout.implementation \nbullet.topology.bullet.spout.args: [ arg-to-your-spout-class-for-example-a-path-to-a-config-file ,  another-arg-to-your-spout-class ]\nbullet.topology.bullet.spout.parallelism: 64\nbullet.topology.bullet.spout.cpu.load: 200.0\nbullet.topology.bullet.spout.memory.on.heap.load: 512.0\nbullet.topology.bullet.spout.memory.off.heap.load: 256.0  You can pass other arguments to Storm using the -c argument. The example above uses 64 ackers, for example and uses Storm's  reliable message processing mechanisms . Certain components in the Bullet Storm topology cannot be reliable due to how Bullet operates currently. Hundreds of millions of Storm tuples could go into any query running in Bullet and it is intractable to  anchor  a single Bullet aggregation to those tuples, particularly when the results are approximate. However, you should enable acking to ensure at least once message deliveries for the hop from your topology (or spout) to the Filter bolts and for the Query spouts to the Filter and Join bolts. Ackers are lightweight so you need not have the same number of tasks as components that ack in your topology so you can tweak it accordingly. The example above also sets max spout pending to control how fast the spout emits. You could use the back-pressure mechanisms in Storm in addition or in lieu of as you choose. We have found that max spout pending gives a much more predictable way of throttling our spouts during catch up or data spikes.   Main Class Arguments  If you run the main class without arguments or pass in the  --help  argument, you can see what these arguments mean and what others are supported.", 
            "title": "Launch"
        }, 
        {
            "location": "/backend/storm-setup/#using-bullet-dsl", 
            "text": "Instead of implementing your own spout or Topology, you can also use the provided DSL spout (and optionally, DSL bolt) with  Bullet DSL . To do so, add the following settings to your YAML configuration:  bullet.topology.dsl.spout.enable: true\nbullet.topology.dsl.spout.parallelism:\nbullet.topology.dsl.spout.cpu.load:\nbullet.topology.dsl.spout.memory.on.heap.load:\nbullet.topology.dsl.spout.memory.off.heap.load:\n\nbullet.topology.dsl.bolt.enable: false\nbullet.topology.dsl.bolt.parallelism:\nbullet.topology.dsl.bolt.cpu.load:\nbullet.topology.dsl.bolt.memory.on.heap.load:\nbullet.topology.dsl.bolt.memory.off.heap.load:\n\nbullet.topology.dsl.deserializer.enable: false  If the DSL bolt is enabled in addition to the spout (the spout is always required!), Storm will read your data in the spout and convert it in the bolt. Without the bolt, reading and converting are done entirely in the spout. If you wish to separate the two by enabling the DSL bolt, you can lower per-worker latencies when data volume is large and scale them independently.  There is also a setting to enable  BulletDeserializer , which is an optional component of Bullet DSL for deserializing data between reading and converting.", 
            "title": "Using Bullet DSL"
        }, 
        {
            "location": "/backend/storm-setup/#setup", 
            "text": "The Bullet Storm jar is not built with Bullet DSL or with other dependencies you may want such as Kafka and Pulsar. Instead, you will have to either add the dependencies (the DSL fat jar and your particular connector dependencies) to the Storm launcher and worker environments or build a fat jar with the dependencies. In Storm 1.2.2+, however, you also have the option of directly adding the following jars to the classpath in the  storm jar  command.", 
            "title": "Setup"
        }, 
        {
            "location": "/backend/storm-setup/#kafka", 
            "text": "Kafka Clients 2.1.0", 
            "title": "Kafka"
        }, 
        {
            "location": "/backend/storm-setup/#pulsar", 
            "text": "Pulsar Client 2.2.1  Pulsar Client Schema 2.2.1  Pulsar Protobuf Shaded 2.1.0-incubating", 
            "title": "Pulsar"
        }, 
        {
            "location": "/backend/storm-setup/#example", 
            "text": "The following is an example for Pulsar in Storm 1.2.2+:  \nstorm jar bullet-storm-0.9.1.jar \\\n          com.yahoo.bullet.storm.Topology \\\n          --bullet-conf ./bullet_settings.yaml \\\n          --jars  bullet-dsl-0.1.2.jar,pulsar-client-2.2.1.jar,pulsar-client-schema-2.2.1.jar,protobuf-shaded-2.1.0-incubating.jar", 
            "title": "Example"
        }, 
        {
            "location": "/backend/storm-setup/#storage-and-replay", 
            "text": "If you set up the  Storage layer in the Web Service , you can turn on the replaying feature in Bullet on Storm. This wires up the Replay bolts to the topology. This component keeps track of the queries in the backend and replays queries from the Storage layer upon restart or component failure.  Currently, only queries are stored. In the future, the Storage module will also be used for storing intermediate results in addition to queries to accommodate for restarts or component failures without loss of data for executing queries.", 
            "title": "Storage and Replay"
        }, 
        {
            "location": "/backend/storm-performance/", 
            "text": "Tuning and Performance of Bullet in Storm\n\n\nThe performance of a Bullet instance running on a multi-tenant Storm cluster has a lot of independent variables that we could vary and have an effect including:\n\n\n\n\nThe amount of data we consume\n\n\nThe number of simultaneous queries Bullet runs\n\n\nThe kinds of simultaneous queries - \nRAW\n, \nGROUP\n, \nCOUNT DISTINCT\n, etc.\n\n\nVarying parallelisms of the components - increase the parallelisms of Filter bolts disproportionately to others\n\n\nThe hardware configuration of machines\n\n\nThe various Storm versions\n\n\nHow free the cluster is and the kinds of topologies running on the cluster - CPU heavy, Disk/memory heavy, network heavy etc\n\n\nThe source of the data and tuning consumption from it\n\n\n\n\n...and many more.\n\n\nAs a streaming system, the two main features to measure is how much data it operates on and how many queries can you run simultaneously (1 and 2 above). For these results, see [performance][../performance.md]. This section will deal with determining what these are and how they vary. We will focus on 1 and 2, while keeping the others as fixed as possible. This section is to give you some insight as to what to tune to improve performance. This is not meant to be a rigorous benchmark.\n\n\nPrerequisites\n\n\nYou should be familiar with \nStorm\n, \nKafka\n and the \nBullet on Storm architecture\n.\n\n\nHow was this tested?\n\n\nAll tests run here were using \nBullet-Storm 0.4.2\n and \nBullet-Storm 0.4.3\n. We are working with just the Storm piece without going through the Web Service or the UI. The DRPC REST endpoint provided by Storm lets us do just that.\n\n\nThis particular version of Bullet on Storm was \nprior to the architecture shift\n to a PubSub layer but this would be the equivalent to using the Storm DRPC PubSub layer on a newer version of Bullet on Storm. You can replace DRPC spout and PrepareRequest bolt with Query spout and ReturnResults bolt with Result bolt conceptually. The actual implementation of the DRPC based PubSub layer just uses these spout and bolt implementations underneath anyway for the Publishers and Subscribers so the parallelisms and CPU utilizations should map 1-1.\n\n\nUsing the pluggable metrics interface in Bullet on Storm, we captured worker level metrics such as CPU time, Heap usage, GC times and types, sent them to a in-house monitoring service for time-slicing and graphing. The figures shown below use this service.\n\n\nSee \n0.3.0\n for how to plug in your own metrics collection.\n\n\n\n\nOld Version of Bullet\n\n\nThis was tested with a relatively old version of Bullet Storm and has not been updated since. With the latest changes in Storm, Bullet Storm, using a proper PubSub, and partitioning in Bullet Core, the performance is actually a lot better. However, the point of this performance section is to simply conclude that (Spoilers Ahead) scaling out is pretty linear and queries mostly fit into the overhead of reading the data when the simultaneous queries desired is in the hundreds.\n\n\n\n\nTools used\n\n\n\n\njq\n - a nice tool to parse Bullet JSON responses\n\n\ncurl, bash and python - for running and analyzing Bullet queries\n\n\n\n\nCluster\n\n\n\n\nRunning a custom build of Storm - Storm 0.10.2 with Storm 1.0+ features backported. For all intents and purposes, it's Storm 1.0.3\n\n\nThe spec for the machines we were running on:\n\n\n2 x Intel E5-2680v3 (12 core, 24 threads) - One reserved core gives each machine 47 cores from the Storm scheduler point of view\n\n\n256 GB RAM\n\n\n4 TB SATA Disk\n\n\n10 G Network Interface\n\n\n\n\n\n\nMulti-tenant cluster with other topologies running. Average cluster utilization ranged from \n70% - 90%\n\n\nThe DRPC servers in the cluster:\n\n\n2 x Intel E5620 (4 cores, 8 Threads) - 16 cores\n\n\n24 GB RAM\n\n\n1 TB SATA Disk\n\n\n10 G Network Interface\n\n\n\n\n\n\n\n\nData\n\n\n\n\nOur data was read from a Kafka cluster. We test with both Kafka 0.9.0.1 and 0.10.0.1\n\n\nThe Kafka cluster was located within the same datacenter as the Storm cluster - close network proximity gives us some measure of confidence that large data transmission delays aren't a factor.\n\n\nThe data volume can be measured by measuring how many individual records are being produced per second and what the size of the data throughput is per second.\n\n\nThere is variance in the volume of data over time as this is real data. For each of the tests below, the data volume at that time will be provided in this format: \nData: XX R/s and XX MiB/s\n, where each of the numbers are the average for each metric over the hour of when the test was done. \nR/s\n is \nRecords per second\n and \nMiB/s\n is \nMebiBytes per second\n. The data is compressed with a compression ratio of \n1.2\n.\n\n\nOur data schema contained \n92\n fields with \n62\n Strings, \n4\n Longs, \n23\n Maps and \n3\n Lists of Maps. Most of the data is generally present in the Maps and Lists of Maps.\n\n\n\n\nConfiguration\n\n\nHere is the default configuration we used to launch instances of Bullet.\n\n\nbullet.topology.metrics.enable: true\nbullet.topology.metrics.built.in.enable: true\nbullet.topology.metrics.built.in.emit.interval.mapping:\n   bullet_active_queries: 5\n   default: 60\nbullet.topology.metrics.classes:\n  - \npackage.containing.our.custom.class.pushing.metrics\n\nbullet.topology.tick.interval.secs: 1\nbullet.query.default.duration: 30000\nbullet.query.max.duration: 540000\nbullet.query.aggregation.max.size: 512\nbullet.query.aggregation.raw.max.size: 500\nbullet.query.aggregation.distribution.max.points: 200\n\n\n\n\nAny setting not listed here defaults to the defaults in \nbullet_defaults.yaml\n. In particular, \nmetadata collection\n and \ntimestamp injection\n is enabled. \nRAW\n type queries also micro-batch by size 1 (in other words, do not micro-batch).\n\n\nThe parallelisms, CPU and memory settings for the components are listed below.\n\n\nTesting on Kafka 0.9.0.1\n\n\nFor Tests 1 through 4, we read from a Kafka 0.9 cluster with the following configuration for the various Bullet components (unless specified). We use the single spout model to read from the Kafka topic, partitioned into \n64\n partitions.\n\n\nResource utilization\n\n\n\n\n\n\n\n\n\n\n\nComponent\n\n\nParallelism\n\n\nCPU (cores)\n\n\nOn Heap Memory (MiB)\n\n\nOff Heap Memory (MiB)\n\n\nTotal Memory (MiB)\n\n\n\n\n\n\n\n\n\n\nDataSource spout\n\n\n64\n\n\n64\n\n\n768.0\n\n\n192.0\n\n\n61440\n\n\n\n\n\n\nFilter bolt\n\n\n128\n\n\n128\n\n\n384.0\n\n\n192.0\n\n\n73728\n\n\n\n\n\n\nJoin bolt\n\n\n2\n\n\n1\n\n\n384.0\n\n\n192.0\n\n\n1152\n\n\n\n\n\n\nDRPC spout\n\n\n2\n\n\n0.4\n\n\n128.0\n\n\n192.0\n\n\n640\n\n\n\n\n\n\nPrepareRequest bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nReturnResults bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nIMetricsConsumer\n\n\n1\n\n\n0.1\n\n\n128.0\n\n\n0\n\n\n128\n\n\n\n\n\n\nAckers\n\n\n256\n\n\n25.6\n\n\n128.0\n\n\n0\n\n\n32768\n\n\n\n\n\n\nTotal\n\n\n455\n\n\n219.5\n\n\n\n\n\n\n170496\n\n\n\n\n\n\n\n\nWith our ~47 virtual core machines, we would need \n5\n of these machines to run this instance of Bullet reading this data source and supporting a certain number of queries. What this certain number is, we will determine below.\n\n\nThe topology was also launched (command-line args to Storm) with the following Storm settings:\n\n\nstorm jar\n    ...\n    --bullet-spout-parallelism 64\n    --bullet-spout-cpu-load 100.0 \\\n    --bullet-spout-on-heap-memory-load 768.0 \\\n    --bullet-spout-off-heap-memory-load 192.0 \\\n    -c topology.acker.executors=256 \\\n    -c topology.max.spout.pending=20000 \\\n    -c topology.backpressure.enable=false \\\n    -c topology.worker.max.heap.size.mb=4096.0 \\\n    -c topology.worker.gc.childopts=\n-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:NewSize=128m -XX:CMSInitiatingOccupancyFraction=70 -XX:-CMSConcurrentMTEnabled -XX:NewRatio=1\n \\\n    ...\n\n\n\n\n\n\nThe spout parallelism is 64 because it is going to read from a Kafka topic with 64 partitions (any more is meaningless since it cannot be split further). It reads and converts the data into Bullet Records.\n\n\nWe've fanned out from the spouts to the Filter bolts by a ratio of 2. We may or may not need this.\n\n\nWe use \ntopology.max.spout.pending=20000\n to limit the number of in-flight tuples there can be from a DataSource spout instance and throttle it if too many queries are slowing down processing downstream. This is set pretty high to account for catch-up and skew in our Kafka partitions\n\n\nWe have set the max heap size for a worker to \n4 GiB\n since we do not want too large of a worker. If a component dies or a worker is killed by RAS, it will not affect too many other components. It also makes heap dumps etc manageable.\n\n\nWe set \ntopology.worker.gc.childopts\n to use \nParNewGC\n and \nCMS\n. These are our cluster defaults but we are listing them here since this may not be true for all Storm clusters. We have also added the \n-XX:NewRatio=1\n to the defaults since most of our objects are short-lived and having a larger Young Generation reduces our Young Generation GC (ParNew) frequency.\n\n\nWe are using 256 acker tasks. There is acking from the DataSource spout to the Filter bolt and from the DRPCSpout and the PrepareRequestBolt, so about ~130 components will be acking. We could get away with using much less ackers as they are very light-weight.\n\n\n\n\nTest 1: Measuring the minimum latency of Bullet\n\n\nWe are \nrunning this query\n in this test. This \nRAW\n query without any filters will serve to measure the intrinsic delay added by Bullet. The data record pulled out has a timestamp for when the record was emitted into Kafka, Bullet will inject the timestamp into the record when the Filter bolt sends it on and the metadata collection logs timestamps for when the query was received and terminated. Using these, we can measure the end-to-end latency for getting one record through Bullet.\n\n\nResult\n\n\nThe following table shows the timestamps averaged by running \n100\n of these queries. The delays below are shown \nrelative\n to the Query Received timestamp (when the query was received by Bullet at the Join bolt).\n\n\n\n\n\n\n\n\n\n\n\nTimestamp\n\n\nDelay (ms)\n\n\n\n\n\n\n\n\n\n\nKafka Received\n\n\n-710.75\n\n\n\n\n\n\nBullet Filtered\n\n\n-2.16\n\n\n\n\n\n\nQuery Received\n\n\n0\n\n\n\n\n\n\nQuery Finished\n\n\n1.66\n\n\n\n\n\n\n\n\nThe Bullet Filtered timestamp above is negative because the Filter bolt received the query and emitted an arbitrary record \n2.16 ms\n before the Join bolt received the query. The data was submitted into Kafka about \n710.75 ms\n before the query was received by Bullet and that difference is the processing time of Kafka and the time for our spouts to read the data into Bullet.\n\n\nConclusion\n\n\nBullet adds a delay of a few ms - \n1.66 ms\n in the test above - to just pull out a record. This result shows that this is the fastest Bullet can be. It cannot return data any faster than this for meaningful queries.\n\n\nTest 2: Measuring the time to find a record\n\n\nThe \nlast test\n attempted to measure how long Bullet takes to pick out a record. Here we will measure how long it takes to find a record \nthat we generate\n. This is the average of running \n100\n queries across a time interval of 30 minutes trying to filter for a record with a single unique value in a field \nsimilar to this query\n.\n\n\nWe added a timestamp into the record when the record was initially read by the DataSource spout. Using this and the Bullet Filtered timestamp and Query Finished timestamps, we can easily track the record through Bullet.\n\n\nSince we are looking at values in the data, the average data volume across this test was: \nData: 76,000 R/s and 101 MiB/s\n\n\nResult\n\n\n\n\n\n\n\n\n\n\n\nTimestamp\n\n\nDelay (ms)\n\n\n\n\n\n\n\n\n\n\nKafka Received\n\n\n445.7\n\n\n\n\n\n\nBullet Received\n\n\n996.5\n\n\n\n\n\n\nBullet Filtered\n\n\n1003.3\n\n\n\n\n\n\nQuery Received\n\n\n0\n\n\n\n\n\n\nQuery Finished\n\n\n1006.8\n\n\n\n\n\n\n\n\nThe record was emitted into Kafka \n445.81 ms\n after the query was received. The delay is the time it takes for the generated record to flow through our network and into Kafka.\n\n\nBullet received the record \n996.5 ms\n after the query was received. The delay from when Kafka received the record to Bullet received is the delay for Kafka to make the record available for reading.\n\n\nConclusion\n\n\nWe see that Bullet took on average \n1006.8 ms - 996.5 ms\n or \n10.3 ms\n from the time it saw the record first in DataSource spout to finishing up the query and returning it in the Join bolt.\n\n\nTest 3: Measuring the maximum number of parallel \nRAW\n queries\n\n\nThis test runs a query similar to the \nsimple filtering query\n. The query looks for \n10\n records that match the filter or runs for \n30\n seconds, whichever comes first. The average, uncompressed record size was about \n1.8 KiB\n.\n\n\nWhat is meant by maximum?\n\n\nWe want to see how many of these queries we can have running simultaneously till the Filter bolt is unable to process records from the spouts in time. If a Filter bolt is unable to keep up with the rate of data produced by the spouts, our queries will not find all 10 records. Workers may start dying (killed by RAS for exceeding capacity) as well. We will be trying to find the number of queries in parallel that we can run without these happening.\n\n\nThe average data volume across this test was: \nData: 85,000 R/s and 126 MiB/s\n\n\nQuery\n\n\n{\n \nfilters\n : [\n              {\nfield\n : \nid\n, \noperation\n : \n==\n, \nvalues\n: [\nc39ba8cfe8\n]}\n             ],\n \naggregation\n : {\ntype\n : \nLIMIT\n, \nsize\n : 10},\n \nduration\n : 30000\n}\n\n\n\n\nWe will run a certain number of these queries then generate records matching this query. We will validate that we have all 10 records for all queries at the end.\n\n\nWe ran a number of queries in parallel (you may have to use \nulimit\n to change maximum user processes if you specify a large number), then generated data for the queries (the same data) and waited till the results came back. We then used jq to validate that all generated data was present. In a later test, we will actually measure the latency increase caused by increasing the queries.\n\n\n\n\nIt generates a provided number of the \nquery above\n and runs them in parallel against a randomly chosen DRPC server\n\n\nIt generates data for the query\n\n\nIt waits out the rest of the time and uses jq to validate that all the generated data was found\n\n\n\n\nResult\n\n\nWe were able to run 200 queries successfully but 300 and higher started causing our Filter bolts to slow down. This slow down caused our spouts to be throttled and fall behind reading data. This caused the matching data to not show up in time during the queries. Some of our attempts would not return all the expected 10 records.\n\n\nUsing our metrics that were captured using our in-house metrics aggregation system (that our IMetricsConsumer publishes to), let's take a look at the CPU, Heap utilizations.\n\n\nBefore you look at the figures:\n\n\n\n\nAll the figures below are for the same time interval. The X-axis represents time in \n1 minute\n intervals\n\n\nFigure 1\n shows the number of queries running for a time interval\n\n\nThe other figures show a metric across \nall\n the workers (JVMs) in the Storm topology, each running a mix of a components (spouts reading from Kafka, Filter bolts etc)\n\n\nThe majority of the components (excluding ackers) are spouts reading from Kafka or Filter bolts, so the figures can be taken to be primarily describing those workers\n\n\n\n\nFigure 1. Queries running\n\n\n\n\nFigure 2. CPU user-time usage\n\n\n\n\nFigure 3. On Heap usage\n\n\n\n\nFigure 4. Garbage Collection times\n\n\n\n\nFigure 1\n shows that we first ran 100 queries, then 200, then 400 and finally 300. The numbers go over their target because we only added a 2 s buffer in our script. Network and tick delays caused some queries to not be entirely purged before the next set of N simultaneous queries came in.\n\n\nFigure 2\n shows the milliseconds of CPU time used per minute. For example, a value of \n300K ms\n ms for a line (worker) means that the worker used \n300K ms/min\n  or \n300s/60s\n or \n5\n CPU cores (virtual) in that minute.\n\n\nFigure 3\n shows raw numbers for Heap utilizations in bytes.\n\n\nFigure 4\n shows the time spent garbage collecting in ms.\n\n\n\n\nGarbage collection\n\n\nAs we increase the number of queries sent into Bullet, more objects are created in the Filter and Join bolts. These quickly fill up our heap and cause GCs. The zig-zags represents heaps being cleared after GC and filling back up quickly. Also, note that the CPU usage is directly related to the GC times. In other words, performance is pretty much directly correlated with the amount of GC we do.\n\n\n\n\nThe following table summarizes these figures:\n\n\n\n\n\n\n\n\n\n\n\nSimultaneous Queries\n\n\nAverage CPU (ms)\n\n\nAverage Result size\n\n\n\n\n\n\n\n\n\n\n0\n\n\n90K\n\n\nN/A\n\n\n\n\n\n\n100\n\n\n130K\n\n\n10\n\n\n\n\n\n\n200\n\n\n170K\n\n\n10\n\n\n\n\n\n\n300\n\n\n230K\n\n\n8.9\n\n\n\n\n\n\n400\n\n\n270K\n\n\n7.3\n\n\n\n\n\n\n\n\nConclusion\n\n\nWe are able to run somewhere between 200 and 300 \nRAW\n queries simultaneously before losing data.\n\n\nTest 4: Improving the maximum number of simultaneous \nRAW\n queries\n\n\nThe \nlast test\n showed us that the \nstandard configuration\n lets us run somewhere from 200 and 300 \nRAW\n queries. Let's improve that. The \nGC times\n and the \nheap usage\n tell us that a larger heap may help. Also, since each of our machines has 256 GB of RAM and we have a lot of unused heap being wasted per machine (since we use up all the CPU cores), we can be use a lot more heap and not have to GC so much. Let's also try to make the worker even leaner by decreasing the max worker size so that slow components don't block others.\n\n\nChanges:\n\n\n-bullet.topology.filter.bolt.memory.on.heap.load: 384.0\n+bullet.topology.filter.bolt.memory.on.heap.load: 1024.0\n\n- --bullet-spout-on-heap-memory-load 768.0 \\\n+ --bullet-spout-on-heap-memory-load 1280.0 \\\n\n- -c topology.worker.max.heap.size.mb=4096.0 \\\n+ -c topology.worker.max.heap.size.mb=3072.0 \\\n\n\n\n\nOur resource utilization is now:\n\n\n\n\n\n\n\n\n\n\n\nComponent\n\n\nParallelism\n\n\nCPU (cores)\n\n\nOn Heap Memory (MiB)\n\n\nOff Heap Memory (MiB)\n\n\nTotal Memory (MiB)\n\n\n\n\n\n\n\n\n\n\nDataSource spout\n\n\n64\n\n\n64\n\n\n1280.0\n\n\n192.0\n\n\n94208\n\n\n\n\n\n\nFilter bolt\n\n\n128\n\n\n128\n\n\n1024.0\n\n\n192.0\n\n\n155648\n\n\n\n\n\n\nJoin bolt\n\n\n2\n\n\n1\n\n\n384.0\n\n\n192.0\n\n\n1152\n\n\n\n\n\n\nDRPC spout\n\n\n2\n\n\n0.4\n\n\n128.0\n\n\n192.0\n\n\n640\n\n\n\n\n\n\nPrepareRequest bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nReturnResults bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nIMetricsConsumer\n\n\n1\n\n\n0.1\n\n\n128.0\n\n\n0\n\n\n128\n\n\n\n\n\n\nAckers\n\n\n256\n\n\n25.6\n\n\n128.0\n\n\n0\n\n\n32768\n\n\n\n\n\n\nTotal\n\n\n455\n\n\n219.5\n\n\n\n\n\n\n285184\n\n\n\n\n\n\n\n\nOur data volume across this test was: \nData: 84,000 R/s and 124 MiB/s\n\n\nResult\n\n\nWith this configuration, we were able to run 700 queries simultaneously and failed at some where between 700 and 800. See below for why.\n\n\nWe notice that the GC times have improved a lot (down to ~12K ms from ~35K ms in \nFigure 4\n). While our overall CPU usage seems to have gone down since we GC a lot less, remember that our changes to the maximum worker size makes our workers run less components and as a result, use less CPU. This is why there are more lines overall (more workers).\n\n\nFigure 5. Queries running\n\n\n\n\nFigure 6. CPU user-time usage\n\n\n\n\nFigure 7. On Heap usage\n\n\n\n\nFigure 8. Garbage Collection times\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimultaneous Queries\n\n\nAverage CPU (ms)\n\n\nAverage Result size\n\n\n\n\n\n\n\n\n\n\n0\n\n\n50K\n\n\nN/A\n\n\n\n\n\n\n300\n\n\n80K\n\n\n10\n\n\n\n\n\n\n500\n\n\n95K\n\n\n10\n\n\n\n\n\n\n600\n\n\n100K\n\n\n10\n\n\n\n\n\n\n700\n\n\n100K\n\n\n10\n\n\n\n\n\n\n735\n\n\n105K\n\n\n10\n\n\n\n\n\n\n800\n\n\n105K\n\n\n9.19\n\n\n\n\n\n\n\n\nWe seem to cap out at 735 queries. This is actually due how the blocking implementation of Storm DRPC works. Storm DRPC currently dedicates a thread to each DRPC request and does not allow more requests till they are finished. For Bullet, when we ran 800 queries for the test, only the first 735 would even be sent to Bullet. The rest 65 would be sent after some of the first return but all of those 65 would return 0 records because the data that they are looking for had long since been processed. We had \n3\n DRPC servers in the cluster and you could \nscale by adding more of these\n.\n\n\nConclusion\n\n\nWith this change in heap usage, we could get to \n735\n of these queries simultaneously without any issues. We could do more but were limited by the number of DRPC servers in our cluster.\n\n\n\n\n735 is a hard limit then?\n\n\nThis is what our Storm cluster's configuration and our usage of the Storm DRPC limits us to. There is an async implementation for DRPC that we could eventually switch to. If we used another PubSub implementation like Kafka, we would be able to bypass this limit.\n\n\n\n\nTesting on Kafka 0.10.0.1\n\n\nFor this and subsequent tests, we upgraded our Kafka cluster to 0.10. We used the new Kafka consumer APIs to read \nbatches\n of messages instead of a message at a time. We changed our DataSource spout to read batches of messages (raw bytes) instead and added a DataSource bolt that converts each batch message into Bullet records. Switching to this model let us be a lot more efficient in our data reading.\n\n\nTo read more data, we will be trying to read a topic that is a superset of our data set so far and produces up to \n13\n times the number of records (maximum of 1.3 million records/sec) and \n20\n times the size of the data we were reading till now. This Kafka topic has \n256\n partitions.\n\n\nTest 5: Reading more Data\n\n\nOur average data volume across this test was: \nData: 756,000 R/s and 3080 MiB/s\n\n\n\n\n\n\n\n\n\n\n\nComponent\n\n\nParallelism\n\n\nCPU (cores)\n\n\nOn Heap Memory (MiB)\n\n\nOff Heap Memory (MiB)\n\n\nTotal Memory (MiB)\n\n\n\n\n\n\n\n\n\n\nDataSource spout\n\n\n128\n\n\n128\n\n\n1024.0\n\n\n192.0\n\n\n155648\n\n\n\n\n\n\nDataSource bolt\n\n\n256\n\n\n512\n\n\n2580.0\n\n\n192.0\n\n\n709632\n\n\n\n\n\n\nFilter bolt\n\n\n512\n\n\n512\n\n\n1024.0\n\n\n192.0\n\n\n622592\n\n\n\n\n\n\nJoin bolt\n\n\n2\n\n\n1\n\n\n512.0\n\n\n192.0\n\n\n1408\n\n\n\n\n\n\nDRPC spout\n\n\n2\n\n\n0.4\n\n\n128.0\n\n\n192.0\n\n\n640\n\n\n\n\n\n\nPrepareRequest bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nReturnResults bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nIMetricsConsumer\n\n\n4\n\n\n0.4\n\n\n128.0\n\n\n0\n\n\n512\n\n\n\n\n\n\nAckers\n\n\n256\n\n\n25.6\n\n\n128.0\n\n\n0\n\n\n32768\n\n\n\n\n\n\nTotal\n\n\n1162\n\n\n1179.8\n\n\n\n\n\n\n1523840\n\n\n\n\n\n\n\n\nIn terms of \nour machines\n, the CPU is clearly the limiting factor. We end up using about \n25\n machines to run this topology.\n\n\nWe also tweaked our GC and Storm settings. We ran the topology with the following command:\n\n\nstorm jar\n          ...\n          --bullet-conf bullet_settings.yaml \\\n          -c topology.max.spout.pending=30 \\\n          -c topology.backpressure.enable=false \\\n          -c topology.acker.executors=256 \\\n          -c topology.component.resources.onheap.memory.mb=128.0 \\\n          -c topology.worker.max.heap.size.mb=3072.0 \\\n          -c topology.worker.gc.childopts=\n-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:NewSize=128m -XX:NewRatio=2 -XX:SurvivorRatio=6 -XX:+UseCMSInitiatingOccupancyOnly -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=50 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=4\n \\\n          ...\n\n\n\n\nWe capped the GC threads to \n8\n and \n4\n, which helps performance on our 48 core machines when multiple JVMs run on a machine. We also bumped up the Young Generation size (\nNewRatio=2\n) and triggered the CMS collection to run at \n50%\n of Old Generation instead of the default of \n70%\n. Our objects are short-lived in our streaming scenario, this makes sense. We arrived at this after a few iterations, which we'll skip here for brevity.\n\n\n\n\nMax Spout Pending is now 30 ?!\n\n\nWe use \ntopology.max.spout.pending\n as a way to throttle how fast we read from Kafka. There is no acking past the Filter bolt. The maximum number of batch messages we read is \n500\n from Kafka. This makes our true max spout pending: \n500 * 30 = 15,000\n. The tuple that is emitted from the spout is a large tuple that contains up to \n500\n records and we limit up to \n30\n of those to go unacked from any single spout before we throttle it. Since we have \n128\n spouts, we can have \n128 * 15,000\n messages unacked in the topology at any time at the most.\n\n\n\n\nResult\n\n\nWe were able to run over 600 queries with this configuration and failed in the high 600s due to hitting the DRPC limit again. With our improved reading model and GC configs, our base resource footprint has improved from \n50K\n CPU ms/min to around \n25K\n CPU ms/min.\n\n\nFigure 9. Queries running\n\n\n\n\nFigure 10. CPU user-time usage\n\n\n\n\nFigure 11. On Heap usage\n\n\n\n\nFigure 12. Garbage Collection times\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimultaneous Queries\n\n\nAverage CPU (ms)\n\n\nAverage Result size\n\n\n\n\n\n\n\n\n\n\n0\n\n\n25K\n\n\nN/A\n\n\n\n\n\n\n100\n\n\n35K\n\n\n10\n\n\n\n\n\n\n300\n\n\n60K\n\n\n10\n\n\n\n\n\n\n500\n\n\n85K\n\n\n10\n\n\n\n\n\n\n600\n\n\n90K\n\n\n10\n\n\n\n\n\n\n680\n\n\n95K\n\n\n10\n\n\n\n\n\n\n700\n\n\n95K\n\n\n9.79\n\n\n\n\n\n\n\n\nNote that the drops when we were running simultaneous queries in \nFigure 9\n are due to some\nhiccups in our collection mechanism (the collection granularity was not 5s as we configured but higher) and not actually drops in the simultaneous queries.\n\n\nConclusion\n\n\nWe are trying to read a data source that could have\n13\n times more records and \n20\n times more data volume. So we have roughly increased the parallelism of the components reading the data by 10x (\n128 + 512 = 768\n cores to read and convert the data whereas previously we were using \n64\n cores). Once this is fixed and we can read the data comfortably using our DataSource spouts and bolts, we can scale the Filter bolts and other components to accommodate for queries. We set our Filter bolt parallelism (dominates the rest of the components) to \n512\n. We need about \n25\n machines (5 times more than the previous of \n5\n).\n\n\nWith this configuration, we were able to run \n680\n queries simultaneously before we hit the DRPC limit. Since DRPC is a shared resource for the cluster, this limit is slightly lower than the previously observed number possibly due to our test environment being multi-tenant and other topologies using the shared resource.\n\n\n\n\nMeasuring latency in Bullet\n\n\nSo far, we have been using data being delayed long enough as a proxy for queries failing. \nBullet-Storm 0.4.3\n adds an average latency metric computed in the Filter bolts. For the next tests, we add a timestamp in the Data Source spouts when the record is read and this latency metric tells us exactly how long it takes for the record to be matched against a query and acked. By setting a limit for this latency, we can much more accurately measure acceptable performance.\n\n\n\n\nTest 6: Scaling for More Data\n\n\nFor this test, we'll establish how much resources we need to read various data volumes. We want to be able to:\n\n\n\n\nRead the data\n\n\nBe able to catch up data backlogs at \n 5 : 1\n ratio (\n 5s of data in 1 s)\n\n\nSupport \n 400 RAW\n queries\n\n\nThe average latency for reading a record to filtering it be \n 200 ms\n\n\n\n\nFor reading the data, we have to first scale the DataSource spouts and bolts and then set the parallelism of the Filter bolts to support the minimum 400 queries we want at the data volume. We leave the rest of the components at their default values as seen in in \nTest 5\n.\n\n\nTo get various data volumes, we read a large Kafka topic with (256 partitions) with over 1 million R/s and sample various percentages to get less data. The sampling is done in our DataSource spouts.\n\n\nResult\n\n\nThe following table summarizes the results:\n\n\n\n\n\n\n\n\n\n\n\nData (MiB/s, R/s)\n\n\nComponent\n\n\nParallelism\n\n\nCPU cores\n\n\nOn Heap (MiB)\n\n\nTotal CPU cores\n\n\nTotal Memory (MiB)\n\n\n\n\n\n\n\n\n\n\n307, 69700\n\n\n\n\n\n\n\n\n\n\n98.3\n\n\n123648\n\n\n\n\n\n\n\n\nDataSource spout\n\n\n16\n\n\n0.5\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource bolt\n\n\n32\n\n\n2\n\n\n2048\n\n\n\n\n\n\n\n\n\n\n\n\nFilter bolt\n\n\n24\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n920, 216825\n\n\n\n\n\n\n\n\n\n\n242.7\n\n\n281856\n\n\n\n\n\n\n\n\nDataSource spout\n\n\n32\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource bolt\n\n\n72\n\n\n2\n\n\n2048\n\n\n\n\n\n\n\n\n\n\n\n\nFilter bolt\n\n\n64\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n1535, 374370\n\n\n\n\n\n\n\n\n\n\n531.5\n\n\n616192\n\n\n\n\n\n\n\n\nDataSource spout\n\n\n64\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource bolt\n\n\n160\n\n\n2\n\n\n2048\n\n\n\n\n\n\n\n\n\n\n\n\nFilter bolt\n\n\n144\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n2149, 524266\n\n\n\n\n\n\n\n\n\n\n812.3\n\n\n939264\n\n\n\n\n\n\n\n\nDataSource spout\n\n\n72\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource bolt\n\n\n256\n\n\n2\n\n\n2048\n\n\n\n\n\n\n\n\n\n\n\n\nFilter bolt\n\n\n224\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n3070, 724390\n\n\n\n\n\n\n\n\n\n\n997.1\n\n\n1321984\n\n\n\n\n\n\n\n\nDataSource spout\n\n\n96\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource bolt\n\n\n320\n\n\n2\n\n\n2580\n\n\n\n\n\n\n\n\n\n\n\n\nFilter bolt\n\n\n256\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n4024, 1004500\n\n\n\n\n\n\n\n\n\n\n1189.4\n\n\n1582208\n\n\n\n\n\n\n\n\nDataSource spout\n\n\n96\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource bolt\n\n\n384\n\n\n2\n\n\n2580\n\n\n\n\n\n\n\n\n\n\n\n\nFilter bolt\n\n\n320\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nThe following figures graphs how the data volume relates to the total CPU and Memory needed.\n\n\nFigure 13. Data Volume vs CPU\n\n\n\n\nFigure 14. Data Volume vs Memory\n\n\n\n\nAs these figures show, Bullet scales pretty linearly for more data.  For each core of CPU core, we need about 1.2 GiB of Memory. That is the, CPU to Memory ratio is \n1 core : 1.2 GiB\n. The slopes of figures show that the CPU to data ratio is \n1 core : 850 R/s\n or \n1 core : 3.4 MiB/s\n.\n\n\nConclusion\n\n\nThis test demonstrates that Bullet scales pretty linearly for reading more data. We were able to support at least \n400\nqueries with plenty of headroom for each configuration. Each CPU core required about 1.2 GiB of Memory and gave us roughly 800 R/s or 3.4 MiB/s processing capability. Of course, this is particular to our data. As mentioned in the \ndata section\n, most of our data was concentrated in random maps. This means that in memory, the data is generally not contiguous. This creates memory fragmentation and more GC pressure. Depending on your data schema, your performance may be a lot better.\n\n\nTest 7: Scaling for More Queries\n\n\nFor this test, we'll establish how much resources we need to support more queries for a fixed data volume: \nData: 68,400 R/s and 105 MiB/s\n.\n\n\nAs our 3 server DRPC cluster currently does not let us do more than \n680 RAW\n queries, in this test, we will:\n\n\n\n\nVary the number of Filter bolts as they are the primary bottleneck for supporting more queries.\n\n\nTo simplify things, we will only vary the \nparallelism\n and fix the CPU and memory of each Filter bolt to \n1 Core\n and \n1 GiB Memory\n\n\nWe will use \nRAW\n queries as the queries to scale\n\n\nEach \nRAW\n query will run for \n30 s\n and search for \n10\n records that we generate. The query will actually look for \n11\n records to force it to run for the full \n30 s\n. This is because we want to stress the Filter bolt as much as possible. As long as there is a query in the system, the Filter bolt will deserialize and check every record that it processes\n\n\nWe will measure the same filtering latency: the time taken from the record read in DataSource spout to its emission in the Filter bolt. We want the maximum latency to be less than \n200 ms\n\n\n\n\nResults\n\n\nThe following table summarizes the results:\n\n\n\n\n\n\n\n\n\n\n\nFilter bolt Parallelism\n\n\nQueries\n\n\nAverage Latency (ms)\n\n\nStatus\n\n\nTopology CPU (cores)\n\n\nTopology Memory (MiB)\n\n\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n78.3\n\n\n112256\n\n\n\n\n\n\n\n\n1\n\n\n7\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n10\n\n\n8\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n20\n\n\n8\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n30\n\n\n10\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n40\n\n\n12\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n50\n\n\n167\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n60\n\n\n1002\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n\n\n82.3\n\n\n117120\n\n\n\n\n\n\n\n\n40\n\n\n9\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n60\n\n\n9\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n80\n\n\n9\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n100\n\n\n10\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n120\n\n\n14\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n140\n\n\n18\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n160\n\n\n298\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n170\n\n\n2439\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n12\n\n\n\n\n\n\n\n\n86.3\n\n\n121984\n\n\n\n\n\n\n\n\n160\n\n\n13\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n200\n\n\n14\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n240\n\n\n78\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n280\n\n\n274\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n320\n\n\n680\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n360\n\n\n1700\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n400\n\n\n2685\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n16\n\n\n\n\n\n\n\n\n90.3\n\n\n126848\n\n\n\n\n\n\n\n\n300\n\n\n30\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n350\n\n\n44\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n400\n\n\n481\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n450\n\n\n935\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n500\n\n\n1968\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n550\n\n\n3267\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n20\n\n\n\n\n\n\n\n\n94.3\n\n\n131712\n\n\n\n\n\n\n\n\n350\n\n\n15\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n400\n\n\n18\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n450\n\n\n28\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n500\n\n\n40\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n550\n\n\n670\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n600\n\n\n1183\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n650\n\n\n1924\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n24\n\n\n\n\n\n\n\n\n98.3\n\n\n136576\n\n\n\n\n\n\n\n\n450\n\n\n17\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n500\n\n\n22\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n550\n\n\n30\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n600\n\n\n377\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n650\n\n\n490\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n28\n\n\n\n\n\n\n\n\n102.3\n\n\n141440\n\n\n\n\n\n\n\n\n550\n\n\n39\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n600\n\n\n53\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n650\n\n\n468\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n32\n\n\n\n\n\n\n\n\n106.3\n\n\n146304\n\n\n\n\n\n\n\n\n600\n\n\n26\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n650\n\n\n32\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\nThe following figure summarizes the minimum number of CPU cores (which are also the number of Filter bolts) needed to support the the maximum number of \nRAW\n queries with latency \n 200 ms.\n\n\nFigure 15. CPU vs Max Concurrent Queries\n\n\n\n\nThis shows that the queries supported also scale pretty linearly.\n\n\nYou may have noticed how when latency starts to increase, it increases pretty rapidly. This suggests that there is a \nknee\n or \nexponential\n curve for latency. The following figure shows this in the graph of the latency for queries with \n20\n Filter bolts.\n\n\nFigure 16. Max Concurrent Queries vs Latency\n\n\n\n\nConclusion\n\n\nSince Filter bolts tend to be the most CPU intensive of the query processing components, this test measured how scaling Filter bolts affected the number of queries that can be supported. For the fixed data volume, this relationship is linear.", 
            "title": "Performance"
        }, 
        {
            "location": "/backend/storm-performance/#tuning-and-performance-of-bullet-in-storm", 
            "text": "The performance of a Bullet instance running on a multi-tenant Storm cluster has a lot of independent variables that we could vary and have an effect including:   The amount of data we consume  The number of simultaneous queries Bullet runs  The kinds of simultaneous queries -  RAW ,  GROUP ,  COUNT DISTINCT , etc.  Varying parallelisms of the components - increase the parallelisms of Filter bolts disproportionately to others  The hardware configuration of machines  The various Storm versions  How free the cluster is and the kinds of topologies running on the cluster - CPU heavy, Disk/memory heavy, network heavy etc  The source of the data and tuning consumption from it   ...and many more.  As a streaming system, the two main features to measure is how much data it operates on and how many queries can you run simultaneously (1 and 2 above). For these results, see [performance][../performance.md]. This section will deal with determining what these are and how they vary. We will focus on 1 and 2, while keeping the others as fixed as possible. This section is to give you some insight as to what to tune to improve performance. This is not meant to be a rigorous benchmark.", 
            "title": "Tuning and Performance of Bullet in Storm"
        }, 
        {
            "location": "/backend/storm-performance/#prerequisites", 
            "text": "You should be familiar with  Storm ,  Kafka  and the  Bullet on Storm architecture .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/backend/storm-performance/#how-was-this-tested", 
            "text": "All tests run here were using  Bullet-Storm 0.4.2  and  Bullet-Storm 0.4.3 . We are working with just the Storm piece without going through the Web Service or the UI. The DRPC REST endpoint provided by Storm lets us do just that.  This particular version of Bullet on Storm was  prior to the architecture shift  to a PubSub layer but this would be the equivalent to using the Storm DRPC PubSub layer on a newer version of Bullet on Storm. You can replace DRPC spout and PrepareRequest bolt with Query spout and ReturnResults bolt with Result bolt conceptually. The actual implementation of the DRPC based PubSub layer just uses these spout and bolt implementations underneath anyway for the Publishers and Subscribers so the parallelisms and CPU utilizations should map 1-1.  Using the pluggable metrics interface in Bullet on Storm, we captured worker level metrics such as CPU time, Heap usage, GC times and types, sent them to a in-house monitoring service for time-slicing and graphing. The figures shown below use this service.  See  0.3.0  for how to plug in your own metrics collection.   Old Version of Bullet  This was tested with a relatively old version of Bullet Storm and has not been updated since. With the latest changes in Storm, Bullet Storm, using a proper PubSub, and partitioning in Bullet Core, the performance is actually a lot better. However, the point of this performance section is to simply conclude that (Spoilers Ahead) scaling out is pretty linear and queries mostly fit into the overhead of reading the data when the simultaneous queries desired is in the hundreds.", 
            "title": "How was this tested?"
        }, 
        {
            "location": "/backend/storm-performance/#tools-used", 
            "text": "jq  - a nice tool to parse Bullet JSON responses  curl, bash and python - for running and analyzing Bullet queries", 
            "title": "Tools used"
        }, 
        {
            "location": "/backend/storm-performance/#cluster", 
            "text": "Running a custom build of Storm - Storm 0.10.2 with Storm 1.0+ features backported. For all intents and purposes, it's Storm 1.0.3  The spec for the machines we were running on:  2 x Intel E5-2680v3 (12 core, 24 threads) - One reserved core gives each machine 47 cores from the Storm scheduler point of view  256 GB RAM  4 TB SATA Disk  10 G Network Interface    Multi-tenant cluster with other topologies running. Average cluster utilization ranged from  70% - 90%  The DRPC servers in the cluster:  2 x Intel E5620 (4 cores, 8 Threads) - 16 cores  24 GB RAM  1 TB SATA Disk  10 G Network Interface", 
            "title": "Cluster"
        }, 
        {
            "location": "/backend/storm-performance/#data", 
            "text": "Our data was read from a Kafka cluster. We test with both Kafka 0.9.0.1 and 0.10.0.1  The Kafka cluster was located within the same datacenter as the Storm cluster - close network proximity gives us some measure of confidence that large data transmission delays aren't a factor.  The data volume can be measured by measuring how many individual records are being produced per second and what the size of the data throughput is per second.  There is variance in the volume of data over time as this is real data. For each of the tests below, the data volume at that time will be provided in this format:  Data: XX R/s and XX MiB/s , where each of the numbers are the average for each metric over the hour of when the test was done.  R/s  is  Records per second  and  MiB/s  is  MebiBytes per second . The data is compressed with a compression ratio of  1.2 .  Our data schema contained  92  fields with  62  Strings,  4  Longs,  23  Maps and  3  Lists of Maps. Most of the data is generally present in the Maps and Lists of Maps.", 
            "title": "Data"
        }, 
        {
            "location": "/backend/storm-performance/#configuration", 
            "text": "Here is the default configuration we used to launch instances of Bullet.  bullet.topology.metrics.enable: true\nbullet.topology.metrics.built.in.enable: true\nbullet.topology.metrics.built.in.emit.interval.mapping:\n   bullet_active_queries: 5\n   default: 60\nbullet.topology.metrics.classes:\n  -  package.containing.our.custom.class.pushing.metrics \nbullet.topology.tick.interval.secs: 1\nbullet.query.default.duration: 30000\nbullet.query.max.duration: 540000\nbullet.query.aggregation.max.size: 512\nbullet.query.aggregation.raw.max.size: 500\nbullet.query.aggregation.distribution.max.points: 200  Any setting not listed here defaults to the defaults in  bullet_defaults.yaml . In particular,  metadata collection  and  timestamp injection  is enabled.  RAW  type queries also micro-batch by size 1 (in other words, do not micro-batch).  The parallelisms, CPU and memory settings for the components are listed below.", 
            "title": "Configuration"
        }, 
        {
            "location": "/backend/storm-performance/#testing-on-kafka-0901", 
            "text": "For Tests 1 through 4, we read from a Kafka 0.9 cluster with the following configuration for the various Bullet components (unless specified). We use the single spout model to read from the Kafka topic, partitioned into  64  partitions.", 
            "title": "Testing on Kafka 0.9.0.1"
        }, 
        {
            "location": "/backend/storm-performance/#resource-utilization", 
            "text": "Component  Parallelism  CPU (cores)  On Heap Memory (MiB)  Off Heap Memory (MiB)  Total Memory (MiB)      DataSource spout  64  64  768.0  192.0  61440    Filter bolt  128  128  384.0  192.0  73728    Join bolt  2  1  384.0  192.0  1152    DRPC spout  2  0.4  128.0  192.0  640    PrepareRequest bolt  1  0.2  128.0  192.0  320    ReturnResults bolt  1  0.2  128.0  192.0  320    IMetricsConsumer  1  0.1  128.0  0  128    Ackers  256  25.6  128.0  0  32768    Total  455  219.5    170496     With our ~47 virtual core machines, we would need  5  of these machines to run this instance of Bullet reading this data source and supporting a certain number of queries. What this certain number is, we will determine below.  The topology was also launched (command-line args to Storm) with the following Storm settings:  storm jar\n    ...\n    --bullet-spout-parallelism 64\n    --bullet-spout-cpu-load 100.0 \\\n    --bullet-spout-on-heap-memory-load 768.0 \\\n    --bullet-spout-off-heap-memory-load 192.0 \\\n    -c topology.acker.executors=256 \\\n    -c topology.max.spout.pending=20000 \\\n    -c topology.backpressure.enable=false \\\n    -c topology.worker.max.heap.size.mb=4096.0 \\\n    -c topology.worker.gc.childopts= -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:NewSize=128m -XX:CMSInitiatingOccupancyFraction=70 -XX:-CMSConcurrentMTEnabled -XX:NewRatio=1  \\\n    ...   The spout parallelism is 64 because it is going to read from a Kafka topic with 64 partitions (any more is meaningless since it cannot be split further). It reads and converts the data into Bullet Records.  We've fanned out from the spouts to the Filter bolts by a ratio of 2. We may or may not need this.  We use  topology.max.spout.pending=20000  to limit the number of in-flight tuples there can be from a DataSource spout instance and throttle it if too many queries are slowing down processing downstream. This is set pretty high to account for catch-up and skew in our Kafka partitions  We have set the max heap size for a worker to  4 GiB  since we do not want too large of a worker. If a component dies or a worker is killed by RAS, it will not affect too many other components. It also makes heap dumps etc manageable.  We set  topology.worker.gc.childopts  to use  ParNewGC  and  CMS . These are our cluster defaults but we are listing them here since this may not be true for all Storm clusters. We have also added the  -XX:NewRatio=1  to the defaults since most of our objects are short-lived and having a larger Young Generation reduces our Young Generation GC (ParNew) frequency.  We are using 256 acker tasks. There is acking from the DataSource spout to the Filter bolt and from the DRPCSpout and the PrepareRequestBolt, so about ~130 components will be acking. We could get away with using much less ackers as they are very light-weight.", 
            "title": "Resource utilization"
        }, 
        {
            "location": "/backend/storm-performance/#test-1-measuring-the-minimum-latency-of-bullet", 
            "text": "We are  running this query  in this test. This  RAW  query without any filters will serve to measure the intrinsic delay added by Bullet. The data record pulled out has a timestamp for when the record was emitted into Kafka, Bullet will inject the timestamp into the record when the Filter bolt sends it on and the metadata collection logs timestamps for when the query was received and terminated. Using these, we can measure the end-to-end latency for getting one record through Bullet.", 
            "title": "Test 1: Measuring the minimum latency of Bullet"
        }, 
        {
            "location": "/backend/storm-performance/#result", 
            "text": "The following table shows the timestamps averaged by running  100  of these queries. The delays below are shown  relative  to the Query Received timestamp (when the query was received by Bullet at the Join bolt).      Timestamp  Delay (ms)      Kafka Received  -710.75    Bullet Filtered  -2.16    Query Received  0    Query Finished  1.66     The Bullet Filtered timestamp above is negative because the Filter bolt received the query and emitted an arbitrary record  2.16 ms  before the Join bolt received the query. The data was submitted into Kafka about  710.75 ms  before the query was received by Bullet and that difference is the processing time of Kafka and the time for our spouts to read the data into Bullet.", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion", 
            "text": "Bullet adds a delay of a few ms -  1.66 ms  in the test above - to just pull out a record. This result shows that this is the fastest Bullet can be. It cannot return data any faster than this for meaningful queries.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#test-2-measuring-the-time-to-find-a-record", 
            "text": "The  last test  attempted to measure how long Bullet takes to pick out a record. Here we will measure how long it takes to find a record  that we generate . This is the average of running  100  queries across a time interval of 30 minutes trying to filter for a record with a single unique value in a field  similar to this query .  We added a timestamp into the record when the record was initially read by the DataSource spout. Using this and the Bullet Filtered timestamp and Query Finished timestamps, we can easily track the record through Bullet.  Since we are looking at values in the data, the average data volume across this test was:  Data: 76,000 R/s and 101 MiB/s", 
            "title": "Test 2: Measuring the time to find a record"
        }, 
        {
            "location": "/backend/storm-performance/#result_1", 
            "text": "Timestamp  Delay (ms)      Kafka Received  445.7    Bullet Received  996.5    Bullet Filtered  1003.3    Query Received  0    Query Finished  1006.8     The record was emitted into Kafka  445.81 ms  after the query was received. The delay is the time it takes for the generated record to flow through our network and into Kafka.  Bullet received the record  996.5 ms  after the query was received. The delay from when Kafka received the record to Bullet received is the delay for Kafka to make the record available for reading.", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_1", 
            "text": "We see that Bullet took on average  1006.8 ms - 996.5 ms  or  10.3 ms  from the time it saw the record first in DataSource spout to finishing up the query and returning it in the Join bolt.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#test-3-measuring-the-maximum-number-of-parallel-raw-queries", 
            "text": "This test runs a query similar to the  simple filtering query . The query looks for  10  records that match the filter or runs for  30  seconds, whichever comes first. The average, uncompressed record size was about  1.8 KiB .", 
            "title": "Test 3: Measuring the maximum number of parallel RAW queries"
        }, 
        {
            "location": "/backend/storm-performance/#what-is-meant-by-maximum", 
            "text": "We want to see how many of these queries we can have running simultaneously till the Filter bolt is unable to process records from the spouts in time. If a Filter bolt is unable to keep up with the rate of data produced by the spouts, our queries will not find all 10 records. Workers may start dying (killed by RAS for exceeding capacity) as well. We will be trying to find the number of queries in parallel that we can run without these happening.  The average data volume across this test was:  Data: 85,000 R/s and 126 MiB/s", 
            "title": "What is meant by maximum?"
        }, 
        {
            "location": "/backend/storm-performance/#query", 
            "text": "{\n  filters  : [\n              { field  :  id ,  operation  :  == ,  values : [ c39ba8cfe8 ]}\n             ],\n  aggregation  : { type  :  LIMIT ,  size  : 10},\n  duration  : 30000\n}  We will run a certain number of these queries then generate records matching this query. We will validate that we have all 10 records for all queries at the end.  We ran a number of queries in parallel (you may have to use  ulimit  to change maximum user processes if you specify a large number), then generated data for the queries (the same data) and waited till the results came back. We then used jq to validate that all generated data was present. In a later test, we will actually measure the latency increase caused by increasing the queries.   It generates a provided number of the  query above  and runs them in parallel against a randomly chosen DRPC server  It generates data for the query  It waits out the rest of the time and uses jq to validate that all the generated data was found", 
            "title": "Query"
        }, 
        {
            "location": "/backend/storm-performance/#result_2", 
            "text": "We were able to run 200 queries successfully but 300 and higher started causing our Filter bolts to slow down. This slow down caused our spouts to be throttled and fall behind reading data. This caused the matching data to not show up in time during the queries. Some of our attempts would not return all the expected 10 records.  Using our metrics that were captured using our in-house metrics aggregation system (that our IMetricsConsumer publishes to), let's take a look at the CPU, Heap utilizations.  Before you look at the figures:   All the figures below are for the same time interval. The X-axis represents time in  1 minute  intervals  Figure 1  shows the number of queries running for a time interval  The other figures show a metric across  all  the workers (JVMs) in the Storm topology, each running a mix of a components (spouts reading from Kafka, Filter bolts etc)  The majority of the components (excluding ackers) are spouts reading from Kafka or Filter bolts, so the figures can be taken to be primarily describing those workers", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#figure-1-queries-running", 
            "text": "", 
            "title": "Figure 1. Queries running"
        }, 
        {
            "location": "/backend/storm-performance/#figure-2-cpu-user-time-usage", 
            "text": "", 
            "title": "Figure 2. CPU user-time usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-3-on-heap-usage", 
            "text": "", 
            "title": "Figure 3. On Heap usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-4-garbage-collection-times", 
            "text": "Figure 1  shows that we first ran 100 queries, then 200, then 400 and finally 300. The numbers go over their target because we only added a 2 s buffer in our script. Network and tick delays caused some queries to not be entirely purged before the next set of N simultaneous queries came in.  Figure 2  shows the milliseconds of CPU time used per minute. For example, a value of  300K ms  ms for a line (worker) means that the worker used  300K ms/min   or  300s/60s  or  5  CPU cores (virtual) in that minute.  Figure 3  shows raw numbers for Heap utilizations in bytes.  Figure 4  shows the time spent garbage collecting in ms.   Garbage collection  As we increase the number of queries sent into Bullet, more objects are created in the Filter and Join bolts. These quickly fill up our heap and cause GCs. The zig-zags represents heaps being cleared after GC and filling back up quickly. Also, note that the CPU usage is directly related to the GC times. In other words, performance is pretty much directly correlated with the amount of GC we do.   The following table summarizes these figures:      Simultaneous Queries  Average CPU (ms)  Average Result size      0  90K  N/A    100  130K  10    200  170K  10    300  230K  8.9    400  270K  7.3", 
            "title": "Figure 4. Garbage Collection times"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_2", 
            "text": "We are able to run somewhere between 200 and 300  RAW  queries simultaneously before losing data.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#test-4-improving-the-maximum-number-of-simultaneous-raw-queries", 
            "text": "The  last test  showed us that the  standard configuration  lets us run somewhere from 200 and 300  RAW  queries. Let's improve that. The  GC times  and the  heap usage  tell us that a larger heap may help. Also, since each of our machines has 256 GB of RAM and we have a lot of unused heap being wasted per machine (since we use up all the CPU cores), we can be use a lot more heap and not have to GC so much. Let's also try to make the worker even leaner by decreasing the max worker size so that slow components don't block others.  Changes:  -bullet.topology.filter.bolt.memory.on.heap.load: 384.0\n+bullet.topology.filter.bolt.memory.on.heap.load: 1024.0\n\n- --bullet-spout-on-heap-memory-load 768.0 \\\n+ --bullet-spout-on-heap-memory-load 1280.0 \\\n\n- -c topology.worker.max.heap.size.mb=4096.0 \\\n+ -c topology.worker.max.heap.size.mb=3072.0 \\  Our resource utilization is now:      Component  Parallelism  CPU (cores)  On Heap Memory (MiB)  Off Heap Memory (MiB)  Total Memory (MiB)      DataSource spout  64  64  1280.0  192.0  94208    Filter bolt  128  128  1024.0  192.0  155648    Join bolt  2  1  384.0  192.0  1152    DRPC spout  2  0.4  128.0  192.0  640    PrepareRequest bolt  1  0.2  128.0  192.0  320    ReturnResults bolt  1  0.2  128.0  192.0  320    IMetricsConsumer  1  0.1  128.0  0  128    Ackers  256  25.6  128.0  0  32768    Total  455  219.5    285184     Our data volume across this test was:  Data: 84,000 R/s and 124 MiB/s", 
            "title": "Test 4: Improving the maximum number of simultaneous RAW queries"
        }, 
        {
            "location": "/backend/storm-performance/#result_3", 
            "text": "With this configuration, we were able to run 700 queries simultaneously and failed at some where between 700 and 800. See below for why.  We notice that the GC times have improved a lot (down to ~12K ms from ~35K ms in  Figure 4 ). While our overall CPU usage seems to have gone down since we GC a lot less, remember that our changes to the maximum worker size makes our workers run less components and as a result, use less CPU. This is why there are more lines overall (more workers).", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#figure-5-queries-running", 
            "text": "", 
            "title": "Figure 5. Queries running"
        }, 
        {
            "location": "/backend/storm-performance/#figure-6-cpu-user-time-usage", 
            "text": "", 
            "title": "Figure 6. CPU user-time usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-7-on-heap-usage", 
            "text": "", 
            "title": "Figure 7. On Heap usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-8-garbage-collection-times", 
            "text": "Simultaneous Queries  Average CPU (ms)  Average Result size      0  50K  N/A    300  80K  10    500  95K  10    600  100K  10    700  100K  10    735  105K  10    800  105K  9.19     We seem to cap out at 735 queries. This is actually due how the blocking implementation of Storm DRPC works. Storm DRPC currently dedicates a thread to each DRPC request and does not allow more requests till they are finished. For Bullet, when we ran 800 queries for the test, only the first 735 would even be sent to Bullet. The rest 65 would be sent after some of the first return but all of those 65 would return 0 records because the data that they are looking for had long since been processed. We had  3  DRPC servers in the cluster and you could  scale by adding more of these .", 
            "title": "Figure 8. Garbage Collection times"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_3", 
            "text": "With this change in heap usage, we could get to  735  of these queries simultaneously without any issues. We could do more but were limited by the number of DRPC servers in our cluster.   735 is a hard limit then?  This is what our Storm cluster's configuration and our usage of the Storm DRPC limits us to. There is an async implementation for DRPC that we could eventually switch to. If we used another PubSub implementation like Kafka, we would be able to bypass this limit.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#testing-on-kafka-01001", 
            "text": "For this and subsequent tests, we upgraded our Kafka cluster to 0.10. We used the new Kafka consumer APIs to read  batches  of messages instead of a message at a time. We changed our DataSource spout to read batches of messages (raw bytes) instead and added a DataSource bolt that converts each batch message into Bullet records. Switching to this model let us be a lot more efficient in our data reading.  To read more data, we will be trying to read a topic that is a superset of our data set so far and produces up to  13  times the number of records (maximum of 1.3 million records/sec) and  20  times the size of the data we were reading till now. This Kafka topic has  256  partitions.", 
            "title": "Testing on Kafka 0.10.0.1"
        }, 
        {
            "location": "/backend/storm-performance/#test-5-reading-more-data", 
            "text": "Our average data volume across this test was:  Data: 756,000 R/s and 3080 MiB/s      Component  Parallelism  CPU (cores)  On Heap Memory (MiB)  Off Heap Memory (MiB)  Total Memory (MiB)      DataSource spout  128  128  1024.0  192.0  155648    DataSource bolt  256  512  2580.0  192.0  709632    Filter bolt  512  512  1024.0  192.0  622592    Join bolt  2  1  512.0  192.0  1408    DRPC spout  2  0.4  128.0  192.0  640    PrepareRequest bolt  1  0.2  128.0  192.0  320    ReturnResults bolt  1  0.2  128.0  192.0  320    IMetricsConsumer  4  0.4  128.0  0  512    Ackers  256  25.6  128.0  0  32768    Total  1162  1179.8    1523840     In terms of  our machines , the CPU is clearly the limiting factor. We end up using about  25  machines to run this topology.  We also tweaked our GC and Storm settings. We ran the topology with the following command:  storm jar\n          ...\n          --bullet-conf bullet_settings.yaml \\\n          -c topology.max.spout.pending=30 \\\n          -c topology.backpressure.enable=false \\\n          -c topology.acker.executors=256 \\\n          -c topology.component.resources.onheap.memory.mb=128.0 \\\n          -c topology.worker.max.heap.size.mb=3072.0 \\\n          -c topology.worker.gc.childopts= -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:NewSize=128m -XX:NewRatio=2 -XX:SurvivorRatio=6 -XX:+UseCMSInitiatingOccupancyOnly -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=50 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=4  \\\n          ...  We capped the GC threads to  8  and  4 , which helps performance on our 48 core machines when multiple JVMs run on a machine. We also bumped up the Young Generation size ( NewRatio=2 ) and triggered the CMS collection to run at  50%  of Old Generation instead of the default of  70% . Our objects are short-lived in our streaming scenario, this makes sense. We arrived at this after a few iterations, which we'll skip here for brevity.   Max Spout Pending is now 30 ?!  We use  topology.max.spout.pending  as a way to throttle how fast we read from Kafka. There is no acking past the Filter bolt. The maximum number of batch messages we read is  500  from Kafka. This makes our true max spout pending:  500 * 30 = 15,000 . The tuple that is emitted from the spout is a large tuple that contains up to  500  records and we limit up to  30  of those to go unacked from any single spout before we throttle it. Since we have  128  spouts, we can have  128 * 15,000  messages unacked in the topology at any time at the most.", 
            "title": "Test 5: Reading more Data"
        }, 
        {
            "location": "/backend/storm-performance/#result_4", 
            "text": "We were able to run over 600 queries with this configuration and failed in the high 600s due to hitting the DRPC limit again. With our improved reading model and GC configs, our base resource footprint has improved from  50K  CPU ms/min to around  25K  CPU ms/min.", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#figure-9-queries-running", 
            "text": "", 
            "title": "Figure 9. Queries running"
        }, 
        {
            "location": "/backend/storm-performance/#figure-10-cpu-user-time-usage", 
            "text": "", 
            "title": "Figure 10. CPU user-time usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-11-on-heap-usage", 
            "text": "", 
            "title": "Figure 11. On Heap usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-12-garbage-collection-times", 
            "text": "Simultaneous Queries  Average CPU (ms)  Average Result size      0  25K  N/A    100  35K  10    300  60K  10    500  85K  10    600  90K  10    680  95K  10    700  95K  9.79     Note that the drops when we were running simultaneous queries in  Figure 9  are due to some\nhiccups in our collection mechanism (the collection granularity was not 5s as we configured but higher) and not actually drops in the simultaneous queries.", 
            "title": "Figure 12. Garbage Collection times"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_4", 
            "text": "We are trying to read a data source that could have 13  times more records and  20  times more data volume. So we have roughly increased the parallelism of the components reading the data by 10x ( 128 + 512 = 768  cores to read and convert the data whereas previously we were using  64  cores). Once this is fixed and we can read the data comfortably using our DataSource spouts and bolts, we can scale the Filter bolts and other components to accommodate for queries. We set our Filter bolt parallelism (dominates the rest of the components) to  512 . We need about  25  machines (5 times more than the previous of  5 ).  With this configuration, we were able to run  680  queries simultaneously before we hit the DRPC limit. Since DRPC is a shared resource for the cluster, this limit is slightly lower than the previously observed number possibly due to our test environment being multi-tenant and other topologies using the shared resource.   Measuring latency in Bullet  So far, we have been using data being delayed long enough as a proxy for queries failing.  Bullet-Storm 0.4.3  adds an average latency metric computed in the Filter bolts. For the next tests, we add a timestamp in the Data Source spouts when the record is read and this latency metric tells us exactly how long it takes for the record to be matched against a query and acked. By setting a limit for this latency, we can much more accurately measure acceptable performance.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#test-6-scaling-for-more-data", 
            "text": "For this test, we'll establish how much resources we need to read various data volumes. We want to be able to:   Read the data  Be able to catch up data backlogs at   5 : 1  ratio (  5s of data in 1 s)  Support   400 RAW  queries  The average latency for reading a record to filtering it be   200 ms   For reading the data, we have to first scale the DataSource spouts and bolts and then set the parallelism of the Filter bolts to support the minimum 400 queries we want at the data volume. We leave the rest of the components at their default values as seen in in  Test 5 .  To get various data volumes, we read a large Kafka topic with (256 partitions) with over 1 million R/s and sample various percentages to get less data. The sampling is done in our DataSource spouts.", 
            "title": "Test 6: Scaling for More Data"
        }, 
        {
            "location": "/backend/storm-performance/#result_5", 
            "text": "The following table summarizes the results:      Data (MiB/s, R/s)  Component  Parallelism  CPU cores  On Heap (MiB)  Total CPU cores  Total Memory (MiB)      307, 69700      98.3  123648     DataSource spout  16  0.5  1024       DataSource bolt  32  2  2048       Filter bolt  24  1  1024      920, 216825      242.7  281856     DataSource spout  32  1  1024       DataSource bolt  72  2  2048       Filter bolt  64  1  1024      1535, 374370      531.5  616192     DataSource spout  64  1  1024       DataSource bolt  160  2  2048       Filter bolt  144  1  1024      2149, 524266      812.3  939264     DataSource spout  72  1  1024       DataSource bolt  256  2  2048       Filter bolt  224  1  1024      3070, 724390      997.1  1321984     DataSource spout  96  1  1024       DataSource bolt  320  2  2580       Filter bolt  256  1  1024      4024, 1004500      1189.4  1582208     DataSource spout  96  1  1024       DataSource bolt  384  2  2580       Filter bolt  320  1  1024       The following figures graphs how the data volume relates to the total CPU and Memory needed.", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#figure-13-data-volume-vs-cpu", 
            "text": "", 
            "title": "Figure 13. Data Volume vs CPU"
        }, 
        {
            "location": "/backend/storm-performance/#figure-14-data-volume-vs-memory", 
            "text": "As these figures show, Bullet scales pretty linearly for more data.  For each core of CPU core, we need about 1.2 GiB of Memory. That is the, CPU to Memory ratio is  1 core : 1.2 GiB . The slopes of figures show that the CPU to data ratio is  1 core : 850 R/s  or  1 core : 3.4 MiB/s .", 
            "title": "Figure 14. Data Volume vs Memory"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_5", 
            "text": "This test demonstrates that Bullet scales pretty linearly for reading more data. We were able to support at least  400 queries with plenty of headroom for each configuration. Each CPU core required about 1.2 GiB of Memory and gave us roughly 800 R/s or 3.4 MiB/s processing capability. Of course, this is particular to our data. As mentioned in the  data section , most of our data was concentrated in random maps. This means that in memory, the data is generally not contiguous. This creates memory fragmentation and more GC pressure. Depending on your data schema, your performance may be a lot better.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#test-7-scaling-for-more-queries", 
            "text": "For this test, we'll establish how much resources we need to support more queries for a fixed data volume:  Data: 68,400 R/s and 105 MiB/s .  As our 3 server DRPC cluster currently does not let us do more than  680 RAW  queries, in this test, we will:   Vary the number of Filter bolts as they are the primary bottleneck for supporting more queries.  To simplify things, we will only vary the  parallelism  and fix the CPU and memory of each Filter bolt to  1 Core  and  1 GiB Memory  We will use  RAW  queries as the queries to scale  Each  RAW  query will run for  30 s  and search for  10  records that we generate. The query will actually look for  11  records to force it to run for the full  30 s . This is because we want to stress the Filter bolt as much as possible. As long as there is a query in the system, the Filter bolt will deserialize and check every record that it processes  We will measure the same filtering latency: the time taken from the record read in DataSource spout to its emission in the Filter bolt. We want the maximum latency to be less than  200 ms", 
            "title": "Test 7: Scaling for More Queries"
        }, 
        {
            "location": "/backend/storm-performance/#results", 
            "text": "The following table summarizes the results:      Filter bolt Parallelism  Queries  Average Latency (ms)  Status  Topology CPU (cores)  Topology Memory (MiB)      4     78.3  112256     1  7  OK       10  8  OK       20  8  OK       30  10  OK       40  12  OK       50  167  OK       60  1002  FAIL      8     82.3  117120     40  9  OK       60  9  OK       80  9  OK       100  10  OK       120  14  OK       140  18  OK       160  298  FAIL       170  2439  FAIL      12     86.3  121984     160  13  OK       200  14  OK       240  78  OK       280  274  FAIL       320  680  FAIL       360  1700  FAIL       400  2685  FAIL      16     90.3  126848     300  30  OK       350  44  OK       400  481  FAIL       450  935  FAIL       500  1968  FAIL       550  3267  FAIL      20     94.3  131712     350  15  OK       400  18  OK       450  28  OK       500  40  OK       550  670  FAIL       600  1183  FAIL       650  1924  FAIL      24     98.3  136576     450  17  OK       500  22  OK       550  30  OK       600  377  FAIL       650  490  FAIL      28     102.3  141440     550  39  OK       600  53  OK       650  468  FAIL      32     106.3  146304     600  26  OK       650  32  OK       The following figure summarizes the minimum number of CPU cores (which are also the number of Filter bolts) needed to support the the maximum number of  RAW  queries with latency   200 ms.", 
            "title": "Results"
        }, 
        {
            "location": "/backend/storm-performance/#figure-15-cpu-vs-max-concurrent-queries", 
            "text": "This shows that the queries supported also scale pretty linearly.  You may have noticed how when latency starts to increase, it increases pretty rapidly. This suggests that there is a  knee  or  exponential  curve for latency. The following figure shows this in the graph of the latency for queries with  20  Filter bolts.", 
            "title": "Figure 15. CPU vs Max Concurrent Queries"
        }, 
        {
            "location": "/backend/storm-performance/#figure-16-max-concurrent-queries-vs-latency", 
            "text": "", 
            "title": "Figure 16. Max Concurrent Queries vs Latency"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_6", 
            "text": "Since Filter bolts tend to be the most CPU intensive of the query processing components, this test measured how scaling Filter bolts affected the number of queries that can be supported. For the fixed data volume, this relationship is linear.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/spark-architecture/", 
            "text": "Spark architecture\n\n\nThis section describes how the \nBackend architecture\n is implemented in Spark Streaming.\n\n\nData Flow Graph\n\n\nBullet Spark implements the backend piece from the full \nArchitecture\n. It is implemented with Spark Streaming:\n\n\n\n\nThe components in the \nArchitecture\n have direct counterparts here. The Query Receiver reading from the PubSub layer using plugged-in PubSub consumers and the Query Unioning make up the Request Processor. The Filter Streaming and your plugin for your source of data make up the Data Processor. The Join Streaming and the Result Emitter make up the Combiner.\n\n\nThe red lines are the path for the queries that come in through the PubSub, the orange lines are the path for the signals and the blue lines are for the data from your data source. The shapes of the boxes denote the type of transformation/action being executed in the boxes.\n\n\nData processing\n\n\nBullet can accept arbitrary sources of data as long as they can be ingested by Spark. They can be Kafka, Flume, Kinesis, and TCP sockets etc. In order to hook up your data to Bullet Spark, you just need to implement the \nData Producer Trait\n. In your implementation, you can either:\n\n\n\n\nUse \nSpark Streaming built-in sources\n to receive data. Below is a quick example for a direct Kafka source in Scala. You can also write it in Java:\n\n\n\n\nimport com.yahoo.bullet.spark.DataProducer\nimport org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}\n// import all other necessary packages\n\nclass DirectKafkaProducer extends DataProducer {\n  override def getBulletRecordStream(ssc: StreamingContext, config: BulletSparkConfig): DStream[BulletRecord] = {\n    val topics = Array(\ntest\n)\n    val kafkaParams = Map[String, AnyRef](\n      \nbootstrap.servers\n -\n \nserver1, server2\n,\n      \ngroup.id\n -\n \nmygroup\n,\n      \nkey.deserializer\n -\n classOf[StringDeserializer],\n      \nvalue.deserializer\n -\n classOf[ByteArrayDeserializer]\n      // Other kafka params\n      )\n\n    val directKafkaStream = KafkaUtils.createDirectStream[String, Array[Byte]](\n      ssc,\n      LocationStrategies.PreferConsistent,\n      ConsumerStrategies.Subscribe[String, Array[Byte]](topics, kafkaParams))\n\n    directKafkaStream.map(record =\n {\n      // Convert your record to BulletRecord\n    })\n  }\n}\n\n\n\n\n\n\nWrite a \ncustom receiver\n to receive data from any arbitrary data source beyond the ones for which it has built-in support (that is, beyond Flume, Kafka, Kinesis, files, sockets, etc.). See \nexample\n.\n\n\n\n\nAfter receiving your data, you can do any transformations like joins or type conversions in your implementation before emitting to the Filter Streaming stage.\n\n\nThe Filter Streaming stage checks every record from your data source against every query from Query Unioning stage to see if it matches and emits partial results to the Join Streaming stage.\n\n\nRequest processing\n\n\nThe Query Receiver fetches Bullet queries and signals through the PubSub layer using the Subscribers provided by the plugged in PubSub layer. The queries received through the PubSub also contain information about the query such as its unique identifier, potentially other metadata and signals. The Query Unioning collects all active queries by the stateful transformation \nupdateStateByKey\n and broadcasts all the collected queries to every executor for the Filter Streaming stage.\n\n\nThe Query Unioning also sends all active queries and signals to the Join Streaming stage.\n\n\nCombining\n\n\nThe Filter Streaming combines all the partial results from the Filter Streaming by the stateful transformation \nmapWithState\n and produces final results.\n\n\nThe Result Emitter uses the particular publisher from the plugged in PubSub layer to send back results/loop signals.", 
            "title": "Architecture"
        }, 
        {
            "location": "/backend/spark-architecture/#spark-architecture", 
            "text": "This section describes how the  Backend architecture  is implemented in Spark Streaming.", 
            "title": "Spark architecture"
        }, 
        {
            "location": "/backend/spark-architecture/#data-flow-graph", 
            "text": "Bullet Spark implements the backend piece from the full  Architecture . It is implemented with Spark Streaming:   The components in the  Architecture  have direct counterparts here. The Query Receiver reading from the PubSub layer using plugged-in PubSub consumers and the Query Unioning make up the Request Processor. The Filter Streaming and your plugin for your source of data make up the Data Processor. The Join Streaming and the Result Emitter make up the Combiner.  The red lines are the path for the queries that come in through the PubSub, the orange lines are the path for the signals and the blue lines are for the data from your data source. The shapes of the boxes denote the type of transformation/action being executed in the boxes.", 
            "title": "Data Flow Graph"
        }, 
        {
            "location": "/backend/spark-architecture/#data-processing", 
            "text": "Bullet can accept arbitrary sources of data as long as they can be ingested by Spark. They can be Kafka, Flume, Kinesis, and TCP sockets etc. In order to hook up your data to Bullet Spark, you just need to implement the  Data Producer Trait . In your implementation, you can either:   Use  Spark Streaming built-in sources  to receive data. Below is a quick example for a direct Kafka source in Scala. You can also write it in Java:   import com.yahoo.bullet.spark.DataProducer\nimport org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}\n// import all other necessary packages\n\nclass DirectKafkaProducer extends DataProducer {\n  override def getBulletRecordStream(ssc: StreamingContext, config: BulletSparkConfig): DStream[BulletRecord] = {\n    val topics = Array( test )\n    val kafkaParams = Map[String, AnyRef](\n       bootstrap.servers  -   server1, server2 ,\n       group.id  -   mygroup ,\n       key.deserializer  -  classOf[StringDeserializer],\n       value.deserializer  -  classOf[ByteArrayDeserializer]\n      // Other kafka params\n      )\n\n    val directKafkaStream = KafkaUtils.createDirectStream[String, Array[Byte]](\n      ssc,\n      LocationStrategies.PreferConsistent,\n      ConsumerStrategies.Subscribe[String, Array[Byte]](topics, kafkaParams))\n\n    directKafkaStream.map(record =  {\n      // Convert your record to BulletRecord\n    })\n  }\n}   Write a  custom receiver  to receive data from any arbitrary data source beyond the ones for which it has built-in support (that is, beyond Flume, Kafka, Kinesis, files, sockets, etc.). See  example .   After receiving your data, you can do any transformations like joins or type conversions in your implementation before emitting to the Filter Streaming stage.  The Filter Streaming stage checks every record from your data source against every query from Query Unioning stage to see if it matches and emits partial results to the Join Streaming stage.", 
            "title": "Data processing"
        }, 
        {
            "location": "/backend/spark-architecture/#request-processing", 
            "text": "The Query Receiver fetches Bullet queries and signals through the PubSub layer using the Subscribers provided by the plugged in PubSub layer. The queries received through the PubSub also contain information about the query such as its unique identifier, potentially other metadata and signals. The Query Unioning collects all active queries by the stateful transformation  updateStateByKey  and broadcasts all the collected queries to every executor for the Filter Streaming stage.  The Query Unioning also sends all active queries and signals to the Join Streaming stage.", 
            "title": "Request processing"
        }, 
        {
            "location": "/backend/spark-architecture/#combining", 
            "text": "The Filter Streaming combines all the partial results from the Filter Streaming by the stateful transformation  mapWithState  and produces final results.  The Result Emitter uses the particular publisher from the plugged in PubSub layer to send back results/loop signals.", 
            "title": "Combining"
        }, 
        {
            "location": "/backend/spark-setup/", 
            "text": "Bullet on Spark\n\n\nThis section explains how to set up and run Bullet on Spark.\n\n\nConfiguration\n\n\nBullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in \nbullet_spark_defaults.yaml\n. You can find out what these settings do in the comments listed in the defaults.\n\n\nInstallation\n\n\nDownload the Bullet Spark standalone jar from \nJCenter\n.\n\n\nIf you are using Bullet Kafka as pluggable PubSub, you can download the fat jar from \nJCenter\n. Otherwise, you need to plug in your own PubSub jar or use the RESTPubSub built-into bullet-core and turned on in the API.\n\n\nTo use Bullet Spark, you need to implement your own \nData Producer Trait\n with a JVM based project or you can use Bullet DSL (see below). If you choose to implement your own, you have two ways as described in the \nSpark Architecture\n section. You include the Bullet artifact and Spark dependencies in your pom.xml or other equivalent build tools. The artifacts are available through JCenter. Here is an example if you use Scala and Maven:\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\nproperties\n\n    \nscala.version\n2.11.7\n/scala.version\n\n    \nscala.dep.version\n2.11\n/scala.dep.version\n\n    \nspark.version\n2.3.0\n/spark.version\n\n    \nbullet.spark.version\n0.1.1\n/bullet.spark.version\n\n\n/properties\n\n\n\ndependency\n\n    \ngroupId\norg.scala-lang\n/groupId\n\n    \nartifactId\nscala-library\n/artifactId\n\n    \nversion\n${scala.version}\n/version\n\n    \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\ndependency\n\n    \ngroupId\norg.apache.spark\n/groupId\n\n    \nartifactId\nspark-streaming_${scala.dep.version}\n/artifactId\n\n    \nversion\n${spark.version}\n/version\n\n    \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\ndependency\n\n    \ngroupId\norg.apache.spark\n/groupId\n\n    \nartifactId\nspark-core_${scala.dep.version}\n/artifactId\n\n    \nversion\n${spark.version}\n/version\n\n    \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\ndependency\n\n     \ngroupId\ncom.yahoo.bullet\n/groupId\n\n     \nartifactId\nbullet-spark\n/artifactId\n\n     \nversion\n${bullet.spark.version}\n/version\n\n\n/dependency\n\n\n\n\n\nYou can also add \nclassifier\nsources\n/classifier\n or \nclassifier\njavadoc\n/classifier\n if you want the sources or javadoc.\n\n\nUsing Bullet DSL\n\n\nInstead of implementing your own Data Producer, you can also use the provided DSL receiver with \nBullet DSL\n. To do so, add the following settings to your YAML configuration:\n\n\n# If true, enables the Bullet DSL data producer which can be configured to read from a custom data source. If enabled,\n# the DSL data producer is used instead of the producer.\nbullet.spark.dsl.data.producer.enable: true\n\n# If true, enables the deserializer between the Bullet DSL connector and converter components. Otherwise, this step is skipped.\nbullet.spark.dsl.deserializer.enable: false\n\n\n\n\nYou may then use the appropriate DSL settings to point to the class names of the Connector and Converter you wish to use to read from your data source and convert it to BulletRecord instances.\n\n\nThere is also a setting to enable \nBulletDeserializer\n, which is an optional component of Bullet DSL for deserializing data between reading and converting.  \n\n\nLaunch\n\n\nAfter you have implemented your own data producer or used Bullet DSL and built a jar, you could launch your Bullet Spark application. Here is an example command for a \nYARN cluster\n.\n\n\n./bin/spark-submit \\\n    --master yarn \\\n    --deploy-mode cluster \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --queue \nyour queue\n \\\n    --executor-memory 12g \\\n    --executor-cores 2 \\\n    --num-executors 200 \\\n    --driver-cores 2 \\\n    --driver-memory 12g \\\n    --conf spark.streaming.backpressure.enabled=true \\\n    --conf spark.default.parallelism=20 \\\n    ... # other Spark settings\n    --jars /path/to/your-data-producer.jar,/path/to/your-pubsub.jar \\\n    /path/to/downloaded-bullet-spark-standalone.jar \\\n    --bullet-spark-conf /path/to/your-settings.yaml\n\n\n\n\nYou can pass other Spark settings by adding \n--conf key=value\n to the command. For more settings, you can refer to the \nSpark Configuration\n.\n\n\nFor other platforms, you can find the commands from the \nSpark Documentation\n.", 
            "title": "Setup"
        }, 
        {
            "location": "/backend/spark-setup/#bullet-on-spark", 
            "text": "This section explains how to set up and run Bullet on Spark.", 
            "title": "Bullet on Spark"
        }, 
        {
            "location": "/backend/spark-setup/#configuration", 
            "text": "Bullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in  bullet_spark_defaults.yaml . You can find out what these settings do in the comments listed in the defaults.", 
            "title": "Configuration"
        }, 
        {
            "location": "/backend/spark-setup/#installation", 
            "text": "Download the Bullet Spark standalone jar from  JCenter .  If you are using Bullet Kafka as pluggable PubSub, you can download the fat jar from  JCenter . Otherwise, you need to plug in your own PubSub jar or use the RESTPubSub built-into bullet-core and turned on in the API.  To use Bullet Spark, you need to implement your own  Data Producer Trait  with a JVM based project or you can use Bullet DSL (see below). If you choose to implement your own, you have two ways as described in the  Spark Architecture  section. You include the Bullet artifact and Spark dependencies in your pom.xml or other equivalent build tools. The artifacts are available through JCenter. Here is an example if you use Scala and Maven:  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   properties \n     scala.version 2.11.7 /scala.version \n     scala.dep.version 2.11 /scala.dep.version \n     spark.version 2.3.0 /spark.version \n     bullet.spark.version 0.1.1 /bullet.spark.version  /properties  dependency \n     groupId org.scala-lang /groupId \n     artifactId scala-library /artifactId \n     version ${scala.version} /version \n     scope provided /scope  /dependency  dependency \n     groupId org.apache.spark /groupId \n     artifactId spark-streaming_${scala.dep.version} /artifactId \n     version ${spark.version} /version \n     scope provided /scope  /dependency  dependency \n     groupId org.apache.spark /groupId \n     artifactId spark-core_${scala.dep.version} /artifactId \n     version ${spark.version} /version \n     scope provided /scope  /dependency  dependency \n      groupId com.yahoo.bullet /groupId \n      artifactId bullet-spark /artifactId \n      version ${bullet.spark.version} /version  /dependency   You can also add  classifier sources /classifier  or  classifier javadoc /classifier  if you want the sources or javadoc.", 
            "title": "Installation"
        }, 
        {
            "location": "/backend/spark-setup/#using-bullet-dsl", 
            "text": "Instead of implementing your own Data Producer, you can also use the provided DSL receiver with  Bullet DSL . To do so, add the following settings to your YAML configuration:  # If true, enables the Bullet DSL data producer which can be configured to read from a custom data source. If enabled,\n# the DSL data producer is used instead of the producer.\nbullet.spark.dsl.data.producer.enable: true\n\n# If true, enables the deserializer between the Bullet DSL connector and converter components. Otherwise, this step is skipped.\nbullet.spark.dsl.deserializer.enable: false  You may then use the appropriate DSL settings to point to the class names of the Connector and Converter you wish to use to read from your data source and convert it to BulletRecord instances.  There is also a setting to enable  BulletDeserializer , which is an optional component of Bullet DSL for deserializing data between reading and converting.", 
            "title": "Using Bullet DSL"
        }, 
        {
            "location": "/backend/spark-setup/#launch", 
            "text": "After you have implemented your own data producer or used Bullet DSL and built a jar, you could launch your Bullet Spark application. Here is an example command for a  YARN cluster .  ./bin/spark-submit \\\n    --master yarn \\\n    --deploy-mode cluster \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --queue  your queue  \\\n    --executor-memory 12g \\\n    --executor-cores 2 \\\n    --num-executors 200 \\\n    --driver-cores 2 \\\n    --driver-memory 12g \\\n    --conf spark.streaming.backpressure.enabled=true \\\n    --conf spark.default.parallelism=20 \\\n    ... # other Spark settings\n    --jars /path/to/your-data-producer.jar,/path/to/your-pubsub.jar \\\n    /path/to/downloaded-bullet-spark-standalone.jar \\\n    --bullet-spark-conf /path/to/your-settings.yaml  You can pass other Spark settings by adding  --conf key=value  to the command. For more settings, you can refer to the  Spark Configuration .  For other platforms, you can find the commands from the  Spark Documentation .", 
            "title": "Launch"
        }, 
        {
            "location": "/backend/spark-performance/", 
            "text": "Performance of Bullet on Spark\n\n\nThis section describes how we tune the performance of Bullet on Spark. This is not meant to be a rigorous benchmark.\n\n\nPrerequisites\n\n\nYou should be familiar with \nSpark Streaming\n, \nKafka\n and the \nBullet on Spark architecture\n.\n\n\nHow was this tested?\n\n\nAll tests run here were using \nBullet-Spark 0.1.2\n.\n\n\nTools used\n\n\n\n\njq\n - a nice tool to parse Bullet JSON responses\n\n\ncurl, bash and python - for running and analyzing Bullet queries\n\n\nApache JMeter\n - a load testing tool to send multiple queries to the server simultaneously\n\n\n\n\nCluster\n\n\n\n\nHadoop YARN cluster with Apache Spark 2.1.2.12 installed\n\n\nThe spec for the machines we were running on:\n\n\n2 x Intel E5530(4 cores, 8 Threads)\n\n\n24 GB RAM\n\n\n3 TB SATA Disk\n\n\n10 G Network Interface\n\n\n\n\n\n\n\n\nData\n\n\n\n\nOur data was read from a Kafka cluster. Kafka version is 0.10.2.1\n\n\nThe Kafka cluster was located within the same datacenter as the Hadoop YARN cluster - close network proximity gives us some measure of confidence that large data transmission delays aren't a factor.\n\n\nOur data schema contained \n92\n fields with \n62\n Strings, \n4\n Longs, \n23\n Maps and \n3\n Lists of Maps. Most of the data is generally present in the Maps and Lists of Maps.\n\n\nWe tested 2 set of data:\n\n\nThe smaller data was about 36,000 records/s and 43 MB/s\n\n\nThe larger data was about 124,700 records/s and 150 MB/s\n\n\n\n\n\n\n\n\nConfiguration\n\n\nHere are the configurations we used to launch instances of Bullet Spark.\n\n\n\n\nFor the smaller data:\n\n\n\n\nSettings:\n\n\nbullet.spark.batch.duration.ms: 2000\nbullet.spark.receiver.query.block.size: 1\nbullet.result.metadata.enable: true\nbullet.spark.metrics.enabled: true\nbullet.spark.filter.parallel.enabled: true\nbullet.spark.filter.parallelism: 16\nbullet.spark.filter.parallel.query.min.size: 10\nbullet.spark.query.union.checkpoint.duration.multiplier: 20\nbullet.spark.join.checkpoint.duration.multiplier: 20\n\n\n\n\nCommand line:\n\n\n./spark-submit \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --queue default \\\n  --executor-memory 12g \\\n  --executor-cores 2 \\\n  --num-executors 100 \\\n  --driver-cores 2 \\\n  --driver-memory 12g \\\n  --conf spark.streaming.backpressure.enabled=true \\\n  --conf spark.driver.extraJavaOptions=\n-XX:+UseG1GC\n \\\n  --conf spark.executor.extraJavaOptions=\n-XX:+UseG1GC\n \\\n  --conf spark.shuffle.consolidateFiles=true \\\n  --conf spark.dynamicAllocation.enabled=false \\\n  --conf spark.storage.memoryFraction=0.1 \\\n  --conf spark.shuffle.memoryFraction=0.8 \\\n  --conf spark.default.parallelism=20 \\\n  ...\n\n\n\n\n\n\nFor larger Data:\n\n\n\n\nSettings:\n\n\nbullet.spark.batch.duration.ms: 5000\nbullet.spark.receiver.query.block.size: 1\nbullet.result.metadata.enable: true\nbullet.spark.metrics.enabled: true\nbullet.spark.filter.parallel.enabled: true\nbullet.spark.filter.parallelism: 64\nbullet.spark.filter.parallel.query.min.size: 10\nbullet.spark.query.union.checkpoint.duration.multiplier: 20\nbullet.spark.join.checkpoint.duration.multiplier: 20\n\n\n\n\nCommand line:\n\n\n./spark-submit \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --queue default \\\n  --executor-memory 12g \\\n  --executor-cores 2 \\\n  --num-executors 400 \\\n  --driver-cores 2 \\\n  --driver-memory 12g \\\n  --conf spark.streaming.backpressure.enabled=true \\\n  --conf spark.driver.extraJavaOptions=\n-XX:+UseG1GC\n \\\n  --conf spark.executor.extraJavaOptions=\n-XX:+UseG1GC\n \\\n  --conf spark.shuffle.consolidateFiles=true \\\n  --conf spark.dynamicAllocation.enabled=false \\\n  --conf spark.storage.memoryFraction=0.1 \\\n  --conf spark.shuffle.memoryFraction=0.8 \\\n  --conf spark.default.parallelism=50 \\\n  ...\n\n\n\n\nTest 1: Latency of Bullet Spark\n\n\nThis test was done on the smaller data. We used a \nRAW query without any filtering\n to measure the latency added by Bullet Spark. This is not the end-to-end latency for a query. It is the latency from receiving the query to finishing the query, not including the time spent in Kafka. We ran this query 100 times.\n\n\nResult\n\n\nThis graph shows the latency of each attempt:\n\n\n\n\nConclusion\n\n\nThe average latency was 1173 ms. This result shows that this is the fastest Bullet Spark can be. It cannot return data any faster than this for meaningful queries.\n\n\nTest 2: Scalability for smaller data\n\n\nThis test was done on the smaller data. We want to measure how many queries we can have running simultaneously on Bullet Spark. We ran 400, 800, 1500 and 1100 queries each for 10 minutes.\n\n\nResult\n\n\nFigure 1. Spark Streaming UI\n\n\n\n\nFigure 2. Queries running\n\n\n\n\nFigure 3. CPU time\n\n\n\n\nFigure 4. Heap usage\n\n\n\n\nFigure 5. Garbage collection time\n\n\n\n\nFigure 6. Garbage collection count\n\n\n\n\nFigure 1\n shows the Spark Streaming UI when running the test.\n\n\nFigure 2\n shows the simultaneous queries we ran.\n\n\nFigure 3\n shows the milliseconds of CPU time used per minute. For example, a value of \n300K ms\n ms for a line (worker) means that the worker used \n300K ms/min\n  or \n300s/60s\n or \n5\n CPU cores (virtual) in that minute.\n\n\nFigure 4\n shows raw numbers for Heap utilizations in bytes.\n\n\nFigure 5\n shows the time in milliseconds spent for garbage collection per minute.\n\n\nFigure 6\n shows the count of garbage collection events per minute.\n\n\nConclusion\n\n\nThe average processing time for each batch was 1 second 143 ms which was below the batch duration 2 seconds. On average, 1 CPU core and 3GB memory were used in this experiment. CPU and memory usages go slowly up while queries number goes up but they are still within resource limits. We can easily run up to 1500 RAW queries simultaneously in this test.\n\n\nTest 3: Scalability for larger data\n\n\nThis test was done on the larger data. We ran 100, 400, 800 and 600 queries each for 10 minutes.\n\n\nResult\n\n\nFigure 7. Spark stream UI\n\n\n\n\nFigure 8. Queries running\n\n\n\n\nFigure 9. CPU time\n\n\n\n\nFigure 10. Heap usage\n\n\n\n\nFigure 11. Garbage collection time\n\n\n\n\nFigure 12. Garbage collection count\n\n\n\n\nConclusion\n\n\nThe average processing time for each batch was 3 seconds 97 ms which was below the batch duration 5 seconds. On average, 1.2 CPU core and average 5GB memory were used in this experiment. But with queries number goes up, some of the executors memory usage were up to 8-10GB which is close to our resource limits. With more queries running, OOM may happen. So in this experiment, we can only afford up to 800 queries simultaneously.", 
            "title": "Performance"
        }, 
        {
            "location": "/backend/spark-performance/#performance-of-bullet-on-spark", 
            "text": "This section describes how we tune the performance of Bullet on Spark. This is not meant to be a rigorous benchmark.", 
            "title": "Performance of Bullet on Spark"
        }, 
        {
            "location": "/backend/spark-performance/#prerequisites", 
            "text": "You should be familiar with  Spark Streaming ,  Kafka  and the  Bullet on Spark architecture .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/backend/spark-performance/#how-was-this-tested", 
            "text": "All tests run here were using  Bullet-Spark 0.1.2 .", 
            "title": "How was this tested?"
        }, 
        {
            "location": "/backend/spark-performance/#tools-used", 
            "text": "jq  - a nice tool to parse Bullet JSON responses  curl, bash and python - for running and analyzing Bullet queries  Apache JMeter  - a load testing tool to send multiple queries to the server simultaneously", 
            "title": "Tools used"
        }, 
        {
            "location": "/backend/spark-performance/#cluster", 
            "text": "Hadoop YARN cluster with Apache Spark 2.1.2.12 installed  The spec for the machines we were running on:  2 x Intel E5530(4 cores, 8 Threads)  24 GB RAM  3 TB SATA Disk  10 G Network Interface", 
            "title": "Cluster"
        }, 
        {
            "location": "/backend/spark-performance/#data", 
            "text": "Our data was read from a Kafka cluster. Kafka version is 0.10.2.1  The Kafka cluster was located within the same datacenter as the Hadoop YARN cluster - close network proximity gives us some measure of confidence that large data transmission delays aren't a factor.  Our data schema contained  92  fields with  62  Strings,  4  Longs,  23  Maps and  3  Lists of Maps. Most of the data is generally present in the Maps and Lists of Maps.  We tested 2 set of data:  The smaller data was about 36,000 records/s and 43 MB/s  The larger data was about 124,700 records/s and 150 MB/s", 
            "title": "Data"
        }, 
        {
            "location": "/backend/spark-performance/#configuration", 
            "text": "Here are the configurations we used to launch instances of Bullet Spark.   For the smaller data:   Settings:  bullet.spark.batch.duration.ms: 2000\nbullet.spark.receiver.query.block.size: 1\nbullet.result.metadata.enable: true\nbullet.spark.metrics.enabled: true\nbullet.spark.filter.parallel.enabled: true\nbullet.spark.filter.parallelism: 16\nbullet.spark.filter.parallel.query.min.size: 10\nbullet.spark.query.union.checkpoint.duration.multiplier: 20\nbullet.spark.join.checkpoint.duration.multiplier: 20  Command line:  ./spark-submit \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --queue default \\\n  --executor-memory 12g \\\n  --executor-cores 2 \\\n  --num-executors 100 \\\n  --driver-cores 2 \\\n  --driver-memory 12g \\\n  --conf spark.streaming.backpressure.enabled=true \\\n  --conf spark.driver.extraJavaOptions= -XX:+UseG1GC  \\\n  --conf spark.executor.extraJavaOptions= -XX:+UseG1GC  \\\n  --conf spark.shuffle.consolidateFiles=true \\\n  --conf spark.dynamicAllocation.enabled=false \\\n  --conf spark.storage.memoryFraction=0.1 \\\n  --conf spark.shuffle.memoryFraction=0.8 \\\n  --conf spark.default.parallelism=20 \\\n  ...   For larger Data:   Settings:  bullet.spark.batch.duration.ms: 5000\nbullet.spark.receiver.query.block.size: 1\nbullet.result.metadata.enable: true\nbullet.spark.metrics.enabled: true\nbullet.spark.filter.parallel.enabled: true\nbullet.spark.filter.parallelism: 64\nbullet.spark.filter.parallel.query.min.size: 10\nbullet.spark.query.union.checkpoint.duration.multiplier: 20\nbullet.spark.join.checkpoint.duration.multiplier: 20  Command line:  ./spark-submit \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --queue default \\\n  --executor-memory 12g \\\n  --executor-cores 2 \\\n  --num-executors 400 \\\n  --driver-cores 2 \\\n  --driver-memory 12g \\\n  --conf spark.streaming.backpressure.enabled=true \\\n  --conf spark.driver.extraJavaOptions= -XX:+UseG1GC  \\\n  --conf spark.executor.extraJavaOptions= -XX:+UseG1GC  \\\n  --conf spark.shuffle.consolidateFiles=true \\\n  --conf spark.dynamicAllocation.enabled=false \\\n  --conf spark.storage.memoryFraction=0.1 \\\n  --conf spark.shuffle.memoryFraction=0.8 \\\n  --conf spark.default.parallelism=50 \\\n  ...", 
            "title": "Configuration"
        }, 
        {
            "location": "/backend/spark-performance/#test-1-latency-of-bullet-spark", 
            "text": "This test was done on the smaller data. We used a  RAW query without any filtering  to measure the latency added by Bullet Spark. This is not the end-to-end latency for a query. It is the latency from receiving the query to finishing the query, not including the time spent in Kafka. We ran this query 100 times.", 
            "title": "Test 1: Latency of Bullet Spark"
        }, 
        {
            "location": "/backend/spark-performance/#result", 
            "text": "This graph shows the latency of each attempt:", 
            "title": "Result"
        }, 
        {
            "location": "/backend/spark-performance/#conclusion", 
            "text": "The average latency was 1173 ms. This result shows that this is the fastest Bullet Spark can be. It cannot return data any faster than this for meaningful queries.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/spark-performance/#test-2-scalability-for-smaller-data", 
            "text": "This test was done on the smaller data. We want to measure how many queries we can have running simultaneously on Bullet Spark. We ran 400, 800, 1500 and 1100 queries each for 10 minutes.", 
            "title": "Test 2: Scalability for smaller data"
        }, 
        {
            "location": "/backend/spark-performance/#result_1", 
            "text": "", 
            "title": "Result"
        }, 
        {
            "location": "/backend/spark-performance/#figure-1-spark-streaming-ui", 
            "text": "", 
            "title": "Figure 1. Spark Streaming UI"
        }, 
        {
            "location": "/backend/spark-performance/#figure-2-queries-running", 
            "text": "", 
            "title": "Figure 2. Queries running"
        }, 
        {
            "location": "/backend/spark-performance/#figure-3-cpu-time", 
            "text": "", 
            "title": "Figure 3. CPU time"
        }, 
        {
            "location": "/backend/spark-performance/#figure-4-heap-usage", 
            "text": "", 
            "title": "Figure 4. Heap usage"
        }, 
        {
            "location": "/backend/spark-performance/#figure-5-garbage-collection-time", 
            "text": "", 
            "title": "Figure 5. Garbage collection time"
        }, 
        {
            "location": "/backend/spark-performance/#figure-6-garbage-collection-count", 
            "text": "Figure 1  shows the Spark Streaming UI when running the test.  Figure 2  shows the simultaneous queries we ran.  Figure 3  shows the milliseconds of CPU time used per minute. For example, a value of  300K ms  ms for a line (worker) means that the worker used  300K ms/min   or  300s/60s  or  5  CPU cores (virtual) in that minute.  Figure 4  shows raw numbers for Heap utilizations in bytes.  Figure 5  shows the time in milliseconds spent for garbage collection per minute.  Figure 6  shows the count of garbage collection events per minute.", 
            "title": "Figure 6. Garbage collection count"
        }, 
        {
            "location": "/backend/spark-performance/#conclusion_1", 
            "text": "The average processing time for each batch was 1 second 143 ms which was below the batch duration 2 seconds. On average, 1 CPU core and 3GB memory were used in this experiment. CPU and memory usages go slowly up while queries number goes up but they are still within resource limits. We can easily run up to 1500 RAW queries simultaneously in this test.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/spark-performance/#test-3-scalability-for-larger-data", 
            "text": "This test was done on the larger data. We ran 100, 400, 800 and 600 queries each for 10 minutes.", 
            "title": "Test 3: Scalability for larger data"
        }, 
        {
            "location": "/backend/spark-performance/#result_2", 
            "text": "", 
            "title": "Result"
        }, 
        {
            "location": "/backend/spark-performance/#figure-7-spark-stream-ui", 
            "text": "", 
            "title": "Figure 7. Spark stream UI"
        }, 
        {
            "location": "/backend/spark-performance/#figure-8-queries-running", 
            "text": "", 
            "title": "Figure 8. Queries running"
        }, 
        {
            "location": "/backend/spark-performance/#figure-9-cpu-time", 
            "text": "", 
            "title": "Figure 9. CPU time"
        }, 
        {
            "location": "/backend/spark-performance/#figure-10-heap-usage", 
            "text": "", 
            "title": "Figure 10. Heap usage"
        }, 
        {
            "location": "/backend/spark-performance/#figure-11-garbage-collection-time", 
            "text": "", 
            "title": "Figure 11. Garbage collection time"
        }, 
        {
            "location": "/backend/spark-performance/#figure-12-garbage-collection-count", 
            "text": "", 
            "title": "Figure 12. Garbage collection count"
        }, 
        {
            "location": "/backend/spark-performance/#conclusion_2", 
            "text": "The average processing time for each batch was 3 seconds 97 ms which was below the batch duration 5 seconds. On average, 1.2 CPU core and average 5GB memory were used in this experiment. But with queries number goes up, some of the executors memory usage were up to 8-10GB which is close to our resource limits. With more queries running, OOM may happen. So in this experiment, we can only afford up to 800 queries simultaneously.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/pubsub/architecture/", 
            "text": "PubSub Architecture\n\n\nThis section describes how the Publish-Subscribe or \nPubSub layer\n works in Bullet.\n\n\nWhy a PubSub?\n\n\nWhen we initially created Bullet, it was built on \nApache Storm\n and leveraged a feature in it called Storm DRPC to deliver queries to and extract results from the Bullet Backend. Storm DRPC is supported by a set of clusters that are physically part of the Storm cluster and is a shared resource for the cluster. While many other stream processors support some form of RPC and we could support multiple versions of the Web Service for those, it quickly became clear that abstracting the transport layer from the Web Service to the Backend was needed. This was particularly highlighted when we wanted to switch Bullet queries from operating in a request-response model (one response at the end of the query) to a streaming model. Streaming responses back to the user for a query through DRPC would be cumbersome and require a lot of logic to handle. A PubSub system was a natural solution to this. Since DRPC was a shared resource per cluster, we also were \ntying the Backend's scalability\n to a resource that we didn't control.\n\n\nHowever, we didn't want to pick a particular PubSub like Kafka and restrict a user's choice. So, we added a PubSub layer that was generic and entirely pluggable into both the Backend and the Web Service. We would support a select few like \nKafka\n or \nStorm DRPC\n. See \nbelow\n for how to create your own.\n\n\nWith the transport mechanism abstracted out, it opens up a lot of possibilities like implementing Bullet on other stream processors, allowing for the development of \nBullet on Spark\n along with other possible implementations in the future.\n\n\nWhat does it do?\n\n\nA PubSub operates in two contexts:\n\n\n\n\nSubmitting queries and reading results. This is the \nQUERY_SUBMISSION\n context and the PubSub mode for the Web Service\n\n\nReading queries and submitting results. This is the \nQUERY_PROCESSING\n context and the PubSub mode for the Backend\n\n\n\n\nA PubSub provides Publisher and Subscriber instances that, depending on the context it is in, will write and read differently. Publishers in \nQUERY_SUBMISSION\n write queries to your PubSub whereas Publishers in \nQUERY_PROCESSING\n write results. Similarly, Subscribers in \nQUERY_SUBMISSION\n read results, but Subscribers in \nQUERY_PROCESSING\n read queries. A Publisher and Subscriber in a particular context make up read and write halves of the \npipes\n for stream of queries and stream of results.\n\n\nMessages\n\n\nThe PubSub layer does not deal with queries and results and just works on instances of messages of type \ncom.yahoo.bullet.pubsub.PubSubMessage\n. These \nPubSubMessages\n are keyed (\nid\n and \nsequence\n), store content and metadata. This is a light wrapper around the payload and is tailored to work with multiple results per query and support communicating additional information and signals to and from the PubSub in addition to just queries and results.\n\n\nChoosing a PubSub implementation\n\n\nIf you want to use an implementation already built, we currently support:\n\n\n\n\nKafka\n for any Backend\n\n\nPulsar\n for any Backend\n\n\nREST\n for any Backend\n\n\nStorm DRPC\n if you're using Bullet on Storm as your Backend\n\n\n\n\nImplementing your own PubSub\n\n\nThe core of the PubSub interfaces are defined in the \ncore Bullet library\n that you can \ndepend on\n.\n\n\nTo create a PubSub, you should extend the abstract class \ncom.yahoo.bullet.pubsub.PubSub\n and implement the abstract methods for getting instances of Publishers (\ncom.yahoo.bullet.pubsub.Publisher\n) and Subscribers (\ncom.yahoo.bullet.pubsub.Subscriber\n). Depending on how you have configured the Web Service and the Backend, they will call the required methods to get the required number of Publishers or Subscribers to parallelize the reading or the writing. You should ensure that they are thread-safe. They will most likely be tied to your units of parallelisms for the underlying PubSub you are invoking.\n\n\nIf you are running sharded instances of your Web Service, you should ensure that your Publishers writing queries add Metadata to the messages to help the Publishers writing results to send the results back to the right Web Service instance that is waiting for them.\n\n\nReliability\n\n\nYou can choose to make your Publishers and Subscribers as reliable as you want. Both the Web Service and the Backend will call the appropriate reliability methods (\ncommit\n and \nfail\n), but your implementations can choose to be no-ops if you do not want to implement reliability. Alternatively, if you want to make your Subscribers reliable, you could use a simple, in-memory reliable implementation by extending \ncom.yahoo.bullet.pubsub.BufferingSubscriber\n. This keeps track of uncommitted messages in memory up to a configured threshold (does not read more messages if there are this many uncommitted messages left) and re-emits messages on failures using it.\n\n\nCanonical example\n\n\nFor an example of a PubSub implementation, see the \nBullet Kafka PubSub project\n. This is implemented in Java and is a simple implementation that wraps the Kafka client APIs. It supports reliability through the use of the \nBufferingSubscriber\n mentioned above. It allows you to specify one or two Kafka topics for queries and results. It can be sharded across multiple Web Service machines using Kafka topic partitions. See the \nconfiguration\n for details.", 
            "title": "Architecture"
        }, 
        {
            "location": "/pubsub/architecture/#pubsub-architecture", 
            "text": "This section describes how the Publish-Subscribe or  PubSub layer  works in Bullet.", 
            "title": "PubSub Architecture"
        }, 
        {
            "location": "/pubsub/architecture/#why-a-pubsub", 
            "text": "When we initially created Bullet, it was built on  Apache Storm  and leveraged a feature in it called Storm DRPC to deliver queries to and extract results from the Bullet Backend. Storm DRPC is supported by a set of clusters that are physically part of the Storm cluster and is a shared resource for the cluster. While many other stream processors support some form of RPC and we could support multiple versions of the Web Service for those, it quickly became clear that abstracting the transport layer from the Web Service to the Backend was needed. This was particularly highlighted when we wanted to switch Bullet queries from operating in a request-response model (one response at the end of the query) to a streaming model. Streaming responses back to the user for a query through DRPC would be cumbersome and require a lot of logic to handle. A PubSub system was a natural solution to this. Since DRPC was a shared resource per cluster, we also were  tying the Backend's scalability  to a resource that we didn't control.  However, we didn't want to pick a particular PubSub like Kafka and restrict a user's choice. So, we added a PubSub layer that was generic and entirely pluggable into both the Backend and the Web Service. We would support a select few like  Kafka  or  Storm DRPC . See  below  for how to create your own.  With the transport mechanism abstracted out, it opens up a lot of possibilities like implementing Bullet on other stream processors, allowing for the development of  Bullet on Spark  along with other possible implementations in the future.", 
            "title": "Why a PubSub?"
        }, 
        {
            "location": "/pubsub/architecture/#what-does-it-do", 
            "text": "A PubSub operates in two contexts:   Submitting queries and reading results. This is the  QUERY_SUBMISSION  context and the PubSub mode for the Web Service  Reading queries and submitting results. This is the  QUERY_PROCESSING  context and the PubSub mode for the Backend   A PubSub provides Publisher and Subscriber instances that, depending on the context it is in, will write and read differently. Publishers in  QUERY_SUBMISSION  write queries to your PubSub whereas Publishers in  QUERY_PROCESSING  write results. Similarly, Subscribers in  QUERY_SUBMISSION  read results, but Subscribers in  QUERY_PROCESSING  read queries. A Publisher and Subscriber in a particular context make up read and write halves of the  pipes  for stream of queries and stream of results.", 
            "title": "What does it do?"
        }, 
        {
            "location": "/pubsub/architecture/#messages", 
            "text": "The PubSub layer does not deal with queries and results and just works on instances of messages of type  com.yahoo.bullet.pubsub.PubSubMessage . These  PubSubMessages  are keyed ( id  and  sequence ), store content and metadata. This is a light wrapper around the payload and is tailored to work with multiple results per query and support communicating additional information and signals to and from the PubSub in addition to just queries and results.", 
            "title": "Messages"
        }, 
        {
            "location": "/pubsub/architecture/#choosing-a-pubsub-implementation", 
            "text": "If you want to use an implementation already built, we currently support:   Kafka  for any Backend  Pulsar  for any Backend  REST  for any Backend  Storm DRPC  if you're using Bullet on Storm as your Backend", 
            "title": "Choosing a PubSub implementation"
        }, 
        {
            "location": "/pubsub/architecture/#implementing-your-own-pubsub", 
            "text": "The core of the PubSub interfaces are defined in the  core Bullet library  that you can  depend on .  To create a PubSub, you should extend the abstract class  com.yahoo.bullet.pubsub.PubSub  and implement the abstract methods for getting instances of Publishers ( com.yahoo.bullet.pubsub.Publisher ) and Subscribers ( com.yahoo.bullet.pubsub.Subscriber ). Depending on how you have configured the Web Service and the Backend, they will call the required methods to get the required number of Publishers or Subscribers to parallelize the reading or the writing. You should ensure that they are thread-safe. They will most likely be tied to your units of parallelisms for the underlying PubSub you are invoking.  If you are running sharded instances of your Web Service, you should ensure that your Publishers writing queries add Metadata to the messages to help the Publishers writing results to send the results back to the right Web Service instance that is waiting for them.", 
            "title": "Implementing your own PubSub"
        }, 
        {
            "location": "/pubsub/architecture/#reliability", 
            "text": "You can choose to make your Publishers and Subscribers as reliable as you want. Both the Web Service and the Backend will call the appropriate reliability methods ( commit  and  fail ), but your implementations can choose to be no-ops if you do not want to implement reliability. Alternatively, if you want to make your Subscribers reliable, you could use a simple, in-memory reliable implementation by extending  com.yahoo.bullet.pubsub.BufferingSubscriber . This keeps track of uncommitted messages in memory up to a configured threshold (does not read more messages if there are this many uncommitted messages left) and re-emits messages on failures using it.", 
            "title": "Reliability"
        }, 
        {
            "location": "/pubsub/architecture/#canonical-example", 
            "text": "For an example of a PubSub implementation, see the  Bullet Kafka PubSub project . This is implemented in Java and is a simple implementation that wraps the Kafka client APIs. It supports reliability through the use of the  BufferingSubscriber  mentioned above. It allows you to specify one or two Kafka topics for queries and results. It can be sharded across multiple Web Service machines using Kafka topic partitions. See the  configuration  for details.", 
            "title": "Canonical example"
        }, 
        {
            "location": "/pubsub/kafka/", 
            "text": "Kafka PubSub\n\n\nThe Kafka implementation of the Bullet PubSub can be used on any Backend and Web Service. It uses \nApache Kafka\n as the backing PubSub queue and works on all Backends.\n\n\nHow does it work?\n\n\nThe implementation by default asks you to create two topics in a Kafka cluster - one for queries and another for results. The Web Service publishes queries to the queries topic and reads results from the results topic. Similarly, the Backend reads queries from the queries topic and writes results to the results topic. All messages are sent as \nPubSubMessages\n.\n\n\nYou do not need to have two topics. You can have one but you should use multiple partitions and configure your Web Service and Backend to produce to and consume from the right partitions. See the \nsetup\n section for more details.\n\n\n\n\nKafka Client API\n\n\nThe Bullet Kafka implementation uses the Kafka 2.0.0 client APIs. Generally, your forward or backward compatibilities should work as expected.\n\n\n\n\nSetup\n\n\nBefore setting up, you will need a Kafka cluster setup with your topic(s) created. This cluster need only be a couple of machines if it's devoted for Bullet. However, this depends on your query and result volumes. Generally, these are at most a few hundred or thousands of messages per second and a small Kafka cluster will suffice.\n\n\nTo setup Kafka, follow the \ninstructions here\n.\n\n\nPlug into the Backend\n\n\nDepending on how your Backend is built, either add Bullet Kafka to your classpath or include it in your build tool. Head over to our \nreleases page\n for getting the artifacts. If you're adding Bullet Kafka to the classpath instead of building a fat jar, you will need to get the jar with the classifier: \nfat\n since you will need Bullet Kafka and all its dependencies.\n\n\nConfigure the backend to use the Kafka PubSub:\n\n\nbullet.pubsub.context.name: \nQUERY_PROCESSING\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.kafka.KafkaPubSub\n\nbullet.pubsub.kafka.bootstrap.servers: \nserver1:port1,server2:port2,...\n\nbullet.pubsub.kafka.request.topic.name: \nyour-query-topic\n\nbullet.pubsub.kafka.response.topic.name: \nyour-result-topic\n\n\n\n\n\nYou will then need to configure the Publishers and Subscribers. For details on what to configure and what the defaults are, see the \nconfiguration file\n.\n\n\nPlug into the Web Service\n\n\nYou will need to head over to our \nreleases page\n and get the JAR artifact with the \nfat\n classifier. For example, you can download the artifact for the 0.2.0 release \ndirectly from JCenter\n).\n\n\nYou should then plug in this JAR to your Web Service following the instructions \nhere\n.\n\n\nFor configuration, you should \nfollow the steps here\n to create and provide a YAML file to the Web Service. Remember to change the context to \nQUERY_SUBMISSION\n.\n\n\nbullet.pubsub.context.name: \nQUERY_SUBMISSION\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.kafka.KafkaPubSub\n\nbullet.pubsub.kafka.bootstrap.servers: \nserver1:port1,server2:port2,...\n\nbullet.pubsub.kafka.request.topic.name: \nyour-query-topic\n\nbullet.pubsub.kafka.response.topic.name: \nyour-result-topic\n\n\n\n\n\nAs with the Backend, you will then need to configure the Publishers and Subscribers. See the \nconfiguration file\n. Remember that your Subscribers in the Backend are reading what the Producers in your Web Service are producing and vice-versa, so make sure to match up the topics and settings accordingly if you have any custom changes.\n\n\nPassthrough Configuration\n\n\nYou can pass additional Kafka Producer or Consumer properties to the PubSub Publishers and Subscribers by prefixing them with either \nbullet.pubsub.kafka.producer.\n for Producers or \nbullet.pubsub.kafka.consumer.\n for Consumers. The PubSub configuration uses and provides a few defaults for settings it thinks is important to manage. You can tweak them and add others. For a list of properties that you can configure, see the \nProducer\n or \nConsumer\n configs in Kafka.\n\n\n\n\nTypes for the properties\n\n\nAll Kafka properties are better off specified as Strings since Kafka type casts them accordingly. If you provide types, you might run into issues where YAML types do not match what the Kafka client is expecting.\n\n\n\n\nPartitions\n\n\nYou may choose to partition your topics for a couple of reasons:\n\n\n\n\nYou may have one topic for both queries and responses and use partitions as a way to separate them.\n\n\nYou may use two topics and partition one or both for scalability when reading and writing\n\n\nYou may use two topics and partition one or both for sharding across multiple Web Service instances (and multiple instances in your Backend)\n\n\n\n\nYou can accomplish all this with partition maps. You can configure what partitions your Publishers (Web Service or Backend) will write to using \nbullet.pubsub.kafka.request.partitions\n and what partitions your Subscribers will read from using \nbullet.pubsub.kafka.response.partitions\n. Providing these to an instance of the Web Service or the Backend in the YAML file ensures that the Publishers in that instance only write to these request partitions and Subscribers only read from the response partitions. The Publishers will randomly adds one of the response partitions in the messages sent to ensure that the responses only arrive to one of those partitions this instance's Subscribers are waiting on. For more details, see the \nconfiguration file\n.\n\n\nSecurity\n\n\nIf you're using secure Kafka, you will need to do the necessary metadata setup to make sure your principals have access to your topic(s) for reading and writing. If you're using SSL for securing your Kafka cluster, you will need to add the necessary SSL certificates to the keystore for your JVM before launching the Web Service or the Backend.\n\n\nStorm\n\n\nWe have tested Kafka with \nBullet Storm\n using \nKerberos\n from the Storm cluster and SSL from the Web Service. For Kerberos, you may need to add a \nJAAS\n \nconfig file\n to the \nStorm BlobStore\n and add it to your worker JVMs. To do this, you will need a JAAS configuration entry. For example, if your Kerberos KDC is shared with your Storm cluster's KDC, you may be adding a jaas_file.conf with\n\n\nKafkaClient {\n   org.apache.storm.security.auth.kerberos.AutoTGTKrb5LoginModule required\n   serviceName=\nkafka\n;\n};\n\n\n\n\nPut this file into Storm's BlobStore using:\n\n\nstorm blobstore create --file jaas_file.conf --acl o::rwa,u:$USER:rwa --repl-fctr 3 jaas_file.conf\n\n\n\n\nThen while launching your topology, you should provide as arguments to the \nstorm jar\n command, the following arguments:\n\n\n-c topology.blobstore.map='{\njaas_file.conf\n: {} }' \\\n-c topology.worker.childopts=\n-Djava.security.auth.login.config=./jaas_file.conf\n \\\n\n\n\n\nThis will add this to all your worker JVMs. You can refresh Kerberos credentials periodically and push credentials to Storm as \nmentioned here\n.", 
            "title": "Kafka"
        }, 
        {
            "location": "/pubsub/kafka/#kafka-pubsub", 
            "text": "The Kafka implementation of the Bullet PubSub can be used on any Backend and Web Service. It uses  Apache Kafka  as the backing PubSub queue and works on all Backends.", 
            "title": "Kafka PubSub"
        }, 
        {
            "location": "/pubsub/kafka/#how-does-it-work", 
            "text": "The implementation by default asks you to create two topics in a Kafka cluster - one for queries and another for results. The Web Service publishes queries to the queries topic and reads results from the results topic. Similarly, the Backend reads queries from the queries topic and writes results to the results topic. All messages are sent as  PubSubMessages .  You do not need to have two topics. You can have one but you should use multiple partitions and configure your Web Service and Backend to produce to and consume from the right partitions. See the  setup  section for more details.   Kafka Client API  The Bullet Kafka implementation uses the Kafka 2.0.0 client APIs. Generally, your forward or backward compatibilities should work as expected.", 
            "title": "How does it work?"
        }, 
        {
            "location": "/pubsub/kafka/#setup", 
            "text": "Before setting up, you will need a Kafka cluster setup with your topic(s) created. This cluster need only be a couple of machines if it's devoted for Bullet. However, this depends on your query and result volumes. Generally, these are at most a few hundred or thousands of messages per second and a small Kafka cluster will suffice.  To setup Kafka, follow the  instructions here .", 
            "title": "Setup"
        }, 
        {
            "location": "/pubsub/kafka/#plug-into-the-backend", 
            "text": "Depending on how your Backend is built, either add Bullet Kafka to your classpath or include it in your build tool. Head over to our  releases page  for getting the artifacts. If you're adding Bullet Kafka to the classpath instead of building a fat jar, you will need to get the jar with the classifier:  fat  since you will need Bullet Kafka and all its dependencies.  Configure the backend to use the Kafka PubSub:  bullet.pubsub.context.name:  QUERY_PROCESSING \nbullet.pubsub.class.name:  com.yahoo.bullet.kafka.KafkaPubSub \nbullet.pubsub.kafka.bootstrap.servers:  server1:port1,server2:port2,... \nbullet.pubsub.kafka.request.topic.name:  your-query-topic \nbullet.pubsub.kafka.response.topic.name:  your-result-topic   You will then need to configure the Publishers and Subscribers. For details on what to configure and what the defaults are, see the  configuration file .", 
            "title": "Plug into the Backend"
        }, 
        {
            "location": "/pubsub/kafka/#plug-into-the-web-service", 
            "text": "You will need to head over to our  releases page  and get the JAR artifact with the  fat  classifier. For example, you can download the artifact for the 0.2.0 release  directly from JCenter ).  You should then plug in this JAR to your Web Service following the instructions  here .  For configuration, you should  follow the steps here  to create and provide a YAML file to the Web Service. Remember to change the context to  QUERY_SUBMISSION .  bullet.pubsub.context.name:  QUERY_SUBMISSION \nbullet.pubsub.class.name:  com.yahoo.bullet.kafka.KafkaPubSub \nbullet.pubsub.kafka.bootstrap.servers:  server1:port1,server2:port2,... \nbullet.pubsub.kafka.request.topic.name:  your-query-topic \nbullet.pubsub.kafka.response.topic.name:  your-result-topic   As with the Backend, you will then need to configure the Publishers and Subscribers. See the  configuration file . Remember that your Subscribers in the Backend are reading what the Producers in your Web Service are producing and vice-versa, so make sure to match up the topics and settings accordingly if you have any custom changes.", 
            "title": "Plug into the Web Service"
        }, 
        {
            "location": "/pubsub/kafka/#passthrough-configuration", 
            "text": "You can pass additional Kafka Producer or Consumer properties to the PubSub Publishers and Subscribers by prefixing them with either  bullet.pubsub.kafka.producer.  for Producers or  bullet.pubsub.kafka.consumer.  for Consumers. The PubSub configuration uses and provides a few defaults for settings it thinks is important to manage. You can tweak them and add others. For a list of properties that you can configure, see the  Producer  or  Consumer  configs in Kafka.   Types for the properties  All Kafka properties are better off specified as Strings since Kafka type casts them accordingly. If you provide types, you might run into issues where YAML types do not match what the Kafka client is expecting.", 
            "title": "Passthrough Configuration"
        }, 
        {
            "location": "/pubsub/kafka/#partitions", 
            "text": "You may choose to partition your topics for a couple of reasons:   You may have one topic for both queries and responses and use partitions as a way to separate them.  You may use two topics and partition one or both for scalability when reading and writing  You may use two topics and partition one or both for sharding across multiple Web Service instances (and multiple instances in your Backend)   You can accomplish all this with partition maps. You can configure what partitions your Publishers (Web Service or Backend) will write to using  bullet.pubsub.kafka.request.partitions  and what partitions your Subscribers will read from using  bullet.pubsub.kafka.response.partitions . Providing these to an instance of the Web Service or the Backend in the YAML file ensures that the Publishers in that instance only write to these request partitions and Subscribers only read from the response partitions. The Publishers will randomly adds one of the response partitions in the messages sent to ensure that the responses only arrive to one of those partitions this instance's Subscribers are waiting on. For more details, see the  configuration file .", 
            "title": "Partitions"
        }, 
        {
            "location": "/pubsub/kafka/#security", 
            "text": "If you're using secure Kafka, you will need to do the necessary metadata setup to make sure your principals have access to your topic(s) for reading and writing. If you're using SSL for securing your Kafka cluster, you will need to add the necessary SSL certificates to the keystore for your JVM before launching the Web Service or the Backend.", 
            "title": "Security"
        }, 
        {
            "location": "/pubsub/kafka/#storm", 
            "text": "We have tested Kafka with  Bullet Storm  using  Kerberos  from the Storm cluster and SSL from the Web Service. For Kerberos, you may need to add a  JAAS   config file  to the  Storm BlobStore  and add it to your worker JVMs. To do this, you will need a JAAS configuration entry. For example, if your Kerberos KDC is shared with your Storm cluster's KDC, you may be adding a jaas_file.conf with  KafkaClient {\n   org.apache.storm.security.auth.kerberos.AutoTGTKrb5LoginModule required\n   serviceName= kafka ;\n};  Put this file into Storm's BlobStore using:  storm blobstore create --file jaas_file.conf --acl o::rwa,u:$USER:rwa --repl-fctr 3 jaas_file.conf  Then while launching your topology, you should provide as arguments to the  storm jar  command, the following arguments:  -c topology.blobstore.map='{ jaas_file.conf : {} }' \\\n-c topology.worker.childopts= -Djava.security.auth.login.config=./jaas_file.conf  \\  This will add this to all your worker JVMs. You can refresh Kerberos credentials periodically and push credentials to Storm as  mentioned here .", 
            "title": "Storm"
        }, 
        {
            "location": "/pubsub/rest/", 
            "text": "REST PubSub\n\n\nThe REST PubSub implementation is included in bullet-core and can be launched along with the Web Service. If it is enabled, the Web Service will expose two additional REST endpoints, one for reading/writing Bullet queries, and one for reading/writing results.\n\n\nHow does it work?\n\n\nWhen the Web Service receives a query from a user, it will create a PubSubMessage and write the message to the \"query\" RESTPubSub endpoint. This PubSubMessage will contain not only the query, but also some metadata, including the appropriate host/port to which the response should be sent (this is done to allow for multiple Web Services running simultaneously). The query is then stored in memory until the backend does a GET from this endpoint, at which time the query will be served to the backend, and dropped from the queue in memory.\n\n\nOnce the backed has generated the results of the query, it will wrap those results in PubSubMessage. The backend extracts the URL to send the results to from the metadata and writes the results PubSubMessage to the \"results\" REST endpoint with a POST. This result will then be stored in memory until the Web Service does a GET to that endpoint, at which time the Web Service will have the results of the query to send back to the user.\n\n\nSetup\n\n\nTo enable the RESTPubSub and expose the two additional necessary REST endpoints, you must enable the setting:\n\n\nbullet.pubsub.builtin.rest.enabled: true\n\n\n\n\n...in the Web Service \napplication.yaml\n configuration file. This can also be done from the command line when launching the Web Service jar file by adding the command-line option:\n\n\n--bullet.pubsub.builtin.rest.enabled=true\n\n\n\n\nThis will enable the two necessary REST endpoints, the paths for which can be configured in the \napplication.yaml\n file with the settings:\n\n\nbullet.pubsub.builtin.rest.query.path: /pubsub/query\nbullet.pubsub.builtin.rest.result.path: /pubsub/result\n\n\n\n\nPlug into the Backend\n\n\nConfigure the backend to use the REST PubSub:\n\n\nbullet.pubsub.context.name: \nQUERY_PROCESSING\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.pubsub.rest.RESTPubSub\n\nbullet.pubsub.rest.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.urls:\n    - \nhttp://\nAPI_HOST_A\n:9901/api/bullet/pubsub/query\n\n    - \nhttp://\nAPI_HOST_B\n:9901/api/bullet/pubsub/query\n\n\n\n\n\n\n\n\n\n\n\nSetting Name\n\n\nDefault Value\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nbullet.pubsub.context.name\n\n\nQUERY_PROCESSING\n\n\nTells the PubSub that it is running in the backend\n\n\n\n\n\n\nbullet.pubsub.class.name\n\n\ncom.yahoo.bullet.pubsub.rest.RESTPubSub\n\n\nTells Bullet to use this class for its PubSub\n\n\n\n\n\n\nbullet.pubsub.rest.connect.timeout.ms\n\n\n5000\n\n\nSets the HTTP connect timeout to 5 s\n\n\n\n\n\n\nbullet.pubsub.rest.subscriber.max.uncommitted.messages\n\n\n100\n\n\nThis is the maximum number of uncommitted messages allowed to be read by the subscriber before blocking\n\n\n\n\n\n\nbullet.pubsub.rest.query.subscriber.min.wait.ms\n\n\n10\n\n\nThis is used to avoid making an HTTP request too rapidly and overloading the HTTP endpoint. It will force the backend to poll the query endpoint at most once every 10ms\n\n\n\n\n\n\nbullet.pubsub.rest.query.urls\n\n\n\n\nThis should be a list of all the query REST endpoint URLs. If you are only running one Web Service this will only contain one URL (the URL of your Web Service followed by the full path of the query endpoint)\n\n\n\n\n\n\n\n\nPlug into the Web Service\n\n\nConfigure the Web Service to use the REST PubSub:\n\n\nbullet.pubsub.context.name: \nQUERY_SUBMISSION\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.pubsub.rest.RESTPubSub\n\nbullet.pubsub.rest.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.result.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.result.url: \nhttp://localhost:9901/api/bullet/pubsub/result\n\nbullet.pubsub.rest.query.urls:\n    - \nhttp://localhost:9901/api/bullet/pubsub/query\n\n\n\n\n\n\n\n\n\n\n\nSetting Name\n\n\nDefault Value\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nbullet.pubsub.context.name\n\n\nQUERY_SUBMISSION\n\n\nTells the PubSub that it is running in the Web Service\n\n\n\n\n\n\nbullet.pubsub.class.name\n\n\ncom.yahoo.bullet.pubsub.rest.RESTPubSub\n\n\nTells Bullet to use this class for its PubSub\n\n\n\n\n\n\nbullet.pubsub.rest.connect.timeout.ms\n\n\n5000\n\n\nSets the HTTP connect timeout to 5 s\n\n\n\n\n\n\nbullet.pubsub.rest.subscriber.max.uncommitted.messages\n\n\n100\n\n\nThis is the maximum number of uncommitted messages allowed to be read by the subscriber before blocking\n\n\n\n\n\n\nbullet.pubsub.rest.result.subscriber.min.wait.ms\n\n\n10\n\n\nThis is used to avoid making an HTTP request too rapidly and overloading the HTTP endpoint. It will force the Web Service to poll the query endpoint at most once every 10ms\n\n\n\n\n\n\nbullet.pubsub.rest.result.url\n\n\nhttp://localhost:9901/api/bullet/pubsub/result\n\n\nThis is the endpoint from which the Web Service should read results. This is the hostname of that machine the Web Service is running on (or \nlocalhost\n)\n\n\n\n\n\n\nbullet.pubsub.rest.query.urls\n\n\nhttp://localhost:9901/api/bullet/pubsub/query\n\n\nIn the Web Service, this should contain \nexactly one\n URL (the URL to which queries should be written). This is the hostname of that machine the Web Service is running on (or \nlocalhost\n)", 
            "title": "REST"
        }, 
        {
            "location": "/pubsub/rest/#rest-pubsub", 
            "text": "The REST PubSub implementation is included in bullet-core and can be launched along with the Web Service. If it is enabled, the Web Service will expose two additional REST endpoints, one for reading/writing Bullet queries, and one for reading/writing results.", 
            "title": "REST PubSub"
        }, 
        {
            "location": "/pubsub/rest/#how-does-it-work", 
            "text": "When the Web Service receives a query from a user, it will create a PubSubMessage and write the message to the \"query\" RESTPubSub endpoint. This PubSubMessage will contain not only the query, but also some metadata, including the appropriate host/port to which the response should be sent (this is done to allow for multiple Web Services running simultaneously). The query is then stored in memory until the backend does a GET from this endpoint, at which time the query will be served to the backend, and dropped from the queue in memory.  Once the backed has generated the results of the query, it will wrap those results in PubSubMessage. The backend extracts the URL to send the results to from the metadata and writes the results PubSubMessage to the \"results\" REST endpoint with a POST. This result will then be stored in memory until the Web Service does a GET to that endpoint, at which time the Web Service will have the results of the query to send back to the user.", 
            "title": "How does it work?"
        }, 
        {
            "location": "/pubsub/rest/#setup", 
            "text": "To enable the RESTPubSub and expose the two additional necessary REST endpoints, you must enable the setting:  bullet.pubsub.builtin.rest.enabled: true  ...in the Web Service  application.yaml  configuration file. This can also be done from the command line when launching the Web Service jar file by adding the command-line option:  --bullet.pubsub.builtin.rest.enabled=true  This will enable the two necessary REST endpoints, the paths for which can be configured in the  application.yaml  file with the settings:  bullet.pubsub.builtin.rest.query.path: /pubsub/query\nbullet.pubsub.builtin.rest.result.path: /pubsub/result", 
            "title": "Setup"
        }, 
        {
            "location": "/pubsub/rest/#plug-into-the-backend", 
            "text": "Configure the backend to use the REST PubSub:  bullet.pubsub.context.name:  QUERY_PROCESSING \nbullet.pubsub.class.name:  com.yahoo.bullet.pubsub.rest.RESTPubSub \nbullet.pubsub.rest.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.urls:\n    -  http:// API_HOST_A :9901/api/bullet/pubsub/query \n    -  http:// API_HOST_B :9901/api/bullet/pubsub/query      Setting Name  Default Value  Meaning      bullet.pubsub.context.name  QUERY_PROCESSING  Tells the PubSub that it is running in the backend    bullet.pubsub.class.name  com.yahoo.bullet.pubsub.rest.RESTPubSub  Tells Bullet to use this class for its PubSub    bullet.pubsub.rest.connect.timeout.ms  5000  Sets the HTTP connect timeout to 5 s    bullet.pubsub.rest.subscriber.max.uncommitted.messages  100  This is the maximum number of uncommitted messages allowed to be read by the subscriber before blocking    bullet.pubsub.rest.query.subscriber.min.wait.ms  10  This is used to avoid making an HTTP request too rapidly and overloading the HTTP endpoint. It will force the backend to poll the query endpoint at most once every 10ms    bullet.pubsub.rest.query.urls   This should be a list of all the query REST endpoint URLs. If you are only running one Web Service this will only contain one URL (the URL of your Web Service followed by the full path of the query endpoint)", 
            "title": "Plug into the Backend"
        }, 
        {
            "location": "/pubsub/rest/#plug-into-the-web-service", 
            "text": "Configure the Web Service to use the REST PubSub:  bullet.pubsub.context.name:  QUERY_SUBMISSION \nbullet.pubsub.class.name:  com.yahoo.bullet.pubsub.rest.RESTPubSub \nbullet.pubsub.rest.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.result.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.result.url:  http://localhost:9901/api/bullet/pubsub/result \nbullet.pubsub.rest.query.urls:\n    -  http://localhost:9901/api/bullet/pubsub/query      Setting Name  Default Value  Meaning      bullet.pubsub.context.name  QUERY_SUBMISSION  Tells the PubSub that it is running in the Web Service    bullet.pubsub.class.name  com.yahoo.bullet.pubsub.rest.RESTPubSub  Tells Bullet to use this class for its PubSub    bullet.pubsub.rest.connect.timeout.ms  5000  Sets the HTTP connect timeout to 5 s    bullet.pubsub.rest.subscriber.max.uncommitted.messages  100  This is the maximum number of uncommitted messages allowed to be read by the subscriber before blocking    bullet.pubsub.rest.result.subscriber.min.wait.ms  10  This is used to avoid making an HTTP request too rapidly and overloading the HTTP endpoint. It will force the Web Service to poll the query endpoint at most once every 10ms    bullet.pubsub.rest.result.url  http://localhost:9901/api/bullet/pubsub/result  This is the endpoint from which the Web Service should read results. This is the hostname of that machine the Web Service is running on (or  localhost )    bullet.pubsub.rest.query.urls  http://localhost:9901/api/bullet/pubsub/query  In the Web Service, this should contain  exactly one  URL (the URL to which queries should be written). This is the hostname of that machine the Web Service is running on (or  localhost )", 
            "title": "Plug into the Web Service"
        }, 
        {
            "location": "/pubsub/pulsar/", 
            "text": "Pulsar PubSub\n\n\nThe Pulsar implementation of the Bullet PubSub uses \nApache Pulsar\n as the backing PubSub messaging queue and can be used with any Backend and Web Service.\n\n\nHow does it work?\n\n\nBullet Pulsar requires at least one Pulsar topic for queries and at least one Pulsar topic for results. On the Web Service, the PubSub is used to create publishers and subscribers that write queries and read results. Similarly, on the Backend, the PubSub is used to create publishers and subscribers that read queries and write results. Both queries and results are sent as \nPubSubMessages\n over the Pulsar topics.\n\n\nWith the Pulsar implementation, it is also possible to shard the Web Service by having each Web Service specify a different topic to read results from. Additionally, you may specify a different topic for each Web Service to write queries to, but this is optional as there is only one Backend reading queries.\n\n\nSetup\n\n\nBefore using Bullet Pulsar, you will need to first set up a Pulsar cluster. To do so, follow the \ninstructions here\n.\n\n\n\n\nNote\n\n\nFor a quick setup, Pulsar topics can be automatically created\n\n\n\n\nPlug into the Backend\n\n\nDepending on how your Backend is built, you will need to either add Bullet Pulsar to your classpath or include it in your build tool. Get the artifact on our \nreleases page\n.\n\n\nNote, if you're adding Bullet Pulsar to the classpath instead of building a fat jar, you will need to get the jar with the classifier \nfat\n since you will need Bullet Pulsar with all of its dependencies.\n\n\nConfigure the Backend to use the Pulsar PubSub:\n\n\nbullet.pubsub.context.name: \nQUERY_PROCESSING\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.pulsar.PulsarPubSub\n\nbullet.pubsub.pulsar.client.serviceUrl: \npulsar://your-service-url\n\nbullet.pubsub.pulsar.consumer.subscriptionName: \nyour-subscription-name\n\\\nbullet.pubsub.pulsar.response.topic.names: [\npersistent://sample/ns1/your-query-topic\n]\n\n\n\n\nYou will also need to configure the Pulsar Client, Producer, and Consumer properties used by the PubSub Publishers and Subscribers.\n\n\nSee the \ndefault configuration file\n for complete configuration details.\n\n\nPlug into the Web Service\n\n\nYou will need to head over to our \nreleases page\n and get the JAR artifact with the \nfat\n classifier. You can download the artifact for the 0.1.0 release \ndirectly from JCenter\n).\n\n\nYou should then plug in this JAR to your Web Service following the instructions \nhere\n.\n\n\nFor configuration, you should \nfollow the steps here\n to create and provide a YAML file to the Web Service. Remember to set the context to \nQUERY_SUBMISSION\n.\n\n\nbullet.pubsub.context.name: \nQUERY_SUBMISSION\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.pulsar.PulsarPubSub\n\nbullet.pubsub.pulsar.client.serviceUrl: \npulsar://your-service-url\n\nbullet.pubsub.pulsar.consumer.subscriptionName: \nyour-subscription-name\n\nbullet.pubsub.pulsar.request.topic.name: \npersistent://sample/ns1/your-query-topic\n\nbullet.pubsub.pulsar.response.topic.name: \npersistent://sample/ns1/your-response-topic\n\n\n\n\n\nAs with the Backend, you will need to configure the Pulsar Client, Producer, and Consumer properties used by the PubSub Publishers and Subscribers.\n\n\nSee the \ndefault configuration file\n for complete configuration details.\n\n\nRemember that your Subscribers in the Backend are reading what the Producers in your Web Service are producing and vice-versa, so make sure to match up topics and settings accordingly.\n\n\nPassthrough configuration\n\n\nYou can configure the Pulsar Client, Producer, or Consumer properties for the PubSub Publishers and Subscribers by prefixing them with either \nbullet.pubsub.pulsar.client.\n, \nbullet.pubsub.pulsar.producer.\n, or \nbullet.pubsub.pulsar.consumer.\n respectively. The PubSub configuration sets a few of its own defaults but otherwise uses the default Pulsar settings.\n\n\nSee the \ndefault configuration file\n for complete configuration details with their defaults.\n\n\nSecurity\n\n\nPulsar supports Athenz Authentication and TLS, and Bullet Pulsar can be configured to use these. For complete details, see the \nPulsar documentation\n.\n\n\n# Note, these properties are not prefixed by \nbullet.pubsub.pulsar.client.\n\nbullet.pubsub.pulsar.auth.enable: true\nbullet.pubsub.pulsar.auth.plugin.class.name: \norg.apache.pulsar.client.impl.auth.AuthenticationAthenz\n\nbullet.pubsub.pulsar.auth.params.string: '{\ntenantDomain\n:\nyour_domain\n,\ntenantService\n:\nyour_app\n,\nproviderDomain\n:\npulsar\n,\nprivateKey\n:\nfile:///path/to/private.pem\n,\nkeyId\n:\nv1\n}'\n\n\n\n\n# Enable TLS\nbullet.pubsub.pulsar.client.useTls: true\nbullet.pubsub.pulsar.client.tlsAllowInsecureConnection: false\nbullet.pubsub.pulsar.client.tlsTrustCertsFilePath: \n/path/to/cacert.pem", 
            "title": "Pulsar"
        }, 
        {
            "location": "/pubsub/pulsar/#pulsar-pubsub", 
            "text": "The Pulsar implementation of the Bullet PubSub uses  Apache Pulsar  as the backing PubSub messaging queue and can be used with any Backend and Web Service.", 
            "title": "Pulsar PubSub"
        }, 
        {
            "location": "/pubsub/pulsar/#how-does-it-work", 
            "text": "Bullet Pulsar requires at least one Pulsar topic for queries and at least one Pulsar topic for results. On the Web Service, the PubSub is used to create publishers and subscribers that write queries and read results. Similarly, on the Backend, the PubSub is used to create publishers and subscribers that read queries and write results. Both queries and results are sent as  PubSubMessages  over the Pulsar topics.  With the Pulsar implementation, it is also possible to shard the Web Service by having each Web Service specify a different topic to read results from. Additionally, you may specify a different topic for each Web Service to write queries to, but this is optional as there is only one Backend reading queries.", 
            "title": "How does it work?"
        }, 
        {
            "location": "/pubsub/pulsar/#setup", 
            "text": "Before using Bullet Pulsar, you will need to first set up a Pulsar cluster. To do so, follow the  instructions here .   Note  For a quick setup, Pulsar topics can be automatically created", 
            "title": "Setup"
        }, 
        {
            "location": "/pubsub/pulsar/#plug-into-the-backend", 
            "text": "Depending on how your Backend is built, you will need to either add Bullet Pulsar to your classpath or include it in your build tool. Get the artifact on our  releases page .  Note, if you're adding Bullet Pulsar to the classpath instead of building a fat jar, you will need to get the jar with the classifier  fat  since you will need Bullet Pulsar with all of its dependencies.  Configure the Backend to use the Pulsar PubSub:  bullet.pubsub.context.name:  QUERY_PROCESSING \nbullet.pubsub.class.name:  com.yahoo.bullet.pulsar.PulsarPubSub \nbullet.pubsub.pulsar.client.serviceUrl:  pulsar://your-service-url \nbullet.pubsub.pulsar.consumer.subscriptionName:  your-subscription-name \\\nbullet.pubsub.pulsar.response.topic.names: [ persistent://sample/ns1/your-query-topic ]  You will also need to configure the Pulsar Client, Producer, and Consumer properties used by the PubSub Publishers and Subscribers.  See the  default configuration file  for complete configuration details.", 
            "title": "Plug into the Backend"
        }, 
        {
            "location": "/pubsub/pulsar/#plug-into-the-web-service", 
            "text": "You will need to head over to our  releases page  and get the JAR artifact with the  fat  classifier. You can download the artifact for the 0.1.0 release  directly from JCenter ).  You should then plug in this JAR to your Web Service following the instructions  here .  For configuration, you should  follow the steps here  to create and provide a YAML file to the Web Service. Remember to set the context to  QUERY_SUBMISSION .  bullet.pubsub.context.name:  QUERY_SUBMISSION \nbullet.pubsub.class.name:  com.yahoo.bullet.pulsar.PulsarPubSub \nbullet.pubsub.pulsar.client.serviceUrl:  pulsar://your-service-url \nbullet.pubsub.pulsar.consumer.subscriptionName:  your-subscription-name \nbullet.pubsub.pulsar.request.topic.name:  persistent://sample/ns1/your-query-topic \nbullet.pubsub.pulsar.response.topic.name:  persistent://sample/ns1/your-response-topic   As with the Backend, you will need to configure the Pulsar Client, Producer, and Consumer properties used by the PubSub Publishers and Subscribers.  See the  default configuration file  for complete configuration details.  Remember that your Subscribers in the Backend are reading what the Producers in your Web Service are producing and vice-versa, so make sure to match up topics and settings accordingly.", 
            "title": "Plug into the Web Service"
        }, 
        {
            "location": "/pubsub/pulsar/#passthrough-configuration", 
            "text": "You can configure the Pulsar Client, Producer, or Consumer properties for the PubSub Publishers and Subscribers by prefixing them with either  bullet.pubsub.pulsar.client. ,  bullet.pubsub.pulsar.producer. , or  bullet.pubsub.pulsar.consumer.  respectively. The PubSub configuration sets a few of its own defaults but otherwise uses the default Pulsar settings.  See the  default configuration file  for complete configuration details with their defaults.", 
            "title": "Passthrough configuration"
        }, 
        {
            "location": "/pubsub/pulsar/#security", 
            "text": "Pulsar supports Athenz Authentication and TLS, and Bullet Pulsar can be configured to use these. For complete details, see the  Pulsar documentation .  # Note, these properties are not prefixed by  bullet.pubsub.pulsar.client. \nbullet.pubsub.pulsar.auth.enable: true\nbullet.pubsub.pulsar.auth.plugin.class.name:  org.apache.pulsar.client.impl.auth.AuthenticationAthenz \nbullet.pubsub.pulsar.auth.params.string: '{ tenantDomain : your_domain , tenantService : your_app , providerDomain : pulsar , privateKey : file:///path/to/private.pem , keyId : v1 }'  # Enable TLS\nbullet.pubsub.pulsar.client.useTls: true\nbullet.pubsub.pulsar.client.tlsAllowInsecureConnection: false\nbullet.pubsub.pulsar.client.tlsTrustCertsFilePath:  /path/to/cacert.pem", 
            "title": "Security"
        }, 
        {
            "location": "/pubsub/storm-drpc/", 
            "text": "Storm DRPC PubSub\n\n\n\n\nNOTE: This PubSub only works with old versions of the Storm Backend!\n\n\nSince DRPC is part of Storm and can only support a single query/response model, this PubSub implementation can only be used with the Storm backend and cannot support Windowed queries (bullet-storm 0.8.0 and later).\n\n\n\n\nBullet on \nStorm\n can use \nStorm DRPC\n as a PubSub layer. DRPC, or Distributed Remote Procedure Call, is built into Storm and consists of a set of servers that are part of the Storm cluster.\n\n\nHow does it work?\n\n\nWhen a Storm topology that uses DRPC is launched, it registers a spout with a unique name (the procedure in the Distributed Remote Procedure Call) with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology that registered that name (Bullet). The result from topology is sent back to the client. We picked Storm to implement Bullet on first not only because it was the most popular Streaming framework at Yahoo but also since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.\n\n\nYou can communicate with DRPC using \nApache Thrift\n or REST. Our implementation uses REST. The Web Service sends a JSON serialized PubSubMessage with the query in it through HTTP and asynchronously waits for the results back through DRPC.\n\n\n\n\nREST and DRPC\n\n\nWhile DRPC exposes a \nThrift\n endpoint, the PubSub implementation uses REST. When you launch your topology with the DRPC PubSub, you can POST a JSON Bullet PubSubMessage containing a String JSON query to a DRPC server directly with the function name that you specify in the \nBullet configuration\n. For example,\n\nbash\n  curl -s -X POST -d '{\"id\":\"\", \"content\":\"{}\"}' http://\nDRPC_SERVER\n:\nDRPC_PORT\n/drpc/\nDRPC_FUNCTION_FROM_YOUR_BULLET_CONF\n\n to get a random record (inside a JSON representation of a PubSubMessage) from your data instantly if you left the Raw aggregation micro-batch size at the default of 1. The \ncontent\n above in the JSON is the actual (empty) Bullet query. This is a quick way to check if your topology is up and running!\n\n\n\n\nSetup\n\n\nThe DRPC PubSub is part of the \nBullet Storm\n starting with versions 0.6.2 and above.\n\n\nPlug into the Storm Backend\n\n\nWhen you are setting up your Bullet topology with your plug-in data source (a Spout or a topology), you will naturally build a JAR with all the dependencies or a \nfat\n JAR. This will include all the DRPC PubSub code and dependencies. You do not need anything else. For configuration, the YAML file that you probably already provide to your topology needs to have the additional settings listed below (the function name is optional but you should change the default since the DRPC function needs to be unique per Storm cluster). Now if you launch your topology, it should be wired up to use Storm DRPC.\n\n\nbullet.pubsub.context.name: \nQUERY_PROCESSING\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.storm.drpc.DRPCPubSub\n\nbullet.pubsub.storm.drpc.function: \ncustom-name\n\n\n\n\n\nSecurity\n\n\nIf your Storm  cluster is secured with \nKerberos\n (a standard for Big Data platforms), you will need to periodically refresh your Kerberos TGT and push the credentials to your Storm topology. This is generally done with \nkinit\n for your topology user, followed by a \nstorm upload-credentials \nTOPOLOGY_NAME\n. You would probably run this as a \ncron\n task.\n\n\nPlug into the Web Service\n\n\nWhen you're plugging in the DRPC PubSub layer into your Web Service, you will need the Bullet Storm JAR with dependencies that you can download from \nJCenter\n. The classifier for this JAR is \nfat\n if you are depending on it through Maven. You can also download the JAR for the 0.6.2 version directly through \nJCenter here\n.\n\n\nYou should then plug in this JAR to your Web Service following the instructions \nhere\n.\n\n\nFor configuration, you should \nfollow the steps here\n and add the context and class name listed above. You will need to point to your DRPC servers and set the function to the same value you chose \nabove\n. You can configure this and other settings that are explained further in the \nPubSub and PubSub Storm DRPC defaults section\n in the Bullet Storm defaults file.\n\n\nbullet.pubsub.context.name: \nQUERY_SUBMISSION\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.storm.drpc.DRPCPubSub\n\nbullet.pubsub.storm.drpc.servers:\n  - server1\n  - server2\n  - server3\nbullet.pubsub.storm.drpc.function: \ncustom-name\n\nbullet.pubsub.storm.drpc.http.protocol: \nhttp\n\nbullet.pubsub.storm.drpc.http.port: \n4080\n\nbullet.pubsub.storm.drpc.http.path: \ndrpc\n\nbullet.pubsub.storm.drpc.http.connect.retry.limit: 3\nbullet.pubsub.storm.drpc.http.connect.timeout.ms: 1000\n\n\n\n\nCaveats with Storm DRPC\n\n\nScalability\n\n\nDRPC servers are a shared resource per Storm cluster and it may be possible that you have to contend with other topologies in your multi-tenant cluster. While it is horizontally scalable, it does tie the scalability of the Bullet backend to it. If you only have a few DRPC servers in your Storm cluster, you may need to add more to support more simultaneous DRPC requests. We have \nfound that\n each server gives us about ~250 simultaneous queries. There is an Async implementation coming in Storm 2.0 that should increase the throughput.\n\n\nQuery Duration\n\n\nThe maximum time a query can run for depends on the maximum time Storm DRPC request can last in your Storm topology. Generally the default is set to 10 minutes. This means that the \nlongest query duration possible will be 10 minutes\n. The value of this is up to your cluster maintainers.\n\n\nRequest-Response\n\n\nOur PubSub uses DRPC using HTTP REST in a request-response model. This means that it will not support incremental results as it is! We could switch our usage of DRPC to send signals to the topology to fetch results and start queries. Depending on if there is demand, we may support this in our implementation in the future.\n\n\nReliability\n\n\nStorm DRPC follows the principle of leaving retries to the DRPC user (in our case, the Bullet web service). At this moment, we have not chosen to add reliability mechanisms to the query publishing, result publishing or result subscribing sides of our DRPC PubSub implementations but the query subscribers do use the \nBufferingSubscriber\n mentioned \nhere\n.", 
            "title": "Storm DRPC"
        }, 
        {
            "location": "/pubsub/storm-drpc/#storm-drpc-pubsub", 
            "text": "NOTE: This PubSub only works with old versions of the Storm Backend!  Since DRPC is part of Storm and can only support a single query/response model, this PubSub implementation can only be used with the Storm backend and cannot support Windowed queries (bullet-storm 0.8.0 and later).   Bullet on  Storm  can use  Storm DRPC  as a PubSub layer. DRPC, or Distributed Remote Procedure Call, is built into Storm and consists of a set of servers that are part of the Storm cluster.", 
            "title": "Storm DRPC PubSub"
        }, 
        {
            "location": "/pubsub/storm-drpc/#how-does-it-work", 
            "text": "When a Storm topology that uses DRPC is launched, it registers a spout with a unique name (the procedure in the Distributed Remote Procedure Call) with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology that registered that name (Bullet). The result from topology is sent back to the client. We picked Storm to implement Bullet on first not only because it was the most popular Streaming framework at Yahoo but also since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.  You can communicate with DRPC using  Apache Thrift  or REST. Our implementation uses REST. The Web Service sends a JSON serialized PubSubMessage with the query in it through HTTP and asynchronously waits for the results back through DRPC.   REST and DRPC  While DRPC exposes a  Thrift  endpoint, the PubSub implementation uses REST. When you launch your topology with the DRPC PubSub, you can POST a JSON Bullet PubSubMessage containing a String JSON query to a DRPC server directly with the function name that you specify in the  Bullet configuration . For example, bash\n  curl -s -X POST -d '{\"id\":\"\", \"content\":\"{}\"}' http:// DRPC_SERVER : DRPC_PORT /drpc/ DRPC_FUNCTION_FROM_YOUR_BULLET_CONF \n to get a random record (inside a JSON representation of a PubSubMessage) from your data instantly if you left the Raw aggregation micro-batch size at the default of 1. The  content  above in the JSON is the actual (empty) Bullet query. This is a quick way to check if your topology is up and running!", 
            "title": "How does it work?"
        }, 
        {
            "location": "/pubsub/storm-drpc/#setup", 
            "text": "The DRPC PubSub is part of the  Bullet Storm  starting with versions 0.6.2 and above.", 
            "title": "Setup"
        }, 
        {
            "location": "/pubsub/storm-drpc/#plug-into-the-storm-backend", 
            "text": "When you are setting up your Bullet topology with your plug-in data source (a Spout or a topology), you will naturally build a JAR with all the dependencies or a  fat  JAR. This will include all the DRPC PubSub code and dependencies. You do not need anything else. For configuration, the YAML file that you probably already provide to your topology needs to have the additional settings listed below (the function name is optional but you should change the default since the DRPC function needs to be unique per Storm cluster). Now if you launch your topology, it should be wired up to use Storm DRPC.  bullet.pubsub.context.name:  QUERY_PROCESSING \nbullet.pubsub.class.name:  com.yahoo.bullet.storm.drpc.DRPCPubSub \nbullet.pubsub.storm.drpc.function:  custom-name", 
            "title": "Plug into the Storm Backend"
        }, 
        {
            "location": "/pubsub/storm-drpc/#security", 
            "text": "If your Storm  cluster is secured with  Kerberos  (a standard for Big Data platforms), you will need to periodically refresh your Kerberos TGT and push the credentials to your Storm topology. This is generally done with  kinit  for your topology user, followed by a  storm upload-credentials  TOPOLOGY_NAME . You would probably run this as a  cron  task.", 
            "title": "Security"
        }, 
        {
            "location": "/pubsub/storm-drpc/#plug-into-the-web-service", 
            "text": "When you're plugging in the DRPC PubSub layer into your Web Service, you will need the Bullet Storm JAR with dependencies that you can download from  JCenter . The classifier for this JAR is  fat  if you are depending on it through Maven. You can also download the JAR for the 0.6.2 version directly through  JCenter here .  You should then plug in this JAR to your Web Service following the instructions  here .  For configuration, you should  follow the steps here  and add the context and class name listed above. You will need to point to your DRPC servers and set the function to the same value you chose  above . You can configure this and other settings that are explained further in the  PubSub and PubSub Storm DRPC defaults section  in the Bullet Storm defaults file.  bullet.pubsub.context.name:  QUERY_SUBMISSION \nbullet.pubsub.class.name:  com.yahoo.bullet.storm.drpc.DRPCPubSub \nbullet.pubsub.storm.drpc.servers:\n  - server1\n  - server2\n  - server3\nbullet.pubsub.storm.drpc.function:  custom-name \nbullet.pubsub.storm.drpc.http.protocol:  http \nbullet.pubsub.storm.drpc.http.port:  4080 \nbullet.pubsub.storm.drpc.http.path:  drpc \nbullet.pubsub.storm.drpc.http.connect.retry.limit: 3\nbullet.pubsub.storm.drpc.http.connect.timeout.ms: 1000", 
            "title": "Plug into the Web Service"
        }, 
        {
            "location": "/pubsub/storm-drpc/#caveats-with-storm-drpc", 
            "text": "", 
            "title": "Caveats with Storm DRPC"
        }, 
        {
            "location": "/pubsub/storm-drpc/#scalability", 
            "text": "DRPC servers are a shared resource per Storm cluster and it may be possible that you have to contend with other topologies in your multi-tenant cluster. While it is horizontally scalable, it does tie the scalability of the Bullet backend to it. If you only have a few DRPC servers in your Storm cluster, you may need to add more to support more simultaneous DRPC requests. We have  found that  each server gives us about ~250 simultaneous queries. There is an Async implementation coming in Storm 2.0 that should increase the throughput.", 
            "title": "Scalability"
        }, 
        {
            "location": "/pubsub/storm-drpc/#query-duration", 
            "text": "The maximum time a query can run for depends on the maximum time Storm DRPC request can last in your Storm topology. Generally the default is set to 10 minutes. This means that the  longest query duration possible will be 10 minutes . The value of this is up to your cluster maintainers.", 
            "title": "Query Duration"
        }, 
        {
            "location": "/pubsub/storm-drpc/#request-response", 
            "text": "Our PubSub uses DRPC using HTTP REST in a request-response model. This means that it will not support incremental results as it is! We could switch our usage of DRPC to send signals to the topology to fetch results and start queries. Depending on if there is demand, we may support this in our implementation in the future.", 
            "title": "Request-Response"
        }, 
        {
            "location": "/pubsub/storm-drpc/#reliability", 
            "text": "Storm DRPC follows the principle of leaving retries to the DRPC user (in our case, the Bullet web service). At this moment, we have not chosen to add reliability mechanisms to the query publishing, result publishing or result subscribing sides of our DRPC PubSub implementations but the query subscribers do use the  BufferingSubscriber  mentioned  here .", 
            "title": "Reliability"
        }, 
        {
            "location": "/ws/setup/", 
            "text": "The Web Service\n\n\nThe Web Service is a Java JAR file that you can deploy on a machine to communicate with the Bullet Backend. You then plug in a particular Bullet PubSub implementation such as \nKafka PubSub\n or \nStorm DRPC PubSub\n. For an example on how to set up a Bullet backend, see the \nStorm example setup\n.\n\n\nThere are three main purposes for this layer at this time:\n\n\n1) It converts queries and sends them through the PubSub to the backend. It handles responses from the backend for both synchronous and asynchronous queries.\n\n\n2) It provides an endpoint that can serve a \nJSON API schema\n for the Bullet UI. Currently, static schemas from a file are supported.\n\n\n3) It manages metadata for queries such unique identifiers or storing queries for resilience for Backends that support replaying.\n\n\nPrerequisites\n\n\nIn order for your Web Service to work with Bullet, you should have an instance of the Backend such as \nStorm\n and a PubSub instance such as \nStorm DRPC\n or \nKafka\n already set up. Alternitively you can run the RESTPubSub as part of the web service. See \nRESTPubSub\n for more info.\n\n\nInstallation\n\n\nYou can download the JAR file directly from \nJCenter\n. The Web Service is a \nSpring Boot\n application. It executes as a standalone application. Note that prior to version 0.1.1, bullet-service was a WAR file that you deployed onto a servlet container like Jetty. It now embeds a \nApache Tomcat\n servlet container.\n\n\nIf you need to depend on the source code directly (to add new endpoints for your own purposes or to build a WAR file out of the JAR), you need to add the JCenter repository and get the artifact through your dependency management system. Maven is shown below.\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-service\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the source or javadoc or \nclassifier\nembedded\n/classifier\n if you want the full JAR with the embedded web server.\n\n\nConfiguration\n\n\nThere are a few different modules in the Web Service:\n\n\n\n\nAPI\n: Configure the Web Service, the web server, the names of various endpoints, and other Spring Boot settings. You can also configure certain top-level settings for the various modules below - such as the number of publishers and subscribers to use for the PubSub etc.\n\n\nPubSub\n: Configure what PubSub to use and the various settings for it.\n\n\nSchema\n (Optional): Configure the Schema file (that powers the \nUI\n.\n\n\nQuery\n (Optional): Configure the various query defaults for queries coming into the API. You can also point to the schema used by the BQL module to do type-checking and other semantic validation.\n\n\nStorage\n (Optional): Configure what Storage to use, the various settings for it through another configuration file.\n\n\nAsynchronous Queries\n (Optional): Configure the Asynchronous query module which lets you send queries to an API but not wait for the results. The results, when received, are sent through a PubSubResponder interface that you can plug in - such as email or writing to another PubSub etc.\n\n\nMetrics\n (Optional): Configure the Metrics collection system which collects various statistics about the endpoints and sends them through a Publisher interface that you can plug in. You can use this for monitoring status codes and errors.\n\n\nStatus\n (Optional): Configure the Status checking system which disables the API if the backend is down or unreachable. It works by sending a simple query through and waiting for results periodically.\n\n\n\n\nAPI Configuration\n\n\nTake a look at the \nsettings\n for a list of the settings that are configured. The Web Service settings start with \nbullet.\n. You can configure various WebSocket settings and other API level configuration.\n\n\nIf you provide a custom settings \napplication.yaml\n, you will \nneed\n to specify the default values in this file since the framework uses your file instead of these defaults. You can also pass in overrides as command-line arguments when launching the server.\n\n\nSpring Boot Configuration\n\n\nYou can also configure various Spring and web server settings here. Take a look at \nthis page\n page for the various values you can supply.\n\n\nPubSub Configuration\n\n\nYou configure the PubSub by providing a configuration YAML file and setting the \nbullet.pubsub.config\n to its path. In \nthat\n file, you will set these two settings at a minimum:\n\n\n\n\nbullet.pubsub.class.name\n should be set to the fully qualified package to your PubSub implementation. Example: \ncom.yahoo.bullet.kafka.KafkaPubSub\n for the \nKafka PubSub\n.\n\n\nbullet.pubsub.context.name: QUERY_SUBMISSION\n. The Web Service requires the PubSub to be in the \nQUERY_SUBMISSION\n context.\n\n\n\n\nYou will also specify other parameters that your chosen PubSub requires or can use.\n\n\nIn the top level configuration for the PubSub in \napplication.yaml\n, you may configure the number of threads for reading and writing the PubSub as well as enabling and configuring the built-in \nREST PubSub\n if you choose to use that.\n\n\nSchema Configuration\n\n\nThe Web Service can also provide a endpoint that serves your data schema to your UI. You do not necessarily have to use this to serve your schema. The UI can use any JSON API schema specification. But if your schema is fixed or does not change often, it might be simpler for you to use this endpoint to provide the schema for the UI, instead of creating a new one. The Web Service also takes care to provide the right \nCORS\n headers so that your UI can communicate with it.\n\n\nYou can use \nsample_columns.json\n as a guideline for what your actual schema file should look like or if you want to create your own Web Service that dynamically serves your schema to the UI if it changes frequently.\n\n\nOnce you have your schema file, you can provide it to the Web Service by setting the \nbullet.schema.file\n to the path to your file.\n\n\nQuery Configuration\n\n\nYou can provide a file containing the various query defaults and maximums by using the \nbullet.query.config\n setting. This is configured used by the query building module to make sure incoming queries respect various configurations provided here such as default aggregation sizes or minimum window emit intervals etc. You can also point to a schema file (ideally the same one used if you chose to enable the schema module) that the query builder layer can use for advanced checking. See \nthe defaults\n for more information.\n\n\n\n\nQuerying and Schema\n\n\nIf you provide a schema, the query creation layer in BQL can leverage this for type-checking and advanced semantic validation. This can error out otherwise erroneous queries right at the API layer without having to run a query that returns no results or errors out in the backend.\n\n\n\n\nStorage Configuration\n\n\nThis module lets you set up a Storage layer to write queries to when submitted. These are cleaned up when the query is terminated and the final result sent back to the API. This is particularly relevant if your Bullet instance is fielding long-running queries that need to be resilient. This coupled with a Backend implementation that can leverage the Storage lets you recreate queries in the Backend in case of component failure or restarts. The Storage layer is also particularly relevant if you're using the asynchronous query module with a PubSubResponder interface that relies on the Storage to do additional metadata lookups.\n\n\nYou can configure and provide a Storage implementation by implementing the \nStorageManager interface\n. Note that you cannot turn off the Storage module in the API but by default, the \nNullStorageManager\n is used, which does nothing. You can provide a configuration yaml file that supplies your particular settings for your StorageManager by using the \nbullet.storage.config\n setting. See \nthe defaults\n for more details.\n\n\n\n\nSo you DO have persistence?\n\n\nThis is not the same as storing the data. Bullet's philosophy is to avoid storing the incoming data stream that it is field queries on. This layer is meant for storing query related information. When we extend the storage to storing intermediate results in the backend for extra resiliency between windows, the size of the storage should still be well defined for sketch-based aggregations.\n\n\n\n\nAsynchronous Query Configuration\n\n\nThis module enables the asynchronous query submission endpoint (the \nbullet.endpoint.async\n setting) that lets you submit queries to it without having to hang around for the results to stream back. Instead you use \nthe PubSubResponder interface\n to provide an instance that is used to write results that come back for that query. You can use this for \nalerting\n use-cases where you need to send e-mails on certain alert queries being triggered or if you want your results written to a PubSub that you can consume in a different manner etc.\n\n\nBy default, this module is disabled. However, it is mock configured to use \na standard Bullet PubSubResponder\n that we provide to write the result back to a REST PubSub that is assumed to be running locally. You can change this to write the results to your own PubSub if you desire or plug in something else entirely. You can provide a configuration yaml file that supplies your particular settings for your PubSubResponder by using the \nbullet.async.config\n setting. See the \ndefaults\n for more information.\n\n\nMetrics Configuration\n\n\nThis module lets you monitor the Web Service for information on what is happening. It tracks the various status codes and publishes them using the \nMetricPublisher interface\n to a place of your choice. By default, the \nHTTPMetricPublisher interface\n is configured, which can post to a URL of your choice.\n\n\nYou can provide a configuration yaml file that supplies your particular settings for your MetricPublisher by using the \nbullet.metric.config\n setting. See the \ndefaults\n for more information.\n\n\nStatus Configuration\n\n\nThis module periodically sends a \ntick\n query to the backend to make sure it is functioning properly. You can configure various settings for it here. If enabled, this module can disable the whole API if the backend is unreachable. This can be used if you front multiple Web Service instances talking to different instances of a backend behind a proxy and take down the backends one at a time for upgrades.\n\n\nLaunch\n\n\nTo launch, you will need your PubSub implementation JAR file and launch the application by providing the path to it. For example, if you only wished to provide the PubSub configuration and you had the Web Service jar and your chosen PubSub (say Kafka) in your current directory, you would run:\n\n\njava -Dloader.path=bullet-kafka.jar -jar bullet-service.jar --bullet.pubsub.config=pubsub_settings.yaml  --logging.level.root=INFO\n\n\n\n\nThis launches the Web Service using Kafka as the PubSub, no custom schema (the default sample columns) and the default values in \nsettings\n. It also uses a root logging level of \nINFO\n.\n\n\nYou could also tweak the various Bullet Web Service or Spring Boot settings by passing them in to the command above. For instance, you could also provide a path to your schema file using \n--bullet.schema.file=/path/to/schema.json\n. You could also have a custom \napplication.yaml\n file (you can change the name using \nspring.config.name\n) and pass it to the Web Service instead by running:\n\n\njava -Dloader.path=bullet-kafka.jar -jar bullet-service.jar --spring.config.location=application.yaml\n\n\n\n\nUsage\n\n\nOnce the Web Service is up, you should be able to test to see if it's able to talk to the Bullet Backend:\n\n\nYou can HTTP POST a Bullet query to the API with:\n\n\ncurl -s -H \nContent-Type: text/plain\n -X POST -d '{}' http://localhost:5555/api/bullet/query\n\n\n\n\nYou should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1 when launching the Bullet Backend).\n\n\n\n\nContext Path\n\n\nThe context path, or \"/api/bullet\" in the URL above can be changed using the Spring Boot setting \nserver.context-path\n. You can also change the port (defaults to port 5555) using \nserver.port\n.\n\n\n\n\nIf you provided a path to a schema file in your configuration file when you \nlaunch\n the Web Service, you can also HTTP GET your schema at \nhttp://localhost:5555/api/bullet/columns\n\n\nIf you did not, the schema in \nsample_fields.json\n is the response. The Web Service converts it to a JSON API response and provides the right headers for CORS.", 
            "title": "Setup"
        }, 
        {
            "location": "/ws/setup/#the-web-service", 
            "text": "The Web Service is a Java JAR file that you can deploy on a machine to communicate with the Bullet Backend. You then plug in a particular Bullet PubSub implementation such as  Kafka PubSub  or  Storm DRPC PubSub . For an example on how to set up a Bullet backend, see the  Storm example setup .  There are three main purposes for this layer at this time:  1) It converts queries and sends them through the PubSub to the backend. It handles responses from the backend for both synchronous and asynchronous queries.  2) It provides an endpoint that can serve a  JSON API schema  for the Bullet UI. Currently, static schemas from a file are supported.  3) It manages metadata for queries such unique identifiers or storing queries for resilience for Backends that support replaying.", 
            "title": "The Web Service"
        }, 
        {
            "location": "/ws/setup/#prerequisites", 
            "text": "In order for your Web Service to work with Bullet, you should have an instance of the Backend such as  Storm  and a PubSub instance such as  Storm DRPC  or  Kafka  already set up. Alternitively you can run the RESTPubSub as part of the web service. See  RESTPubSub  for more info.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/ws/setup/#installation", 
            "text": "You can download the JAR file directly from  JCenter . The Web Service is a  Spring Boot  application. It executes as a standalone application. Note that prior to version 0.1.1, bullet-service was a WAR file that you deployed onto a servlet container like Jetty. It now embeds a  Apache Tomcat  servlet container.  If you need to depend on the source code directly (to add new endpoints for your own purposes or to build a WAR file out of the JAR), you need to add the JCenter repository and get the artifact through your dependency management system. Maven is shown below.  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-service /artifactId \n   version ${bullet.version} /version  /dependency   You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the source or javadoc or  classifier embedded /classifier  if you want the full JAR with the embedded web server.", 
            "title": "Installation"
        }, 
        {
            "location": "/ws/setup/#configuration", 
            "text": "There are a few different modules in the Web Service:   API : Configure the Web Service, the web server, the names of various endpoints, and other Spring Boot settings. You can also configure certain top-level settings for the various modules below - such as the number of publishers and subscribers to use for the PubSub etc.  PubSub : Configure what PubSub to use and the various settings for it.  Schema  (Optional): Configure the Schema file (that powers the  UI .  Query  (Optional): Configure the various query defaults for queries coming into the API. You can also point to the schema used by the BQL module to do type-checking and other semantic validation.  Storage  (Optional): Configure what Storage to use, the various settings for it through another configuration file.  Asynchronous Queries  (Optional): Configure the Asynchronous query module which lets you send queries to an API but not wait for the results. The results, when received, are sent through a PubSubResponder interface that you can plug in - such as email or writing to another PubSub etc.  Metrics  (Optional): Configure the Metrics collection system which collects various statistics about the endpoints and sends them through a Publisher interface that you can plug in. You can use this for monitoring status codes and errors.  Status  (Optional): Configure the Status checking system which disables the API if the backend is down or unreachable. It works by sending a simple query through and waiting for results periodically.", 
            "title": "Configuration"
        }, 
        {
            "location": "/ws/setup/#api-configuration", 
            "text": "Take a look at the  settings  for a list of the settings that are configured. The Web Service settings start with  bullet. . You can configure various WebSocket settings and other API level configuration.  If you provide a custom settings  application.yaml , you will  need  to specify the default values in this file since the framework uses your file instead of these defaults. You can also pass in overrides as command-line arguments when launching the server.", 
            "title": "API Configuration"
        }, 
        {
            "location": "/ws/setup/#spring-boot-configuration", 
            "text": "You can also configure various Spring and web server settings here. Take a look at  this page  page for the various values you can supply.", 
            "title": "Spring Boot Configuration"
        }, 
        {
            "location": "/ws/setup/#pubsub-configuration", 
            "text": "You configure the PubSub by providing a configuration YAML file and setting the  bullet.pubsub.config  to its path. In  that  file, you will set these two settings at a minimum:   bullet.pubsub.class.name  should be set to the fully qualified package to your PubSub implementation. Example:  com.yahoo.bullet.kafka.KafkaPubSub  for the  Kafka PubSub .  bullet.pubsub.context.name: QUERY_SUBMISSION . The Web Service requires the PubSub to be in the  QUERY_SUBMISSION  context.   You will also specify other parameters that your chosen PubSub requires or can use.  In the top level configuration for the PubSub in  application.yaml , you may configure the number of threads for reading and writing the PubSub as well as enabling and configuring the built-in  REST PubSub  if you choose to use that.", 
            "title": "PubSub Configuration"
        }, 
        {
            "location": "/ws/setup/#schema-configuration", 
            "text": "The Web Service can also provide a endpoint that serves your data schema to your UI. You do not necessarily have to use this to serve your schema. The UI can use any JSON API schema specification. But if your schema is fixed or does not change often, it might be simpler for you to use this endpoint to provide the schema for the UI, instead of creating a new one. The Web Service also takes care to provide the right  CORS  headers so that your UI can communicate with it.  You can use  sample_columns.json  as a guideline for what your actual schema file should look like or if you want to create your own Web Service that dynamically serves your schema to the UI if it changes frequently.  Once you have your schema file, you can provide it to the Web Service by setting the  bullet.schema.file  to the path to your file.", 
            "title": "Schema Configuration"
        }, 
        {
            "location": "/ws/setup/#query-configuration", 
            "text": "You can provide a file containing the various query defaults and maximums by using the  bullet.query.config  setting. This is configured used by the query building module to make sure incoming queries respect various configurations provided here such as default aggregation sizes or minimum window emit intervals etc. You can also point to a schema file (ideally the same one used if you chose to enable the schema module) that the query builder layer can use for advanced checking. See  the defaults  for more information.   Querying and Schema  If you provide a schema, the query creation layer in BQL can leverage this for type-checking and advanced semantic validation. This can error out otherwise erroneous queries right at the API layer without having to run a query that returns no results or errors out in the backend.", 
            "title": "Query Configuration"
        }, 
        {
            "location": "/ws/setup/#storage-configuration", 
            "text": "This module lets you set up a Storage layer to write queries to when submitted. These are cleaned up when the query is terminated and the final result sent back to the API. This is particularly relevant if your Bullet instance is fielding long-running queries that need to be resilient. This coupled with a Backend implementation that can leverage the Storage lets you recreate queries in the Backend in case of component failure or restarts. The Storage layer is also particularly relevant if you're using the asynchronous query module with a PubSubResponder interface that relies on the Storage to do additional metadata lookups.  You can configure and provide a Storage implementation by implementing the  StorageManager interface . Note that you cannot turn off the Storage module in the API but by default, the  NullStorageManager  is used, which does nothing. You can provide a configuration yaml file that supplies your particular settings for your StorageManager by using the  bullet.storage.config  setting. See  the defaults  for more details.   So you DO have persistence?  This is not the same as storing the data. Bullet's philosophy is to avoid storing the incoming data stream that it is field queries on. This layer is meant for storing query related information. When we extend the storage to storing intermediate results in the backend for extra resiliency between windows, the size of the storage should still be well defined for sketch-based aggregations.", 
            "title": "Storage Configuration"
        }, 
        {
            "location": "/ws/setup/#asynchronous-query-configuration", 
            "text": "This module enables the asynchronous query submission endpoint (the  bullet.endpoint.async  setting) that lets you submit queries to it without having to hang around for the results to stream back. Instead you use  the PubSubResponder interface  to provide an instance that is used to write results that come back for that query. You can use this for  alerting  use-cases where you need to send e-mails on certain alert queries being triggered or if you want your results written to a PubSub that you can consume in a different manner etc.  By default, this module is disabled. However, it is mock configured to use  a standard Bullet PubSubResponder  that we provide to write the result back to a REST PubSub that is assumed to be running locally. You can change this to write the results to your own PubSub if you desire or plug in something else entirely. You can provide a configuration yaml file that supplies your particular settings for your PubSubResponder by using the  bullet.async.config  setting. See the  defaults  for more information.", 
            "title": "Asynchronous Query Configuration"
        }, 
        {
            "location": "/ws/setup/#metrics-configuration", 
            "text": "This module lets you monitor the Web Service for information on what is happening. It tracks the various status codes and publishes them using the  MetricPublisher interface  to a place of your choice. By default, the  HTTPMetricPublisher interface  is configured, which can post to a URL of your choice.  You can provide a configuration yaml file that supplies your particular settings for your MetricPublisher by using the  bullet.metric.config  setting. See the  defaults  for more information.", 
            "title": "Metrics Configuration"
        }, 
        {
            "location": "/ws/setup/#status-configuration", 
            "text": "This module periodically sends a  tick  query to the backend to make sure it is functioning properly. You can configure various settings for it here. If enabled, this module can disable the whole API if the backend is unreachable. This can be used if you front multiple Web Service instances talking to different instances of a backend behind a proxy and take down the backends one at a time for upgrades.", 
            "title": "Status Configuration"
        }, 
        {
            "location": "/ws/setup/#launch", 
            "text": "To launch, you will need your PubSub implementation JAR file and launch the application by providing the path to it. For example, if you only wished to provide the PubSub configuration and you had the Web Service jar and your chosen PubSub (say Kafka) in your current directory, you would run:  java -Dloader.path=bullet-kafka.jar -jar bullet-service.jar --bullet.pubsub.config=pubsub_settings.yaml  --logging.level.root=INFO  This launches the Web Service using Kafka as the PubSub, no custom schema (the default sample columns) and the default values in  settings . It also uses a root logging level of  INFO .  You could also tweak the various Bullet Web Service or Spring Boot settings by passing them in to the command above. For instance, you could also provide a path to your schema file using  --bullet.schema.file=/path/to/schema.json . You could also have a custom  application.yaml  file (you can change the name using  spring.config.name ) and pass it to the Web Service instead by running:  java -Dloader.path=bullet-kafka.jar -jar bullet-service.jar --spring.config.location=application.yaml", 
            "title": "Launch"
        }, 
        {
            "location": "/ws/setup/#usage", 
            "text": "Once the Web Service is up, you should be able to test to see if it's able to talk to the Bullet Backend:  You can HTTP POST a Bullet query to the API with:  curl -s -H  Content-Type: text/plain  -X POST -d '{}' http://localhost:5555/api/bullet/query  You should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1 when launching the Bullet Backend).   Context Path  The context path, or \"/api/bullet\" in the URL above can be changed using the Spring Boot setting  server.context-path . You can also change the port (defaults to port 5555) using  server.port .   If you provided a path to a schema file in your configuration file when you  launch  the Web Service, you can also HTTP GET your schema at  http://localhost:5555/api/bullet/columns  If you did not, the schema in  sample_fields.json  is the response. The Web Service converts it to a JSON API response and provides the right headers for CORS.", 
            "title": "Usage"
        }, 
        {
            "location": "/ws/api/", 
            "text": "Bullet API\n\n\nThis section gives a comprehensive overview of the Web Service API for launching Bullet queries.\n\n\nFor examples of queries, see the \nexamples page\n.\n\n\nBQL is the interface that is exposed to users to query Bullet. BQL queries that are received by the Web Service are converted to an underlying querying format before being sent to the backend. This conversion is done in the web service using \nthe bullet-bql library\n.\n\n\nOverview\n\n\nBullet-BQL provides users with a friendly SQL-like API to submit queries to the Web Service.\n\n\nStatement Syntax\n\n\nSELECT select\nFROM stream\n( WHERE expression )?\n( GROUP BY expression ( , expression )* )?\n( HAVING expression )?\n( ORDER BY orderBy )?\n( WINDOWING window )?\n( LIMIT Integer )?\n';'?\n\n\n\nwhere \nselect\n is\n\n\nDISTINCT? selectItem ( , selectItem )*\n\n\n\nand \nselectItem\n is one of\n\n\nexpression ( AS? identifier )?\n*\n\n\n\nand \nexpression\n is one of\n\n\nvalueExpression                                                                         \nfieldExpression                                                                         \nlistExpression                                                                          \nexpression IS NULL                                                                      \nexpression IS NOT NULL                                                                  \nunaryExpression                                                                         \nfunctionExpression                                                                      \nexpression NOT? IN expression                                    \nexpression RLIKE ANY? expression                                 \nexpression ( * | / ) expression                                  \nexpression ( + | - ) expression                                      \nexpression ( \n | \n= | \n | \n= ) ( ANY | ALL )? expression         \nexpression ( = | != ) ( ANY | ALL )? expression                    \nexpression AND expression                                                 \nexpression XOR expression                                                 \nexpression OR expression                                                  \n( expression )\n\n\n\nwhere \nvalueExpression\n is one of Null, Boolean, Integer, Long, Float, Double, or String\n\n\nand \nfieldExpression\n is one of\n\n\nidentifier ( : fieldType )?\nidentifier [ Integer ] ( : fieldType )?\nidentifier [ Integer ] . identifier ( : fieldType )?\nidentifier . identifier ( : fieldType )?\nidentifier . identifier . identifier ( : fieldType )?\n\n\n\nfieldType\n is one of\n\n\nprimitiveType\nLIST [ primitiveType ]\nMAP [ primitiveType ]\nLIST [ MAP [ primitiveType ] ]\nMAP [ MAP [ primitiveType ] ]\n\n\n\nand \nprimitiveType\n is \nINTEGER\n, \nLONG\n, \nFLOAT\n, \nDOUBLE\n, \nBOOLEAN\n, or \nSTRING\n\n\nwhere \nlistExpression\n is one of\n\n\n[]\n[ expression ( , expression )* ]\n\n\n\nunaryExpression\n is\n\n\n( NOT | SIZEOF ) ( expression )                                                 with optional parentheses\n\n\n\nfunctionExpression\n is one of\n\n\n( SIZEIS | CONTAINSKEY | CONTAINSVALUE | FILTER ) ( expression, expression )      \nIF ( expression ( , expression )* )                                             three arguments                         \naggregateExpression                               \nCAST ( expression AS primitiveType )\n\n\n\nwhere \naggregateExpression\n is one of\n\n\nCOUNT ( * )                                                    \n( SUM | AVG | MIN | MAX ) ( expression )                                \nCOUNT ( DISTINCT expression ( , expression )* )                                           \ndistributionType ( expression, inputMode )                            \nTOP ( Integer ( , Integer )?, expression ( , expression )* )\n\n\n\nwhere \ndistributionType\n is \nQUANTILE\n, \nFREQ\n, or \nCUMFREQ\n\n\nand \ninputMode\n is one of\n\n\nLINEAR, Integer                                                                 evenly spaced\nREGION, Number, Number, Number                                                  evenly spaced in a region\nMANUAL, Number ( , Number )*                                                    defined points\n\n\n\nand \nstream\n is one of\n\n\nSTREAM()                                                                        default time duration will be set from BQLConfig\nSTREAM( ( Integer | MAX ), TIME )                                               time based duration control\n\n\n\nRECORD\n will be supported in the future.\n\n\nand \norderBy\n is\n\n\nexpression ( ASC | DESC )? ( , expression ( ASC | DESC )? )*\n\n\n\nand \nwindow\n is one of\n\n\nEVERY ( Integer, ( TIME | RECORD ), include )\nTUMBLING ( Integer, ( TIME | RECORD ) )\n\n\n\ninclude\n is one of\n\n\nALL\nFIRST, Integer, ( TIME | RECORD )\n\n\n\nData Types\n\n\n\n\n\n\nNull\n: \nNULL\n.\n\n\n\n\n\n\nBoolean\n: \nTRUE\n, \nFALSE\n.\n\n\n\n\n\n\nInteger\n: 32-bit signed two\u2019s complement integer with a minimum value of \n-2^31\n and a maximum value of \n2^31 - 1\n. Example: \n65\n.\n\n\n\n\n\n\nLong\n: 64-bit signed two\u2019s complement integer with a minimum value of \n-2^63 + 1\n and a maximum value of \n2^63 - 1\n. Example: \n9223372036854775807\n, \n-9223372036854775807\n.\n\n\n\n\n\n\nFloat\n: 32-bit inexact, variable-precision with a minimum value of \n2^-149\n and a maximum value of \n(2-2^-23)\u00b72^127\n. Example: \n1.70141183E+38\n, \n1.17549435E-38\n, \n0.15625\n.\n\n\n\n\n\n\nDouble\n: 64-bit inexact, variable-precision with a minimum value of \n2^-1074\n and a maximum value of \n(2-2^-52)\u00b72^1023\n. Example: \n1.7976931348623157E+308\n, \n.17976931348623157E+309\n, \n4.9E-324\n.\n\n\n\n\n\n\nString\n: character string which can have escapes. Example: \n'this is a string'\n, \n'this is ''another'' string'\n.\n\n\n\n\n\n\nIdentifier\n: representation of a field. Unquoted identifier must start with a letter or \n_\n. Example: \ncolumn_name\n, \ncolumn_name.foo\n, \ncolumn_name.foo.bar\n, \ncolumn_name[0].bar\n, or \n\"123column\"\n.\n\n\n\n\n\n\nAll\n: representation of all fields. Example: \n*\n.", 
            "title": "BQL"
        }, 
        {
            "location": "/ws/api/#bullet-api", 
            "text": "This section gives a comprehensive overview of the Web Service API for launching Bullet queries.  For examples of queries, see the  examples page .  BQL is the interface that is exposed to users to query Bullet. BQL queries that are received by the Web Service are converted to an underlying querying format before being sent to the backend. This conversion is done in the web service using  the bullet-bql library .", 
            "title": "Bullet API"
        }, 
        {
            "location": "/ws/api/#overview", 
            "text": "Bullet-BQL provides users with a friendly SQL-like API to submit queries to the Web Service.", 
            "title": "Overview"
        }, 
        {
            "location": "/ws/api/#statement-syntax", 
            "text": "SELECT select\nFROM stream\n( WHERE expression )?\n( GROUP BY expression ( , expression )* )?\n( HAVING expression )?\n( ORDER BY orderBy )?\n( WINDOWING window )?\n( LIMIT Integer )?\n';'?  where  select  is  DISTINCT? selectItem ( , selectItem )*  and  selectItem  is one of  expression ( AS? identifier )?\n*  and  expression  is one of  valueExpression                                                                         \nfieldExpression                                                                         \nlistExpression                                                                          \nexpression IS NULL                                                                      \nexpression IS NOT NULL                                                                  \nunaryExpression                                                                         \nfunctionExpression                                                                      \nexpression NOT? IN expression                                    \nexpression RLIKE ANY? expression                                 \nexpression ( * | / ) expression                                  \nexpression ( + | - ) expression                                      \nexpression (   |  = |   |  = ) ( ANY | ALL )? expression         \nexpression ( = | != ) ( ANY | ALL )? expression                    \nexpression AND expression                                                 \nexpression XOR expression                                                 \nexpression OR expression                                                  \n( expression )  where  valueExpression  is one of Null, Boolean, Integer, Long, Float, Double, or String  and  fieldExpression  is one of  identifier ( : fieldType )?\nidentifier [ Integer ] ( : fieldType )?\nidentifier [ Integer ] . identifier ( : fieldType )?\nidentifier . identifier ( : fieldType )?\nidentifier . identifier . identifier ( : fieldType )?  fieldType  is one of  primitiveType\nLIST [ primitiveType ]\nMAP [ primitiveType ]\nLIST [ MAP [ primitiveType ] ]\nMAP [ MAP [ primitiveType ] ]  and  primitiveType  is  INTEGER ,  LONG ,  FLOAT ,  DOUBLE ,  BOOLEAN , or  STRING  where  listExpression  is one of  []\n[ expression ( , expression )* ]  unaryExpression  is  ( NOT | SIZEOF ) ( expression )                                                 with optional parentheses  functionExpression  is one of  ( SIZEIS | CONTAINSKEY | CONTAINSVALUE | FILTER ) ( expression, expression )      \nIF ( expression ( , expression )* )                                             three arguments                         \naggregateExpression                               \nCAST ( expression AS primitiveType )  where  aggregateExpression  is one of  COUNT ( * )                                                    \n( SUM | AVG | MIN | MAX ) ( expression )                                \nCOUNT ( DISTINCT expression ( , expression )* )                                           \ndistributionType ( expression, inputMode )                            \nTOP ( Integer ( , Integer )?, expression ( , expression )* )  where  distributionType  is  QUANTILE ,  FREQ , or  CUMFREQ  and  inputMode  is one of  LINEAR, Integer                                                                 evenly spaced\nREGION, Number, Number, Number                                                  evenly spaced in a region\nMANUAL, Number ( , Number )*                                                    defined points  and  stream  is one of  STREAM()                                                                        default time duration will be set from BQLConfig\nSTREAM( ( Integer | MAX ), TIME )                                               time based duration control  RECORD  will be supported in the future.  and  orderBy  is  expression ( ASC | DESC )? ( , expression ( ASC | DESC )? )*  and  window  is one of  EVERY ( Integer, ( TIME | RECORD ), include )\nTUMBLING ( Integer, ( TIME | RECORD ) )  include  is one of  ALL\nFIRST, Integer, ( TIME | RECORD )", 
            "title": "Statement Syntax"
        }, 
        {
            "location": "/ws/api/#data-types", 
            "text": "Null :  NULL .    Boolean :  TRUE ,  FALSE .    Integer : 32-bit signed two\u2019s complement integer with a minimum value of  -2^31  and a maximum value of  2^31 - 1 . Example:  65 .    Long : 64-bit signed two\u2019s complement integer with a minimum value of  -2^63 + 1  and a maximum value of  2^63 - 1 . Example:  9223372036854775807 ,  -9223372036854775807 .    Float : 32-bit inexact, variable-precision with a minimum value of  2^-149  and a maximum value of  (2-2^-23)\u00b72^127 . Example:  1.70141183E+38 ,  1.17549435E-38 ,  0.15625 .    Double : 64-bit inexact, variable-precision with a minimum value of  2^-1074  and a maximum value of  (2-2^-52)\u00b72^1023 . Example:  1.7976931348623157E+308 ,  .17976931348623157E+309 ,  4.9E-324 .    String : character string which can have escapes. Example:  'this is a string' ,  'this is ''another'' string' .    Identifier : representation of a field. Unquoted identifier must start with a letter or  _ . Example:  column_name ,  column_name.foo ,  column_name.foo.bar ,  column_name[0].bar , or  \"123column\" .    All : representation of all fields. Example:  * .", 
            "title": "Data Types"
        }, 
        {
            "location": "/ws/examples/", 
            "text": "Query Examples\n\n\nRather than sourcing the examples from the Quick Start, these examples are real-world ones sourced from Bullet running on raw, user events generated by instrumentation on Yahoo sites by Yahoo employees (not all Yahoo users).\n\n\n\n\nDisclaimer\n\n\nThe actual data shown here has been edited and is not how actual Yahoo user events look.\n\n\n\n\nSimplest Query\n\n\nThe simplest query you could write would be:\n\n\nSELECT * FROM STREAM();\n\n\n\n\nThis query would get any records that pass through for a default duration of 20000 ms up to a max default of 500 records.\n\n\nSELECT * FROM STREAM(10000, TIME) LIMIT 1;\n\n\n\n\nIf you wanted to write a smaller or shorter query to, for example, quickly test your connection to Bullet, you could adjust the query duration and size like shown above. This query would only last for a duration of 10000 ms and return at most 1 record.  \n\n\n\n\nWINDOW?\n\n\nThere is only one unified data stream in Bullet, so for clarity the \nFROM\n clause is given a \nSTREAM\n function to denote the look-forward time window for the Bullet query.\n\n\n\n\nSimple Filtering\n\n\nSELECT *\nFROM STREAM(30000, TIME)\nWHERE id = 'btsg8l9b234ha'\nLIMIT 1;\n\n\n\n\nBecause of the default constraints, this query would find at most 1 record with the id matching the value provided. The record would have all its fields.\n\n\nA sample response could be (it has been edited to remove PII and other Yahoo data). The response contains a single matching record, and the associated meta information.\n\n\n{\n   \nrecords\n:[\n       {\n           \nserver_name\n:\nEDITED\n,\n           \npage_uri\n:\n/\n,\n           \nis_page_view\n:true,\n           \ndevice\n:\ntablet\n,\n           \ndebug_codes\n:{\n               \nhttp_status_code\n:\n200\n\n           },\n           \nreferrer_domain\n:\nwww.yahoo.com\n,\n           \nis_logged_in\n:true,\n           \ntimestamp\n:1446842189000,\n           \nevent_family\n:\nview\n,\n           \nid\n:\nbtsg8l9b234ha\n,\n           \nos_name\n:\nmac os\n,\n           \ndemographics\n:{\n               \nage\n : \n25\n,\n               \ngender\n : \nm\n,\n            }\n       }\n    ],\n    \nmeta\n:{\n        \nquery_id\n:1167304238598842449,\n        \nquery_body\n:\n{}\n,\n        \nquery_finish_time\n:1480723799550,\n        \nquery_receive_time\n:1480723799540\n    }\n}\n\n\n\n\nRelational Filters and Projections\n\n\nSELECT timestamp AS ts, device_timestamp AS device_ts,\n       event AS event, page_domain AS domain, page_id AS id\nFROM STREAM(20000, TIME)\nWHERE id = 'btsg8l9b234ha' AND page_id IS NOT NULL\nLIMIT 10;\n\n\n\n\nThe above query finds all events where id is set to 'btsg8l9b234ha' and page_id is not null, projects the fields selected above with their aliases (timestamp as ts, etc.) and limits the results to at most 10 records. The query would wait at most 20 seconds for records to show up.\n\n\nThe resulting response could look like (only 3 events were generated that matched the criteria):\n\n\n{\n    \nrecords\n: [\n        {\n            \ndomain\n: \nhttp://some.url.com\n,\n            \ndevice_ts\n: 1481152233788,\n            \nid\n: 2273844742998,\n            \nevent\n: \npage\n,\n            \nts\n: null\n        },\n        {\n            \ndomain\n: \nwww.yahoo.com\n,\n            \ndevice_ts\n: 1481152233788,\n            \nid\n: 227384472956,\n            \nevent\n: \nclick\n,\n            \nts\n: 1481152233888\n        },\n        {\n            \ndomain\n: \nhttps://news.yahoo.com\n,\n            \ndevice_ts\n: null,\n            \nid\n: 2273844742556,\n            \nevent\n: \npage\n,\n            \nts\n: null\n        }\n    ],\n    \nmeta\n: {\n        \nquery_id\n: -3239746252817510000,\n        \nquery_body\n: \nEDITED OUT\n,\n        \nquery_finish_time\n: 1481152233799,\n        \nquery_receive_time\n: 1481152233796\n    }\n}\n\n\n\n\nRelational Filters using the extended value notation for static values\n\n\nFor the following examples, we will simply show and explain the queries. They also use the extended syntax for specify values in a filter using the \nkind\n field.\n\n\nSIZEOF Filter\n\n\nThis query checks to see if the size of the \ndata_map\n is equal to 4 and returns all records that satisfy this.\n\n\nSELECT *\nFROM STREAM(30000, TIME)\nWHERE SIZEOF(data_map) = 4\nLIMIT 1;\n\n\n\n\nCONTAINSKEY Filter\n\n\nThis query checks to see if the \ndata_map\n contains the key \nid\n and returns all records for which this is true.\n\n\nSELECT *\nFROM STREAM(30000, TIME)\nWHERE CONTAINSKEY(data_map, 'id')\nLIMIT 1;\n\n\n\n\nCONTAINSVALUE Filter\n\n\nThis query checks to see if the \ndata_map\n does not contain the value \nbtsg8l9b234ha\n and returns all records for which this is true. If this was applied on a list field or list of maps field, the inner maps would be checked instead.\n\n\nSELECT *\nFROM STREAM(30000, TIME)\nWHERE NOT CONTAINSVALUE(data_map, 'btsg8l9b234ha')\nLIMIT 1;\n\n\n\n\nRelational Filter comparing to other fields\n\n\nInstead of comparing to static, constant values, you may use the extended values notation and set \nkind\n to \nFIELD\n to  compare to other fields within the same record. The following query returns the first record for which the \nid\n field is set to the \nuid\n field.\n\n\nSELECT *\nFROM STREAM(30000, TIME)\nWHERE id = uid\nLIMIT 1;\n\n\n\n\nA sample result could look like:\n\n\n{\n    \nrecords\n: [\n        {\n            \nuid\n:\n0qcgofdbfqs9s\n,\n            \nexperience\n:\nweb\n,\n            \nlid\n:\n978500434\n,\n            \nid\n:\n0qcgofdbfqs9s\n,\n            \nother fields\n: \nEDITED OUT\n\n        }\n    ],\n    \nmeta\n: {\n        \nquery_id\n: \nc4a336e0-3bb5-452a-8503-40d8751b92d9\n,\n        \nquery_body\n: \nEDITED OUT\n,\n        \nquery_finish_time\n: 1536192342505,\n        \nquery_receive_time\n: 1536192342507\n    }\n}\n\n\n\n\nLogical Filters and Projections\n\n\nSELECT id AS id, experience AS experience, page_id AS pid,\n       link_id AS lid, tags AS tags, demographics.age AS age\nFROM STREAM(60000, TIME)\nWHERE (id = 'c14plm1begla7' AND ((experience = 'web' AND page_id IN ['18025', '47729'])\n                                  OR link_id RLIKE '2.*'))\n      OR (tags.player = 'true' AND demographics.age \n '65')\nLIMIT 1;\n\n\n\n\n\n\nTyping\n\n\nIf demographics.age was of type Long, then Bullet will convert 85 to be an Long, but in this example, we are pretending that it is String.  So, no conversion is made. Similarly for link_id, id, experience and page_id. tags is a Map of String to Boolean so Bullet converts \n\"true\"\n to the Boolean \ntrue\n. See below on how to rewrite it with casting.\n\n\n\n\nLogical Filters and Projections with Casting\n\n\n```SQL\nSELECT id AS id, experience AS experience, page_id AS pid,\n       link_id AS lid, tags AS tags, CAST(demographics.age AS LONG) AS age\nFROM STREAM(60000, TIME)\nWHERE (id = 'c14plm1begla7' AND ((experience = 'web' AND page_id IN ['18025', '47729'])\n                                  OR link_id RLIKE '2.*'))\n      OR (tags.player = 'true' AND CAST(demographics.age AS LONG) \n 65L)\nLIMIT 1;\n```\n\n\n\nThis query is looking for a single event with a specific id and either the page_id is in two specific pages on the \"web\" experience or with a link_id that starts with 2, or a player event where the age is greater than \"65\". In other words, it is looking for senior citizens who generate video player events or the events of a particular person (based on id) events on two specific pages or a group of pages that have link that have ids that start with 2. It then projects out only these fields with different names.\n\n\nA sample result could look like (it matched because of tags.player was true and demographics.age was \n 65):\n\n\n{\n    \nrecords\n: [\n        {\n            \npid\n:\n158\n,\n            \nid\n:\n0qcgofdbfqs9s\n,\n            \nexperience\n:\nweb\n,\n            \nlid\n:\n978500434\n,\n            \nage\n:66,\n            \ntags\n:{\nplayer\n:true}\n        }\n    ],\n    \nmeta\n: {\n        \nquery_id\n: 3239746252812284004,\n        \nquery_body\n: \nEDITED OUT\n,\n        \nquery_finish_time\n: 1481152233805,\n        \nquery_receive_time\n: 1481152233881\n    }\n}\n\n\n\n\nGROUP ALL COUNT Aggregation\n\n\nAn example of a query performing a COUNT all records aggregation would look like:\n\n\nSELECT COUNT(*) AS numSeniors\nFROM STREAM(20000, TIME)\nWHERE demographics.age \n 65\n\n\n\n\nThis query will count the number events for which demographics.age \n 65. The aggregation type GROUP indicates that it is a group aggregation. To group by a key, the \nfields\n key needs to be set in the \naggregation\n part of the query. If \nfields\n is empty or is omitted (as it is in the query above) and the \ntype\n is \nGROUP\n, it is as if all the records are collapsed into a single group - a \nGROUP ALL\n. Adding a \nCOUNT\n in the \noperations\n part of the \nattributes\n indicates that the number of records in this group will be counted, and the \"newName\" key denotes the name the resulting column \"numSeniors\" in the result. Setting the duration to 20000 counts matching records for\nthis duration.\n\n\nA sample result would look like:\n\n\n{\n    \nrecords\n: [\n        {\n            \nnumSeniors\n: 363201\n        }\n    ],\n    \nmeta\n: {}\n}\n\n\n\n\nThis result indicates that 363,201 records were counted with demographics.age \n 65 during the 20s the query was running.\n\n\nGROUP ALL Multiple Aggregations\n\n\nCOUNT is the only GROUP operation for which you can omit a \"field\".\n\n\nSELECT COUNT(*) AS numCalifornians, AVG(demographics.age) AS avgAge,\n       MIN(demographics.age) AS minAge, MAX(demographics.age) AS maxAge\nFROM STREAM(20000, TIME)\nWHERE demographics.state = 'california'\n\n\n\n\nNote that the \nGROUP BY ()\n is optional.\n\n\nA sample result would look like:\n\n\n{\n    \nrecords\n: [\n        {\n            \nmaxAge\n: 94.0,\n            \nnumCalifornians\n: 188451,\n            \nminAge\n: 6.0,\n            \navgAge\n: 33.71828\n        }\n    ],\n    \nmeta\n: {\n        \nquery_id\n: 8051040987827161000,\n        \nquery_body\n: \nEDITED OUT\n,\n        \nquery_finish_time\n: 1482371927435,\n        \nquery_receive_time\n: 1482371916625\n    }\n}\n\n\n\n\nThis result indicates that, among the records observed during the 20s this query ran, there were 188,451 users with demographics.state equal to \"california\". Among these users the average age was 33.71828, the max age observed was 94, and the minimum age observed was 6.\n\n\nExact COUNT DISTINCT Aggregation\n\n\nSELECT COUNT(DISTINCT browser_name, browser_version) AS \nCOUNT DISTINCT\n\nFROM STREAM(10000, TIME);\n\n\n\n\nThis gets the count of the unique browser names and versions in the next 30s (default duration). Note that we do not specify values for the keys in fields. This is because they are not relevant\n\n\n{\n    \nrecords\n: [\n        {\n            \nCOUNT DISTINCT\n: 158.0\n        }\n    ],\n    \nmeta\n: {\n        \nquery_id\n: 4451146261377394443,\n        \nsketches\n: {\n            \nstandard_deviations\n: {\n                \n1\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                },\n                \n2\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                },\n                \n3\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                }\n            },\n            \nwas_estimated\n: false,\n            \nfamily\n: \nTHETA\n,\n            \ntheta\n: 1.0,\n            \nsize\n: 1280\n        },\n        \nquery_body\n: \nEDITED OUT\n,\n        \nquery_finish_time\n: 1484084869073,\n        \nquery_receive_time\n: 1484084832684\n    }\n}\n\n\n\n\nThere were 158 unique combinations on browser names and versions in our dataset for those 30 s. Note the new \nsketches\n object in the meta. It has various metadata about the result. In particular, the \nwas_estimated\n key denotes where the result\nwas estimated or not. The \nstandard_deviations\n key denotes the confidence at various sigmas: 1 (1 sigma = ~68% confidence, 2 sigma = ~95% confidence, 3 sigma = ~99% confidence). Since this result was not estimated, the result is the same as the upper and lower bounds for the result.\n\n\nApproximate COUNT DISTINCT\n\n\nSELECT COUNT(DISTINCT ip_address) AS uniqueIPs\nFROM STREAM(10000, TIME);\n\n\n\n\nThis query gets us the unique IP addresses in the next 10 s. It renames the result column from \"COUNT DISTINCT\" to \"uniqueIPs\".\n\n\n{\n   \nrecords\n:[\n      {\n         \nuniqueIPs\n:130551.07952805843\n      }\n   ],\n   \nmeta\n:{\n      \nquery_id\n:5377782455857451480,\n      \nsketches\n:{\n         \nstandard_deviations\n:{\n            \n1\n:{\n               \nupperBound\n:131512.85413760383,\n               \nlowerBound\n:129596.30223107953\n            },\n            \n2\n:{\n               \nupperBound\n:132477.15103015225,\n               \nlowerBound\n:128652.93906100772\n            },\n            \n3\n:{\n               \nupperBound\n:133448.49248615955,\n               \nlowerBound\n:127716.46773622213\n            }\n         },\n         \nwas_estimated\n:true,\n         \nfamily\n:\nTHETA\n,\n         \ntheta\n:0.12549877074343688,\n         \nsize\n:131096\n      },\n      \nquery_body\n:\nEDITED OUT\n,\n      \nquery_finish_time\n:1484090240812,\n      \nquery_receive_time\n:1484090223351\n   }\n}\n\n\n\n\nThe number of unique IPs in our dataset was 130551 in those 10s (approximately) with the true value between (129596, 131512) at 68% confidence, (128652, 132477) at 95% confidence and (127716, 133448) at 99% confidence. In the \nworst\n case at 3 sigma (99% confidence),\nour error is 2.17%. The final result was computed with 131096 bytes or ~128 KiB as denoted by \nsize\n. This happens to be maximum size the the COUNT DISTINCT sketch will take up at the default nominal entries, so even if we had billions of unique IPs, the size will be the same and the error may be higher (depends on the distribution). For example, the error when the same query was run for 30s was 2.28% at 99% confidence (actual unique IPs: 559428, upper bound: 572514). In fact, the worst the error can get at this\nSketch size is 2.34% as defined \nhere\n, \nregardless of the number of unique entries added to the Sketch!\n.\n\n\nDISTINCT Aggregation\n\n\nSELECT browser_name AS browser\nFROM STREAM(30000, TIME)\nGROUP BY browser_name\nLIMIT 10;\n\n\n\n\nSELECT DISTINCT browser_name AS browser\nFROM STREAM(30000, TIME)\nLIMIT 10;\n\n\n\n\nThis query gets the distinct values for the browser_name field and limit the results to 10. It runs for 30 s.\n\n\n{\n   \nrecords\n:[\n      {\n         \nbrowser\n:\nopera\n\n      },\n      {\n         \nbrowser\n:\nflock\n\n      },\n      {\n         \nbrowser\n:\nlinks\n\n      },\n      {\n         \nbrowser\n:\nmozilla firefox\n\n      },\n      {\n         \nbrowser\n:\ndolfin\n\n      },\n      {\n         \nbrowser\n:\nlynx\n\n      },\n      {\n         \nbrowser\n:\nchrome\n\n      },\n      {\n         \nbrowser\n:\nmicrosoft internet explorer\n\n      },\n      {\n         \nbrowser\n:\naol browser\n\n      },\n      {\n         \nbrowser\n:\nedge\n\n      }\n   ],\n   \nmeta\n:{\n      \nquery_id\n:-4872093887360741287,\n      \nsketches\n:{\n         \nstandard_deviations\n:{\n            \n1\n:{\n               \nupperBound\n:28.0,\n               \nlowerBound\n:28.0\n            },\n            \n2\n:{\n               \nupperBound\n:28.0,\n               \nlowerBound\n:28.0\n            },\n            \n3\n:{\n               \nupperBound\n:28.0,\n               \nlowerBound\n:28.0\n            }\n         },\n         \nwas_estimated\n:false,\n         \nfamily\n:\nTUPLE\n,\n         \nuniques_estimate\n:28.0,\n         \ntheta\n:1.0\n      },\n      \nquery_body\n:\nEDITED OUT\n,\n      \nquery_finish_time\n:1485469087971,\n      \nquery_receive_time\n:1485469054070\n   }\n}\n\n\n\n\nThere were 28 unique results but we asked for 10, so the query returned a uniform sample across the 28 distinct values.\n\n\nDISTINCT is just an alias for GROUP. A GROUP by with no operations is exactly a DISTINCT.\n\n\nGROUP by Aggregation\n\n\nSELECT demographics.country AS country, device AS device,\n       COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nGROUP BY demographics.country, device\nLIMIT 50;\n\n\n\n\nThis query groups by the country and the device and for each unique group gets the count, average age and time spent by the users for the next 20 seconds. It renames demographics.country to country and does not rename device. It limits the groups to 50. If there were more than\n50 groups, the results would be a uniform sampling of the groups (but each group in the result would have the correct result). These parameters can all be tweaked \nin the configuration\n.\n\n\n{\n   \nrecords\n:[\n      {\n         \ncountry\n:\nuk\n,\n         \ndevice\n:\ndesktop\n,\n         \ncount\n:203034,\n         \naverageAge\n:32.42523,\n         \naverageTimespent\n:1.342\n      },\n      {\n         \ncountry\n:\nus\n,\n         \ndevice\n:\ndesktop\n,\n         \ncount\n:1934030,\n         \naverageAge\n:29.42523,\n         \naverageTimespent\n:3.234520\n      },\n      \n...EDITED 41 other such records out for readability...\n,\n   ],\n   \nmeta\n:{\n      \nquery_id\n:1705911449584057747,\n      \nsketches\n:{\n         \nstandard_deviations\n:{\n            \n1\n:{\n               \nupperBound\n:43.0,\n               \nlowerBound\n:43.0\n            },\n            \n2\n:{\n               \nupperBound\n:43.0,\n               \nlowerBound\n:43.0\n            },\n            \n3\n:{\n               \nupperBound\n:43.0,\n               \nlowerBound\n:43.0\n            }\n         },\n         \nwas_estimated\n:false,\n         \nfamily\n:\nTUPLE\n,\n         \nuniques_estimate\n:43.0,\n         \ntheta\n:1.0\n      },\n      \nquery_body\n:\nEDITED OUT\n,\n      \nquery_finish_time\n:1485217172780,\n      \nquery_receive_time\n:1485217148840\n   }\n}\n\n\n\n\nWe received 43 rows for this result. The maximum groups that was allowed for the instance of Bullet was 512. If there were more groups than the maximum specified by your configuration, \na uniform sample\n across them would be chosen\nfor the result. However, for each group, the values computed (average, count) would be exact. The standard deviations, whether the result was estimated and the number of approximate uniques in the metadata would reflect the change.\n\n\nIf you asked for 50 rows in the aggregation (as the query did above) but there were more than 50 in the result (but \n 512), the metadata would reflect the fact that the result was not estimated. You would still get a uniform sample\nbut by increasing your aggregation size higher, you could get the rest.\n\n\nFor readability, if you were just trying to get the unique values for a field or a set of fields, you could leave out the attributes section and specify your fields section. You could also call the type \nDISTINCT\n instead of\n\nGROUP\n to make that explicit. \nDISTINCT\n is just an alias for \nGROUP\n. See \nthe DISTINCT example\n.\n\n\nQUANTILE DISTRIBUTION\n\n\nSELECT QUANTILE(duration, LINEAR, 11)\nFROM STREAM(5000, TIME);\n\n\n\n\nThis query creates 11 points from 0 to 1 (both inclusive) and finds the percentile values of the \nduration\n field (which contains an amount of time in ms) at \n0, 0.1, 0.2 ... 1.0\n or the 0th, 10th, 20th and 100th percentiles. It runs for 5 seconds and returns at most 11 points. As long as the \nsize\n is set to higher than the number of points you generate, \nDISTRIBUTION\n queries will return all your values.\n\n\nThe SQL is not really the same since it will produce one row instead of 11.\n\n\n{\n   \nrecords\n:[\n      {\n         \nValue\n:1,\n         \nQuantile\n:0\n      },\n      {\n         \nValue\n:1352,\n         \nQuantile\n:0.1\n      },\n      {\n         \nValue\n:3045,\n         \nQuantile\n:0.2\n      },\n      {\n         \nValue\n:6501,\n         \nQuantile\n:0.30000000000000004\n      },\n      {\n         \nValue\n:10700,\n         \nQuantile\n:0.4\n      },\n      {\n         \nValue\n:17488,\n         \nQuantile\n:0.5\n      },\n      {\n         \nValue\n:28659,\n         \nQuantile\n:0.6\n      },\n      {\n         \nValue\n:47929,\n         \nQuantile\n:0.7\n      },\n      {\n         \nValue\n:83447,\n         \nQuantile\n:0.7999999999999999\n      },\n      {\n         \nValue\n:177548,\n         \nQuantile\n:0.8999999999999999\n      },\n      {\n         \nValue\n:83525609,\n         \nQuantile\n:1\n      }\n   ],\n   \nmeta\n:{\n      \nquery_finish_time\n:1493748546533,\n      \nquery_body\n: \nEDITED OUT\n,\n      \nquery_id\n:2981902209347343400,\n      \nsketches\n:{\n         \nnormalized_rank_error\n:0.002389303789572841,\n         \nsize\n:16416,\n         \nminimum_value\n:1,\n         \nitems_seen\n:1414,\n         \nmaximum_value\n:83525609,\n         \nfamily\n:\nQUANTILES\n,\n         \nwas_estimated\n:false\n      },\n      \nquery_receive_time\n:1493748538259\n   }\n}\n\n\n\n\nThe result shows the values at the 0th, 10th percentiles etc. The \nwas_estimated\n key indicates that the result was not approximated. Note the the \nminimum_value\n and the \nmaximum_value\n correspond to the 0th and 100th percentiles. There is also a \nnormalized_rank_error\n that describes the error (see \nbelow\n for a detailed explanation) This is constant for all \nDISTRIBUTION\n queries and does not depend on the data or the query.\n\n\nNormalized Rank Error\n\n\nUnlike \nGROUP\n and \nCOUNT DISTINCT\n, the order in which the data arrives to Bullet can affect the results of a \nDISTRIBUTION\n query. The error when the result is estimated is not a Gaussian error function\nand is not described in terms of the values of your field. In other words, if the 50th percentile was estimated to some value, you could not bound the true median by using the the estimated\nvalue +/- constant (see below for a good approximation). The error is expressed in terms of the \nnormalized rank\n. If one were to sort the true data stream, you would obtain a rank for each item from\n0 to the stream length (\nitems_seen\n in the metadata above). If you then divided each rank by length, you would get ranks from 0 to 1. In this domain, a \nnormalized rank error\n of 0.002, for example, means that a value\nreturned for the 0.50 or 50th percentile could actually lie between 0.498 and 0.502 in the normalized ranks with 99% confidence.\n\n\nDistribution Accuracy\n lists the normalized rank error as a percentage for the maximum size of the Sketch used. If you obtain successive quantile values at\ngranularities lower than this rank error, the results may not be accurate. While the sketch speaks of the normalized rank error, you can still obtain reasonable bounds for values. For example, if the normalized rank\nerror was 1% and you obtained quantile values at 0.48, 0.50 and 0.52, you could use the values at 0.52 and 0.48 as very reasonable upper and lower bounds on your true median (you might even be able to use 0.49 and 0.51\nif the error was 1%).\n\n\nPMF DISTRIBUTION Aggregation\n\n\nSELECT FREQ(duration, REGION, 2000, 20000, 500)\nFROM STREAM(5000, TIME);\n\n\n\n\nThis query creates 37 points from 2000 to 20000 in 500 increments to bucketize the duration field using these points as split locations and finds the count of duration values that fall into these intervals. It runs for 5s and returns at most 100 records (this means it will return the 38 records).\n\n\nThe SQL does not include the \n(-\n to 2000)\n and the \n[20000 to +\n)\n intervals and does not produce a probability.\n\n\n{\n   \nrecords\n:[\n      {\n         \nProbability\n:0.1518054532056006,\n         \nCount\n:206,\n         \nRange\n:\n(-\u221e to 2000.0)\n\n      },\n      {\n         \nProbability\n:0.0397936624907885,\n         \nCount\n:53.99999999999999,\n         \nRange\n:\n[2000.0 to 2500.0)\n\n      },\n      \n...EDITED 34 other such records out for readability...\n,\n      {\n         \nProbability\n:0.0058953574060427415,\n         \nCount\n:8,\n         \nRange\n:\n[19500.0 to 20000.0)\n\n      },\n      {\n         \nProbability\n:0.45689019896831246,\n         \nCount\n:620,\n         \nRange\n:\n[20000.0 to +\u221e)\n\n      }\n   ],\n   \nmeta\n:{\n      \nquery_finish_time\n:1493750074795,\n      \nquery_body\n: \nEDITED OUT\n,\n      \nquery_id\n:-2590566941995678000,\n      \nsketches\n:{\n         \nnormalized_rank_error\n:0.002389303789572841,\n         \nsize\n:16416,\n         \nminimum_value\n:1,\n         \nitems_seen\n:1357,\n         \nmaximum_value\n:78240570,\n         \nfamily\n:\nQUANTILES\n,\n         \nwas_estimated\n:false\n      },\n      \nquery_receive_time\n:1493750066022\n   }\n}\n\n\n\n\nThe result consists of 38 records, each denoting an interval in the domain we asked for. The result was not estimated. Note that the interval is denoted by the \nRange\n key and the count by the \nCount\n key. There is also a probability that estimates how likely a value for duration is likely to fall into that range.\n\n\nCDF DISTRIBUTION Aggregation\n\n\nSELECT CUMFREQ(duration, MANUAL, 20000, 2000, 15000, 45000)\nFROM STREAM(5000, TIME);\n\n\n\n\nThis query specifies a list of points manually using \npoints\n property in \nattributes\n. It runs for 5s and finds the\ncumulative frequency distribution using the specified points as break points. It returns at most 100 records (which means we will\nget all of the intervals).\n\n\nThere is no easy SQL equivalent because the points are free-form. It does not produce a probability field like Bullet does.\n\n\n{\n   \nrecords\n:[\n      {\n         \nProbability\n:0.14382632293080055,\n         \nCount\n:212.00000000000003,\n         \nRange\n:\n(-\u221e to 2000.0)\n\n      },\n      {\n         \nProbability\n:0.5210312075983717,\n         \nCount\n:767.9999999999999,\n         \nRange\n:\n(-\u221e to 15000.0)\n\n      },\n      {\n         \nProbability\n:0.5603799185888738,\n         \nCount\n:826,\n         \nRange\n:\n(-\u221e to 20000.0)\n\n      },\n      {\n         \nProbability\n:0.6994572591587517,\n         \nCount\n:1031,\n         \nRange\n:\n(-\u221e to 45000.0)\n\n      },\n      {\n         \nProbability\n:1,\n         \nCount\n:1474,\n         \nRange\n:\n(-\u221e to +\u221e)\n\n      }\n   ],\n   \nmeta\n:{\n      \nquery_finish_time\n:1493755151660,\n      \nquery_body\n: \nEDITED OUT\n,\n      \nquery_id\n:-8460702488693518000,\n      \nsketches\n:{\n         \nnormalized_rank_error\n:0.002389303789572841,\n         \nsize\n:16416,\n         \nminimum_value\n:2,\n         \nitems_seen\n:1474,\n         \nmaximum_value\n:10851113,\n         \nfamily\n:\nQUANTILES\n,\n         \nwas_estimated\n:false\n      },\n      \nquery_receive_time\n:1493755143626\n   }\n}\n\n\n\n\nThe result contains the 5 intervals produced by the split points. It was not estimated so these counts are exact. Note that the start of each interval is \n-\n because\nit is the cumulative frequency distribution.\n\n\nExact TOP K Aggregation\n\n\nThere are two methods for executing a TOP K aggregation in BQL:\n\n\nSELECT TOP(500, 100, demographics.country, browser_name) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE demographics.country IS NOT NULL AND browser_name IS NOT NULL;\n\n\n\n\nOR:\n\n\nSELECT demographics.country, browser_name, COUNT(*) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE demographics.country IS NOT NULL AND browser_name IS NOT NULL\nGROUP BY demographics.country, browser_name\nHAVING COUNT(*) \n= 100\nORDER BY COUNT(*) DESC\nLIMIT 500;\n\n\n\n\nThis query gets the top 500 country, browser combinations where the count of records for each combination is at least 100. It runs for 10s.\n\n\n{\n   \nrecords\n:[\n      {\n         \ncountry\n:\nus\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:2729\n      },\n      {\n         \ncountry\n:\nus\n,\n         \nbrowser\n:\nmozilla firefox\n,\n         \nnumEvents\n:1072\n      },\n      {\n         \ncountry\n:\nuk\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:703\n      },\n      {\n         \ncountry\n:\nfr\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:383\n      },\n      {\n         \ncountry\n:\nfr\n,\n         \nbrowser\n:\nmozilla firefox\n,\n         \nnumEvents\n:278\n      },\n      {\n         \ncountry\n:\nes\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:234\n      },\n      \n...EDITED 10 other such records here for readability\n,\n      {\n         \ncountry\n:\nes\n,\n         \nbrowser\n:\nmozilla firefox\n,\n         \nnumEvents\n:102\n      },\n      {\n         \ncountry\n:\nfr\n,\n         \nbrowser\n:\napple safari\n,\n         \nnumEvents\n:101\n      }\n   ],\n   \nmeta\n:{\n      \nquery_finish_time\n:1493760034414,\n      \nquery_body\n: \nEDITED OUT\n,\n      \nquery_id\n:7515243052399540000,\n      \nsketches\n:{\n         \nmaximum_count_error\n:0,\n         \nactive_items\n:431,\n         \nitems_seen\n:10784,\n         \nfamily\n:\nFREQUENCY\n,\n         \nwas_estimated\n:false\n      },\n      \nquery_receive_time\n:1493760020807\n   }\n}\n\n\n\n\nThe results gave us the top 18 country, browser combinations that had counts over a 100. Note the \nmaximum_count_error\n key in the metadata. This represents how off the count is. It is 0 because these counts are exact.\nIn our data stream, we only had 18 unique combinations of countries and browser names at the time the query was run.\n\n\nApproximate TOP K Aggregation\n\n\nThere are two methods for executing a TOP K aggregation in BQL:\n\n\nSELECT TOP(10, 100, browser_name, browser_version, os_name, os_version, demographics.country, demographics.state) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE os_name IS NOT NULL AND browser_name IS NOT NULL;\n\n\n\n\nOR:\n\n\nSELECT browser_name, browser_version, os_name, os_version, demographics.country, demographics.state, COUNT(*) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE os_name IS NOT NULL AND browser_name IS NOT NULL\nGROUP BY browser_name, browser_version, os_name, os_version, demographics.country, demographics.state\nHAVING COUNT(*) \n= 100\nORDER BY COUNT(*) DESC\nLIMIT 10;\n\n\n\n\nIn order to make the result approximate, this query adds more dimensions to the \nExact TOP K\n query. It runs for 30s and looks for the top \n10\n combinations for these events.\n\n\n{\n   \nrecords\n:[\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nmac os x\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:120823,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n56\n,\n         \noversion\n:\n10.12\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nmac os x\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:4539,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n35\n,\n         \noversion\n:\n10.9\n\n      },\n      {\n         \ncountry\n:\nus\n,\n         \nos\n:\nmac os x\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:3827,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n57\n,\n         \noversion\n:\n10.12\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nios\n,\n         \nbrowser\n:\napple safari\n,\n         \nnumEvents\n:3426,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n9.0\n,\n         \noversion\n:\n9.1\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nwindows nt\n,\n         \nbrowser\n:\nmicrosoft internet explorer\n,\n         \nnumEvents\n:2264,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n6.0\n,\n         \noversion\n:\n5.1\n\n      },\n      {\n         \ncountry\n:\nus\n,\n         \nos\n:\nmac os x\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:1995,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n58\n,\n         \noversion\n:\n10.12\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nwindows nt\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:1416,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n57\n,\n         \noversion\n:\n10.0\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nwindows nt\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:1327,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n58\n,\n         \noversion\n:\n10.0\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nmac os x\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:1187,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n57\n,\n         \noversion\n:\n10.12\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nios\n,\n         \nbrowser\n:\napple safari\n,\n         \nnumEvents\n:1119,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n4.0\n,\n         \noversion\n:\n3.0\n\n      }\n   ],\n   \nmeta\n:{\n      \nquery_finish_time\n:1493761419611,\n      \nquery_body\n: \nEDITED OUT\n,\n      \nquery_id\n:-8797534873217479000,\n      \nsketches\n:{\n         \nmaximum_count_error\n:24,\n         \nactive_items\n:746,\n         \nitems_seen\n:187075,\n         \nfamily\n:\nFREQUENCY\n,\n         \nwas_estimated\n:true\n      },\n      \nquery_receive_time\n:1493761386294\n   }\n}\n\n\n\n\nLike \nDISTRIBUTION\n, the distribution of the data matters for \nTOP K\n. Depending on the distribution, your results could produce different counts and errors bounds if approximate.\n\n\nSince we only filtered for nulls in a couple of fields, the top results end up being fields with null values. Note that the \nmaximum_count_error\n is now 24 and the \nwas_estimated\n property is\nset to true. 24 means that the upper bound - the lower bound for the \nCount\n field for each combination could be off by at most 24. Since Bullet gives you the upper bound, this means that if you\nsubtract 24 from it, you get the lower bound of the true count.\n\n\nNote that this also means the order of the items could be off. If two items had \nCount\n within 24 of each other, it is possible that the higher one \nmay\n actually have had a true count \nlower\n than\nthe second one and possibly be ranked higher. There is no such situation in this result set.\n\n\nWindow - Tumbling Group-By\n\n\nSELECT demographics.country AS country, COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nGROUP BY demographics.country\nWINDOWING TUMBLING(5000, TIME)\nLIMIT 50;\n\n\n\n\nThis query specifies a tumbling window that will emit every 5 seconds and contain 5 seconds of data per window. Results will come back to the user every 5 seconds, and since the duration of the query is 20 seconds,\nthe user will receive a total of 4 results. Since the aggregation size is set to 5, each returned window will contain only 5 groups (which will be chosen randomly). The result might look like this:\n\n\nrecords\n:[\n    {\n        \ncountry\n:\nGermany\n,\n        \ncount\n:1,\n        \naverageAge\n:25.0\n    },\n    {\n        \ncountry\n:\nCanada\n,\n        \ncount\n:106,\n        \naverageAge\n:22.58490566037736\n    },\n    {\n        \ncountry\n:\nUSA\n,\n        \ncount\n:1,\n        \naverageAge\n:28.0\n    },\n    {\n        \ncountry\n:\nEngland\n,\n        \ncount\n:8,\n        \naverageAge\n:34.25\n    },\n    {\n        \ncountry\n:\nPeru\n,\n        \ncount\n:9,\n        \naverageAge\n:30.0\n    }\n],\n\nmeta\n:{\n    \nWindow\n:{\n        \nNumber\n:1,\n        \nEmit Time\n:1529458403038,\n        \nExpected Emit Time\n:1529458403023,\n        \nName\n:\nTumbling\n\n        },\n    \nQuery\n:{\n        \nID\n:\n448d228a-1eed-471f-8777-c800cc866535\n,\n        \nReceive Time\n:1529458398023,\n        \nBody\n:\n...(query body)...}\n,\n        \nSketch\n:{\n            \nWas Estimated\n:false,\n            \nUniques Estimate\n:100.0,\n            \nFamily\n:\nTUPLE\n,\n            \nTheta\n:1.0,\n            \nStandard Deviations\n:{\n                \n1\n:{\n                    \nupperBound\n:100.0,\n                    \nlowerBound\n:100.0\n                },\n                \n2\n:{\n                    \nupperBound\n:100.0,\n                    \nlowerBound\n:100.0\n                },\n                \n3\n:{\n                    \nupperBound\n:100.0,\n                    \nlowerBound\n:100.0\n                }\n            }\n        }\n    }\n}\n\n\nrecords\n:[  \n   {  \n      \ncountry\n:\nCanada\n,\n      \ncount\n:101,\n      \naverageAge\n:32.742574257425744\n   },\n   {  \n      \ncountry\n:\nht\n,\n      \ncount\n:2,\n      \naverageAge\n:32.0\n   },\n   {  \n      \ncountry\n:\nEngland\n,\n      \ncount\n:16,\n      \naverageAge\n:27.0625\n   },\n   {  \n      \ncountry\n:\nPeru\n,\n      \ncount\n:8,\n      \naverageAge\n:23.625\n   },\n   {  \n      \ncountry\n:\nBangladesh\n,\n      \ncount\n:3,\n      \naverageAge\n:27.66666666666667\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:2,\n      \nEmit Time\n:1529458408036,\n      \nExpected Emit Time\n:1529458408023,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n448d228a-1eed-471f-8777-c800cc866535\n,\n      \nReceive Time\n:1529458398023,\n      \nBody\n:\n...(query body)...\n\n   },\n   \nSketch\n:{  \n      \nWas Estimated\n:false,\n      \nUniques Estimate\n:98.0,\n      \nFamily\n:\nTUPLE\n,\n      \nTheta\n:1.0,\n      \nStandard Deviations\n:{  \n         \n1\n:{  \n            \nupperBound\n:98.0,\n            \nlowerBound\n:98.0\n         },\n         \n2\n:{  \n            \nupperBound\n:98.0,\n            \nlowerBound\n:98.0\n         },\n         \n3\n:{  \n            \nupperBound\n:98.0,\n            \nlowerBound\n:98.0\n         }\n      }\n   }\n}\n\n\nrecords\n:[  \n   {  \n      \ncountry\n:\nCanada\n,\n      \ncount\n:121,\n      \naverageAge\n:27.97520661157025\n   },\n   {  \n      \ncountry\n:\nHaiti\n,\n      \ncount\n:3,\n      \naverageAge\n:39.0\n   },\n   {  \n      \ncountry\n:\nCabuyao laguna\n,\n      \ncount\n:2,\n      \naverageAge\n:28.0\n   },\n   {  \n      \ncountry\n:\nUSA\n,\n      \ncount\n:1,\n      \naverageAge\n:20.0\n   },\n   {  \n      \ncountry\n:\nEngland\n,\n      \ncount\n:23,\n      \naverageAge\n:40.869565217391305\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:3,\n      \nEmit Time\n:1529458413031,\n      \nExpected Emit Time\n:1529458413023,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n448d228a-1eed-471f-8777-c800cc866535\n,\n      \nReceive Time\n:1529458398023,\n      \nBody\n:\n...(query body)...\n\n   },\n   \nSketch\n:{  \n      \nWas Estimated\n:false,\n      \nUniques Estimate\n:104.0,\n      \nFamily\n:\nTUPLE\n,\n      \nTheta\n:1.0,\n      \nStandard Deviations\n:{  \n         \n1\n:{  \n            \nupperBound\n:104.0,\n            \nlowerBound\n:104.0\n         },\n         \n2\n:{  \n            \nupperBound\n:104.0,\n            \nlowerBound\n:104.0\n         },\n         \n3\n:{  \n            \nupperBound\n:104.0,\n            \nlowerBound\n:104.0\n         }\n      }\n   }\n}\n\n\nrecords\n:[  \n   {  \n      \ncountry\n:\nCanada\n,\n      \ncount\n:117,\n      \naverageAge\n:21.82051282051282\n   },\n   {  \n      \ncountry\n:\nAzerbaijan\n,\n      \ncount\n:1,\n      \naverageAge\n:30.0\n   },\n   {  \n      \ncountry\n:\nEngland\n,\n      \ncount\n:13,\n      \naverageAge\n:30.923076923076923\n   },\n   {  \n      \ncountry\n:\nCongo\n,\n      \ncount\n:1,\n      \naverageAge\n:32.0\n   },\n   {  \n      \ncountry\n:\nBangladesh\n,\n      \ncount\n:3,\n      \naverageAge\n:24.333333333333336\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:4,\n      \nEmit Time\n:1529458418030,\n      \nExpected Emit Time\n:1529458418023,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nFinish Time\n:1529458418030,\n      \nID\n:\n448d228a-1eed-471f-8777-c800cc866535\n,\n      \nReceive Time\n:1529458398023,\n      \nBody\n:\n...(query body)...\n\n   },\n   \nSketch\n:{  \n      \nWas Estimated\n:false,\n      \nUniques Estimate\n:108.0,\n      \nFamily\n:\nTUPLE\n,\n      \nTheta\n:1.0,\n      \nStandard Deviations\n:{  \n         \n1\n:{  \n            \nupperBound\n:108.0,\n            \nlowerBound\n:108.0\n         },\n         \n2\n:{  \n            \nupperBound\n:108.0,\n            \nlowerBound\n:108.0\n         },\n         \n3\n:{  \n            \nupperBound\n:108.0,\n            \nlowerBound\n:108.0\n         }\n      }\n   }\n}\n\n\n\n\nWindow - Additive Tumbling\n\n\nSELECT COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nWINDOWING EVERY(5000, TIME, ALL)\nLIMIT 50;\n\n\n\n\nThe above query will run for 20 seconds and emit a result every 5 seconds. The result will contain the average age and the count of the records seen since the very beginning of the query. Results might look like this:\n\n\nrecords\n:[  \n   {  \n      \ncount\n:8493,\n      \naverageAge\n:28.8828796983622\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:1,\n      \nEmit Time\n:1529522392188,\n      \nExpected Emit Time\n:1529522392089,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n12e48fbd-a20f-4f5e-8135-0f012d9ba3ef\n,\n      \nReceive Time\n:1529522387089,\n      \nBody\n:\n...(query body)...\n\n   }\n}\n\n\nrecords\n:[  \n   {  \n      \ncount\n:17580,\n      \naverageAge\n:29.842629482071715\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:2,\n      \nEmit Time\n:1529522397191,\n      \nExpected Emit Time\n:1529522397089,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n12e48fbd-a20f-4f5e-8135-0f012d9ba3ef\n,\n      \nReceive Time\n:1529522387089,\n      \nBody\n:\n...(query body)...\n\n   }\n}\n\n\nrecords\n:[  \n   {  \n      \ncount\n:26317,\n      \naverageAge\n:29.86675792835957\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:3,\n      \nEmit Time\n:1529522402185,\n      \nExpected Emit Time\n:1529522402089,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n12e48fbd-a20f-4f5e-8135-0f012d9ba3ef\n,\n      \nReceive Time\n:1529522387089,\n      \nBody\n:\n...(query body)...\n\n   }\n}\n\n\nrecords\n:[  \n   {  \n      \ncount\n:35259,\n      \naverageAge\n:29.8303102557552\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:4,\n      \nEmit Time\n:1529522407182,\n      \nExpected Emit Time\n:1529522407089,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nFinish Time\n:1529522407182,\n      \nID\n:\n12e48fbd-a20f-4f5e-8135-0f012d9ba3ef\n,\n      \nReceive Time\n:1529522387089,\n      \nBody\n:\n...(query body)...\n\n   }\n}\n\n\n\n\nSliding Window of Size 1 with Max Duration\n\n\nSELECT *\nFROM STREAM(MAX, TIME)\nWHERE \nbrowser-id\n = '2siknmdd6kaqm'\nWINDOWING EVERY(1, RECORD, FIRST, 1, RECORD)\n\n\n\n\nThis is a query that will capture raw data, and has a sliding window of size 1. This query will return window results immediately whenever a single record that matches the filters flows through the system. The filters in this example will only match records from a particular browser.\n\n\nThis query will run for the maximum amount of time that the backend is configured to allow.\n\n\nResults might look like this:\n\n\nrecords\n:[  \n   {  \n      \ncountry\n:\nUSA\n,\n      \nevent\n:\npage\n,\n      \nbrowser-id\n:\n2siknmdd6kaqm\n\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:1,\n      \nSize\n:1,\n      \nEmit Time\n:1529521479235,\n      \nName\n:\nSliding\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n31d65a12-ed56-4cc8-81ec-6a8bfe9301ba\n,\n      \nReceive Time\n:1529521475015,\n      \nBody\n:\n...(query body)... \n\n   }\n}\n\n\nrecords\n:[  \n   {  \n      \ncountry\n:\nUSA\n,\n      \nevent\n:\nclick\n,\n      \nbrowser-id\n:\n2siknmdd6kaqm\n\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:6,\n      \nSize\n:1,\n      \nEmit Time\n:1529521764875,\n      \nName\n:\nSliding\n\n   },\n   \nQuery\n:{  \n      \nID\n:\ne9595eb4-ea95-418b-8cff-d00736bf216f\n,\n      \nReceive Time\n:1529521757459,\n      \nBody\n:\n...(query body)...\n\n   }\n}\n\n... (one result returned for each record found for as long as the backend is configured to allow) ...", 
            "title": "Examples"
        }, 
        {
            "location": "/ws/examples/#query-examples", 
            "text": "Rather than sourcing the examples from the Quick Start, these examples are real-world ones sourced from Bullet running on raw, user events generated by instrumentation on Yahoo sites by Yahoo employees (not all Yahoo users).   Disclaimer  The actual data shown here has been edited and is not how actual Yahoo user events look.", 
            "title": "Query Examples"
        }, 
        {
            "location": "/ws/examples/#simplest-query", 
            "text": "The simplest query you could write would be:  SELECT * FROM STREAM();  This query would get any records that pass through for a default duration of 20000 ms up to a max default of 500 records.  SELECT * FROM STREAM(10000, TIME) LIMIT 1;  If you wanted to write a smaller or shorter query to, for example, quickly test your connection to Bullet, you could adjust the query duration and size like shown above. This query would only last for a duration of 10000 ms and return at most 1 record.     WINDOW?  There is only one unified data stream in Bullet, so for clarity the  FROM  clause is given a  STREAM  function to denote the look-forward time window for the Bullet query.", 
            "title": "Simplest Query"
        }, 
        {
            "location": "/ws/examples/#simple-filtering", 
            "text": "SELECT *\nFROM STREAM(30000, TIME)\nWHERE id = 'btsg8l9b234ha'\nLIMIT 1;  Because of the default constraints, this query would find at most 1 record with the id matching the value provided. The record would have all its fields.  A sample response could be (it has been edited to remove PII and other Yahoo data). The response contains a single matching record, and the associated meta information.  {\n    records :[\n       {\n            server_name : EDITED ,\n            page_uri : / ,\n            is_page_view :true,\n            device : tablet ,\n            debug_codes :{\n                http_status_code : 200 \n           },\n            referrer_domain : www.yahoo.com ,\n            is_logged_in :true,\n            timestamp :1446842189000,\n            event_family : view ,\n            id : btsg8l9b234ha ,\n            os_name : mac os ,\n            demographics :{\n                age  :  25 ,\n                gender  :  m ,\n            }\n       }\n    ],\n     meta :{\n         query_id :1167304238598842449,\n         query_body : {} ,\n         query_finish_time :1480723799550,\n         query_receive_time :1480723799540\n    }\n}", 
            "title": "Simple Filtering"
        }, 
        {
            "location": "/ws/examples/#relational-filters-and-projections", 
            "text": "SELECT timestamp AS ts, device_timestamp AS device_ts,\n       event AS event, page_domain AS domain, page_id AS id\nFROM STREAM(20000, TIME)\nWHERE id = 'btsg8l9b234ha' AND page_id IS NOT NULL\nLIMIT 10;  The above query finds all events where id is set to 'btsg8l9b234ha' and page_id is not null, projects the fields selected above with their aliases (timestamp as ts, etc.) and limits the results to at most 10 records. The query would wait at most 20 seconds for records to show up.  The resulting response could look like (only 3 events were generated that matched the criteria):  {\n     records : [\n        {\n             domain :  http://some.url.com ,\n             device_ts : 1481152233788,\n             id : 2273844742998,\n             event :  page ,\n             ts : null\n        },\n        {\n             domain :  www.yahoo.com ,\n             device_ts : 1481152233788,\n             id : 227384472956,\n             event :  click ,\n             ts : 1481152233888\n        },\n        {\n             domain :  https://news.yahoo.com ,\n             device_ts : null,\n             id : 2273844742556,\n             event :  page ,\n             ts : null\n        }\n    ],\n     meta : {\n         query_id : -3239746252817510000,\n         query_body :  EDITED OUT ,\n         query_finish_time : 1481152233799,\n         query_receive_time : 1481152233796\n    }\n}", 
            "title": "Relational Filters and Projections"
        }, 
        {
            "location": "/ws/examples/#relational-filters-using-the-extended-value-notation-for-static-values", 
            "text": "For the following examples, we will simply show and explain the queries. They also use the extended syntax for specify values in a filter using the  kind  field.", 
            "title": "Relational Filters using the extended value notation for static values"
        }, 
        {
            "location": "/ws/examples/#sizeof-filter", 
            "text": "This query checks to see if the size of the  data_map  is equal to 4 and returns all records that satisfy this.  SELECT *\nFROM STREAM(30000, TIME)\nWHERE SIZEOF(data_map) = 4\nLIMIT 1;", 
            "title": "SIZEOF Filter"
        }, 
        {
            "location": "/ws/examples/#containskey-filter", 
            "text": "This query checks to see if the  data_map  contains the key  id  and returns all records for which this is true.  SELECT *\nFROM STREAM(30000, TIME)\nWHERE CONTAINSKEY(data_map, 'id')\nLIMIT 1;", 
            "title": "CONTAINSKEY Filter"
        }, 
        {
            "location": "/ws/examples/#containsvalue-filter", 
            "text": "This query checks to see if the  data_map  does not contain the value  btsg8l9b234ha  and returns all records for which this is true. If this was applied on a list field or list of maps field, the inner maps would be checked instead.  SELECT *\nFROM STREAM(30000, TIME)\nWHERE NOT CONTAINSVALUE(data_map, 'btsg8l9b234ha')\nLIMIT 1;", 
            "title": "CONTAINSVALUE Filter"
        }, 
        {
            "location": "/ws/examples/#relational-filter-comparing-to-other-fields", 
            "text": "Instead of comparing to static, constant values, you may use the extended values notation and set  kind  to  FIELD  to  compare to other fields within the same record. The following query returns the first record for which the  id  field is set to the  uid  field.  SELECT *\nFROM STREAM(30000, TIME)\nWHERE id = uid\nLIMIT 1;  A sample result could look like:  {\n     records : [\n        {\n             uid : 0qcgofdbfqs9s ,\n             experience : web ,\n             lid : 978500434 ,\n             id : 0qcgofdbfqs9s ,\n             other fields :  EDITED OUT \n        }\n    ],\n     meta : {\n         query_id :  c4a336e0-3bb5-452a-8503-40d8751b92d9 ,\n         query_body :  EDITED OUT ,\n         query_finish_time : 1536192342505,\n         query_receive_time : 1536192342507\n    }\n}", 
            "title": "Relational Filter comparing to other fields"
        }, 
        {
            "location": "/ws/examples/#logical-filters-and-projections", 
            "text": "SELECT id AS id, experience AS experience, page_id AS pid,\n       link_id AS lid, tags AS tags, demographics.age AS age\nFROM STREAM(60000, TIME)\nWHERE (id = 'c14plm1begla7' AND ((experience = 'web' AND page_id IN ['18025', '47729'])\n                                  OR link_id RLIKE '2.*'))\n      OR (tags.player = 'true' AND demographics.age   '65')\nLIMIT 1;   Typing  If demographics.age was of type Long, then Bullet will convert 85 to be an Long, but in this example, we are pretending that it is String.  So, no conversion is made. Similarly for link_id, id, experience and page_id. tags is a Map of String to Boolean so Bullet converts  \"true\"  to the Boolean  true . See below on how to rewrite it with casting.", 
            "title": "Logical Filters and Projections"
        }, 
        {
            "location": "/ws/examples/#logical-filters-and-projections-with-casting", 
            "text": "```SQL\nSELECT id AS id, experience AS experience, page_id AS pid,\n       link_id AS lid, tags AS tags, CAST(demographics.age AS LONG) AS age\nFROM STREAM(60000, TIME)\nWHERE (id = 'c14plm1begla7' AND ((experience = 'web' AND page_id IN ['18025', '47729'])\n                                  OR link_id RLIKE '2.*'))\n      OR (tags.player = 'true' AND CAST(demographics.age AS LONG)   65L)\nLIMIT 1;\n```  This query is looking for a single event with a specific id and either the page_id is in two specific pages on the \"web\" experience or with a link_id that starts with 2, or a player event where the age is greater than \"65\". In other words, it is looking for senior citizens who generate video player events or the events of a particular person (based on id) events on two specific pages or a group of pages that have link that have ids that start with 2. It then projects out only these fields with different names.  A sample result could look like (it matched because of tags.player was true and demographics.age was   65):  {\n     records : [\n        {\n             pid : 158 ,\n             id : 0qcgofdbfqs9s ,\n             experience : web ,\n             lid : 978500434 ,\n             age :66,\n             tags :{ player :true}\n        }\n    ],\n     meta : {\n         query_id : 3239746252812284004,\n         query_body :  EDITED OUT ,\n         query_finish_time : 1481152233805,\n         query_receive_time : 1481152233881\n    }\n}", 
            "title": "Logical Filters and Projections with Casting"
        }, 
        {
            "location": "/ws/examples/#group-all-count-aggregation", 
            "text": "An example of a query performing a COUNT all records aggregation would look like:  SELECT COUNT(*) AS numSeniors\nFROM STREAM(20000, TIME)\nWHERE demographics.age   65  This query will count the number events for which demographics.age   65. The aggregation type GROUP indicates that it is a group aggregation. To group by a key, the  fields  key needs to be set in the  aggregation  part of the query. If  fields  is empty or is omitted (as it is in the query above) and the  type  is  GROUP , it is as if all the records are collapsed into a single group - a  GROUP ALL . Adding a  COUNT  in the  operations  part of the  attributes  indicates that the number of records in this group will be counted, and the \"newName\" key denotes the name the resulting column \"numSeniors\" in the result. Setting the duration to 20000 counts matching records for\nthis duration.  A sample result would look like:  {\n     records : [\n        {\n             numSeniors : 363201\n        }\n    ],\n     meta : {}\n}  This result indicates that 363,201 records were counted with demographics.age   65 during the 20s the query was running.", 
            "title": "GROUP ALL COUNT Aggregation"
        }, 
        {
            "location": "/ws/examples/#group-all-multiple-aggregations", 
            "text": "COUNT is the only GROUP operation for which you can omit a \"field\".  SELECT COUNT(*) AS numCalifornians, AVG(demographics.age) AS avgAge,\n       MIN(demographics.age) AS minAge, MAX(demographics.age) AS maxAge\nFROM STREAM(20000, TIME)\nWHERE demographics.state = 'california'  Note that the  GROUP BY ()  is optional.  A sample result would look like:  {\n     records : [\n        {\n             maxAge : 94.0,\n             numCalifornians : 188451,\n             minAge : 6.0,\n             avgAge : 33.71828\n        }\n    ],\n     meta : {\n         query_id : 8051040987827161000,\n         query_body :  EDITED OUT ,\n         query_finish_time : 1482371927435,\n         query_receive_time : 1482371916625\n    }\n}  This result indicates that, among the records observed during the 20s this query ran, there were 188,451 users with demographics.state equal to \"california\". Among these users the average age was 33.71828, the max age observed was 94, and the minimum age observed was 6.", 
            "title": "GROUP ALL Multiple Aggregations"
        }, 
        {
            "location": "/ws/examples/#exact-count-distinct-aggregation", 
            "text": "SELECT COUNT(DISTINCT browser_name, browser_version) AS  COUNT DISTINCT \nFROM STREAM(10000, TIME);  This gets the count of the unique browser names and versions in the next 30s (default duration). Note that we do not specify values for the keys in fields. This is because they are not relevant  {\n     records : [\n        {\n             COUNT DISTINCT : 158.0\n        }\n    ],\n     meta : {\n         query_id : 4451146261377394443,\n         sketches : {\n             standard_deviations : {\n                 1 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                },\n                 2 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                },\n                 3 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                }\n            },\n             was_estimated : false,\n             family :  THETA ,\n             theta : 1.0,\n             size : 1280\n        },\n         query_body :  EDITED OUT ,\n         query_finish_time : 1484084869073,\n         query_receive_time : 1484084832684\n    }\n}  There were 158 unique combinations on browser names and versions in our dataset for those 30 s. Note the new  sketches  object in the meta. It has various metadata about the result. In particular, the  was_estimated  key denotes where the result\nwas estimated or not. The  standard_deviations  key denotes the confidence at various sigmas: 1 (1 sigma = ~68% confidence, 2 sigma = ~95% confidence, 3 sigma = ~99% confidence). Since this result was not estimated, the result is the same as the upper and lower bounds for the result.", 
            "title": "Exact COUNT DISTINCT Aggregation"
        }, 
        {
            "location": "/ws/examples/#approximate-count-distinct", 
            "text": "SELECT COUNT(DISTINCT ip_address) AS uniqueIPs\nFROM STREAM(10000, TIME);  This query gets us the unique IP addresses in the next 10 s. It renames the result column from \"COUNT DISTINCT\" to \"uniqueIPs\".  {\n    records :[\n      {\n          uniqueIPs :130551.07952805843\n      }\n   ],\n    meta :{\n       query_id :5377782455857451480,\n       sketches :{\n          standard_deviations :{\n             1 :{\n                upperBound :131512.85413760383,\n                lowerBound :129596.30223107953\n            },\n             2 :{\n                upperBound :132477.15103015225,\n                lowerBound :128652.93906100772\n            },\n             3 :{\n                upperBound :133448.49248615955,\n                lowerBound :127716.46773622213\n            }\n         },\n          was_estimated :true,\n          family : THETA ,\n          theta :0.12549877074343688,\n          size :131096\n      },\n       query_body : EDITED OUT ,\n       query_finish_time :1484090240812,\n       query_receive_time :1484090223351\n   }\n}  The number of unique IPs in our dataset was 130551 in those 10s (approximately) with the true value between (129596, 131512) at 68% confidence, (128652, 132477) at 95% confidence and (127716, 133448) at 99% confidence. In the  worst  case at 3 sigma (99% confidence),\nour error is 2.17%. The final result was computed with 131096 bytes or ~128 KiB as denoted by  size . This happens to be maximum size the the COUNT DISTINCT sketch will take up at the default nominal entries, so even if we had billions of unique IPs, the size will be the same and the error may be higher (depends on the distribution). For example, the error when the same query was run for 30s was 2.28% at 99% confidence (actual unique IPs: 559428, upper bound: 572514). In fact, the worst the error can get at this\nSketch size is 2.34% as defined  here ,  regardless of the number of unique entries added to the Sketch! .", 
            "title": "Approximate COUNT DISTINCT"
        }, 
        {
            "location": "/ws/examples/#distinct-aggregation", 
            "text": "SELECT browser_name AS browser\nFROM STREAM(30000, TIME)\nGROUP BY browser_name\nLIMIT 10;  SELECT DISTINCT browser_name AS browser\nFROM STREAM(30000, TIME)\nLIMIT 10;  This query gets the distinct values for the browser_name field and limit the results to 10. It runs for 30 s.  {\n    records :[\n      {\n          browser : opera \n      },\n      {\n          browser : flock \n      },\n      {\n          browser : links \n      },\n      {\n          browser : mozilla firefox \n      },\n      {\n          browser : dolfin \n      },\n      {\n          browser : lynx \n      },\n      {\n          browser : chrome \n      },\n      {\n          browser : microsoft internet explorer \n      },\n      {\n          browser : aol browser \n      },\n      {\n          browser : edge \n      }\n   ],\n    meta :{\n       query_id :-4872093887360741287,\n       sketches :{\n          standard_deviations :{\n             1 :{\n                upperBound :28.0,\n                lowerBound :28.0\n            },\n             2 :{\n                upperBound :28.0,\n                lowerBound :28.0\n            },\n             3 :{\n                upperBound :28.0,\n                lowerBound :28.0\n            }\n         },\n          was_estimated :false,\n          family : TUPLE ,\n          uniques_estimate :28.0,\n          theta :1.0\n      },\n       query_body : EDITED OUT ,\n       query_finish_time :1485469087971,\n       query_receive_time :1485469054070\n   }\n}  There were 28 unique results but we asked for 10, so the query returned a uniform sample across the 28 distinct values.  DISTINCT is just an alias for GROUP. A GROUP by with no operations is exactly a DISTINCT.", 
            "title": "DISTINCT Aggregation"
        }, 
        {
            "location": "/ws/examples/#group-by-aggregation", 
            "text": "SELECT demographics.country AS country, device AS device,\n       COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nGROUP BY demographics.country, device\nLIMIT 50;  This query groups by the country and the device and for each unique group gets the count, average age and time spent by the users for the next 20 seconds. It renames demographics.country to country and does not rename device. It limits the groups to 50. If there were more than\n50 groups, the results would be a uniform sampling of the groups (but each group in the result would have the correct result). These parameters can all be tweaked  in the configuration .  {\n    records :[\n      {\n          country : uk ,\n          device : desktop ,\n          count :203034,\n          averageAge :32.42523,\n          averageTimespent :1.342\n      },\n      {\n          country : us ,\n          device : desktop ,\n          count :1934030,\n          averageAge :29.42523,\n          averageTimespent :3.234520\n      },\n       ...EDITED 41 other such records out for readability... ,\n   ],\n    meta :{\n       query_id :1705911449584057747,\n       sketches :{\n          standard_deviations :{\n             1 :{\n                upperBound :43.0,\n                lowerBound :43.0\n            },\n             2 :{\n                upperBound :43.0,\n                lowerBound :43.0\n            },\n             3 :{\n                upperBound :43.0,\n                lowerBound :43.0\n            }\n         },\n          was_estimated :false,\n          family : TUPLE ,\n          uniques_estimate :43.0,\n          theta :1.0\n      },\n       query_body : EDITED OUT ,\n       query_finish_time :1485217172780,\n       query_receive_time :1485217148840\n   }\n}  We received 43 rows for this result. The maximum groups that was allowed for the instance of Bullet was 512. If there were more groups than the maximum specified by your configuration,  a uniform sample  across them would be chosen\nfor the result. However, for each group, the values computed (average, count) would be exact. The standard deviations, whether the result was estimated and the number of approximate uniques in the metadata would reflect the change.  If you asked for 50 rows in the aggregation (as the query did above) but there were more than 50 in the result (but   512), the metadata would reflect the fact that the result was not estimated. You would still get a uniform sample\nbut by increasing your aggregation size higher, you could get the rest.  For readability, if you were just trying to get the unique values for a field or a set of fields, you could leave out the attributes section and specify your fields section. You could also call the type  DISTINCT  instead of GROUP  to make that explicit.  DISTINCT  is just an alias for  GROUP . See  the DISTINCT example .", 
            "title": "GROUP by Aggregation"
        }, 
        {
            "location": "/ws/examples/#quantile-distribution", 
            "text": "SELECT QUANTILE(duration, LINEAR, 11)\nFROM STREAM(5000, TIME);  This query creates 11 points from 0 to 1 (both inclusive) and finds the percentile values of the  duration  field (which contains an amount of time in ms) at  0, 0.1, 0.2 ... 1.0  or the 0th, 10th, 20th and 100th percentiles. It runs for 5 seconds and returns at most 11 points. As long as the  size  is set to higher than the number of points you generate,  DISTRIBUTION  queries will return all your values.  The SQL is not really the same since it will produce one row instead of 11.  {\n    records :[\n      {\n          Value :1,\n          Quantile :0\n      },\n      {\n          Value :1352,\n          Quantile :0.1\n      },\n      {\n          Value :3045,\n          Quantile :0.2\n      },\n      {\n          Value :6501,\n          Quantile :0.30000000000000004\n      },\n      {\n          Value :10700,\n          Quantile :0.4\n      },\n      {\n          Value :17488,\n          Quantile :0.5\n      },\n      {\n          Value :28659,\n          Quantile :0.6\n      },\n      {\n          Value :47929,\n          Quantile :0.7\n      },\n      {\n          Value :83447,\n          Quantile :0.7999999999999999\n      },\n      {\n          Value :177548,\n          Quantile :0.8999999999999999\n      },\n      {\n          Value :83525609,\n          Quantile :1\n      }\n   ],\n    meta :{\n       query_finish_time :1493748546533,\n       query_body :  EDITED OUT ,\n       query_id :2981902209347343400,\n       sketches :{\n          normalized_rank_error :0.002389303789572841,\n          size :16416,\n          minimum_value :1,\n          items_seen :1414,\n          maximum_value :83525609,\n          family : QUANTILES ,\n          was_estimated :false\n      },\n       query_receive_time :1493748538259\n   }\n}  The result shows the values at the 0th, 10th percentiles etc. The  was_estimated  key indicates that the result was not approximated. Note the the  minimum_value  and the  maximum_value  correspond to the 0th and 100th percentiles. There is also a  normalized_rank_error  that describes the error (see  below  for a detailed explanation) This is constant for all  DISTRIBUTION  queries and does not depend on the data or the query.", 
            "title": "QUANTILE DISTRIBUTION"
        }, 
        {
            "location": "/ws/examples/#normalized-rank-error", 
            "text": "Unlike  GROUP  and  COUNT DISTINCT , the order in which the data arrives to Bullet can affect the results of a  DISTRIBUTION  query. The error when the result is estimated is not a Gaussian error function\nand is not described in terms of the values of your field. In other words, if the 50th percentile was estimated to some value, you could not bound the true median by using the the estimated\nvalue +/- constant (see below for a good approximation). The error is expressed in terms of the  normalized rank . If one were to sort the true data stream, you would obtain a rank for each item from\n0 to the stream length ( items_seen  in the metadata above). If you then divided each rank by length, you would get ranks from 0 to 1. In this domain, a  normalized rank error  of 0.002, for example, means that a value\nreturned for the 0.50 or 50th percentile could actually lie between 0.498 and 0.502 in the normalized ranks with 99% confidence.  Distribution Accuracy  lists the normalized rank error as a percentage for the maximum size of the Sketch used. If you obtain successive quantile values at\ngranularities lower than this rank error, the results may not be accurate. While the sketch speaks of the normalized rank error, you can still obtain reasonable bounds for values. For example, if the normalized rank\nerror was 1% and you obtained quantile values at 0.48, 0.50 and 0.52, you could use the values at 0.52 and 0.48 as very reasonable upper and lower bounds on your true median (you might even be able to use 0.49 and 0.51\nif the error was 1%).", 
            "title": "Normalized Rank Error"
        }, 
        {
            "location": "/ws/examples/#pmf-distribution-aggregation", 
            "text": "SELECT FREQ(duration, REGION, 2000, 20000, 500)\nFROM STREAM(5000, TIME);  This query creates 37 points from 2000 to 20000 in 500 increments to bucketize the duration field using these points as split locations and finds the count of duration values that fall into these intervals. It runs for 5s and returns at most 100 records (this means it will return the 38 records).  The SQL does not include the  (-  to 2000)  and the  [20000 to + )  intervals and does not produce a probability.  {\n    records :[\n      {\n          Probability :0.1518054532056006,\n          Count :206,\n          Range : (-\u221e to 2000.0) \n      },\n      {\n          Probability :0.0397936624907885,\n          Count :53.99999999999999,\n          Range : [2000.0 to 2500.0) \n      },\n       ...EDITED 34 other such records out for readability... ,\n      {\n          Probability :0.0058953574060427415,\n          Count :8,\n          Range : [19500.0 to 20000.0) \n      },\n      {\n          Probability :0.45689019896831246,\n          Count :620,\n          Range : [20000.0 to +\u221e) \n      }\n   ],\n    meta :{\n       query_finish_time :1493750074795,\n       query_body :  EDITED OUT ,\n       query_id :-2590566941995678000,\n       sketches :{\n          normalized_rank_error :0.002389303789572841,\n          size :16416,\n          minimum_value :1,\n          items_seen :1357,\n          maximum_value :78240570,\n          family : QUANTILES ,\n          was_estimated :false\n      },\n       query_receive_time :1493750066022\n   }\n}  The result consists of 38 records, each denoting an interval in the domain we asked for. The result was not estimated. Note that the interval is denoted by the  Range  key and the count by the  Count  key. There is also a probability that estimates how likely a value for duration is likely to fall into that range.", 
            "title": "PMF DISTRIBUTION Aggregation"
        }, 
        {
            "location": "/ws/examples/#cdf-distribution-aggregation", 
            "text": "SELECT CUMFREQ(duration, MANUAL, 20000, 2000, 15000, 45000)\nFROM STREAM(5000, TIME);  This query specifies a list of points manually using  points  property in  attributes . It runs for 5s and finds the\ncumulative frequency distribution using the specified points as break points. It returns at most 100 records (which means we will\nget all of the intervals).  There is no easy SQL equivalent because the points are free-form. It does not produce a probability field like Bullet does.  {\n    records :[\n      {\n          Probability :0.14382632293080055,\n          Count :212.00000000000003,\n          Range : (-\u221e to 2000.0) \n      },\n      {\n          Probability :0.5210312075983717,\n          Count :767.9999999999999,\n          Range : (-\u221e to 15000.0) \n      },\n      {\n          Probability :0.5603799185888738,\n          Count :826,\n          Range : (-\u221e to 20000.0) \n      },\n      {\n          Probability :0.6994572591587517,\n          Count :1031,\n          Range : (-\u221e to 45000.0) \n      },\n      {\n          Probability :1,\n          Count :1474,\n          Range : (-\u221e to +\u221e) \n      }\n   ],\n    meta :{\n       query_finish_time :1493755151660,\n       query_body :  EDITED OUT ,\n       query_id :-8460702488693518000,\n       sketches :{\n          normalized_rank_error :0.002389303789572841,\n          size :16416,\n          minimum_value :2,\n          items_seen :1474,\n          maximum_value :10851113,\n          family : QUANTILES ,\n          was_estimated :false\n      },\n       query_receive_time :1493755143626\n   }\n}  The result contains the 5 intervals produced by the split points. It was not estimated so these counts are exact. Note that the start of each interval is  -  because\nit is the cumulative frequency distribution.", 
            "title": "CDF DISTRIBUTION Aggregation"
        }, 
        {
            "location": "/ws/examples/#exact-top-k-aggregation", 
            "text": "There are two methods for executing a TOP K aggregation in BQL:  SELECT TOP(500, 100, demographics.country, browser_name) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE demographics.country IS NOT NULL AND browser_name IS NOT NULL;  OR:  SELECT demographics.country, browser_name, COUNT(*) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE demographics.country IS NOT NULL AND browser_name IS NOT NULL\nGROUP BY demographics.country, browser_name\nHAVING COUNT(*)  = 100\nORDER BY COUNT(*) DESC\nLIMIT 500;  This query gets the top 500 country, browser combinations where the count of records for each combination is at least 100. It runs for 10s.  {\n    records :[\n      {\n          country : us ,\n          browser : google chrome ,\n          numEvents :2729\n      },\n      {\n          country : us ,\n          browser : mozilla firefox ,\n          numEvents :1072\n      },\n      {\n          country : uk ,\n          browser : google chrome ,\n          numEvents :703\n      },\n      {\n          country : fr ,\n          browser : google chrome ,\n          numEvents :383\n      },\n      {\n          country : fr ,\n          browser : mozilla firefox ,\n          numEvents :278\n      },\n      {\n          country : es ,\n          browser : google chrome ,\n          numEvents :234\n      },\n       ...EDITED 10 other such records here for readability ,\n      {\n          country : es ,\n          browser : mozilla firefox ,\n          numEvents :102\n      },\n      {\n          country : fr ,\n          browser : apple safari ,\n          numEvents :101\n      }\n   ],\n    meta :{\n       query_finish_time :1493760034414,\n       query_body :  EDITED OUT ,\n       query_id :7515243052399540000,\n       sketches :{\n          maximum_count_error :0,\n          active_items :431,\n          items_seen :10784,\n          family : FREQUENCY ,\n          was_estimated :false\n      },\n       query_receive_time :1493760020807\n   }\n}  The results gave us the top 18 country, browser combinations that had counts over a 100. Note the  maximum_count_error  key in the metadata. This represents how off the count is. It is 0 because these counts are exact.\nIn our data stream, we only had 18 unique combinations of countries and browser names at the time the query was run.", 
            "title": "Exact TOP K Aggregation"
        }, 
        {
            "location": "/ws/examples/#approximate-top-k-aggregation", 
            "text": "There are two methods for executing a TOP K aggregation in BQL:  SELECT TOP(10, 100, browser_name, browser_version, os_name, os_version, demographics.country, demographics.state) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE os_name IS NOT NULL AND browser_name IS NOT NULL;  OR:  SELECT browser_name, browser_version, os_name, os_version, demographics.country, demographics.state, COUNT(*) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE os_name IS NOT NULL AND browser_name IS NOT NULL\nGROUP BY browser_name, browser_version, os_name, os_version, demographics.country, demographics.state\nHAVING COUNT(*)  = 100\nORDER BY COUNT(*) DESC\nLIMIT 10;  In order to make the result approximate, this query adds more dimensions to the  Exact TOP K  query. It runs for 30s and looks for the top  10  combinations for these events.  {\n    records :[\n      {\n          country : null ,\n          os : mac os x ,\n          browser : google chrome ,\n          numEvents :120823,\n          state : null ,\n          bversion : 56 ,\n          oversion : 10.12 \n      },\n      {\n          country : null ,\n          os : mac os x ,\n          browser : google chrome ,\n          numEvents :4539,\n          state : null ,\n          bversion : 35 ,\n          oversion : 10.9 \n      },\n      {\n          country : us ,\n          os : mac os x ,\n          browser : google chrome ,\n          numEvents :3827,\n          state : null ,\n          bversion : 57 ,\n          oversion : 10.12 \n      },\n      {\n          country : null ,\n          os : ios ,\n          browser : apple safari ,\n          numEvents :3426,\n          state : null ,\n          bversion : 9.0 ,\n          oversion : 9.1 \n      },\n      {\n          country : null ,\n          os : windows nt ,\n          browser : microsoft internet explorer ,\n          numEvents :2264,\n          state : null ,\n          bversion : 6.0 ,\n          oversion : 5.1 \n      },\n      {\n          country : us ,\n          os : mac os x ,\n          browser : google chrome ,\n          numEvents :1995,\n          state : null ,\n          bversion : 58 ,\n          oversion : 10.12 \n      },\n      {\n          country : null ,\n          os : windows nt ,\n          browser : google chrome ,\n          numEvents :1416,\n          state : null ,\n          bversion : 57 ,\n          oversion : 10.0 \n      },\n      {\n          country : null ,\n          os : windows nt ,\n          browser : google chrome ,\n          numEvents :1327,\n          state : null ,\n          bversion : 58 ,\n          oversion : 10.0 \n      },\n      {\n          country : null ,\n          os : mac os x ,\n          browser : google chrome ,\n          numEvents :1187,\n          state : null ,\n          bversion : 57 ,\n          oversion : 10.12 \n      },\n      {\n          country : null ,\n          os : ios ,\n          browser : apple safari ,\n          numEvents :1119,\n          state : null ,\n          bversion : 4.0 ,\n          oversion : 3.0 \n      }\n   ],\n    meta :{\n       query_finish_time :1493761419611,\n       query_body :  EDITED OUT ,\n       query_id :-8797534873217479000,\n       sketches :{\n          maximum_count_error :24,\n          active_items :746,\n          items_seen :187075,\n          family : FREQUENCY ,\n          was_estimated :true\n      },\n       query_receive_time :1493761386294\n   }\n}  Like  DISTRIBUTION , the distribution of the data matters for  TOP K . Depending on the distribution, your results could produce different counts and errors bounds if approximate.  Since we only filtered for nulls in a couple of fields, the top results end up being fields with null values. Note that the  maximum_count_error  is now 24 and the  was_estimated  property is\nset to true. 24 means that the upper bound - the lower bound for the  Count  field for each combination could be off by at most 24. Since Bullet gives you the upper bound, this means that if you\nsubtract 24 from it, you get the lower bound of the true count.  Note that this also means the order of the items could be off. If two items had  Count  within 24 of each other, it is possible that the higher one  may  actually have had a true count  lower  than\nthe second one and possibly be ranked higher. There is no such situation in this result set.", 
            "title": "Approximate TOP K Aggregation"
        }, 
        {
            "location": "/ws/examples/#window-tumbling-group-by", 
            "text": "SELECT demographics.country AS country, COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nGROUP BY demographics.country\nWINDOWING TUMBLING(5000, TIME)\nLIMIT 50;  This query specifies a tumbling window that will emit every 5 seconds and contain 5 seconds of data per window. Results will come back to the user every 5 seconds, and since the duration of the query is 20 seconds,\nthe user will receive a total of 4 results. Since the aggregation size is set to 5, each returned window will contain only 5 groups (which will be chosen randomly). The result might look like this:  records :[\n    {\n         country : Germany ,\n         count :1,\n         averageAge :25.0\n    },\n    {\n         country : Canada ,\n         count :106,\n         averageAge :22.58490566037736\n    },\n    {\n         country : USA ,\n         count :1,\n         averageAge :28.0\n    },\n    {\n         country : England ,\n         count :8,\n         averageAge :34.25\n    },\n    {\n         country : Peru ,\n         count :9,\n         averageAge :30.0\n    }\n], meta :{\n     Window :{\n         Number :1,\n         Emit Time :1529458403038,\n         Expected Emit Time :1529458403023,\n         Name : Tumbling \n        },\n     Query :{\n         ID : 448d228a-1eed-471f-8777-c800cc866535 ,\n         Receive Time :1529458398023,\n         Body : ...(query body)...} ,\n         Sketch :{\n             Was Estimated :false,\n             Uniques Estimate :100.0,\n             Family : TUPLE ,\n             Theta :1.0,\n             Standard Deviations :{\n                 1 :{\n                     upperBound :100.0,\n                     lowerBound :100.0\n                },\n                 2 :{\n                     upperBound :100.0,\n                     lowerBound :100.0\n                },\n                 3 :{\n                     upperBound :100.0,\n                     lowerBound :100.0\n                }\n            }\n        }\n    }\n} records :[  \n   {  \n       country : Canada ,\n       count :101,\n       averageAge :32.742574257425744\n   },\n   {  \n       country : ht ,\n       count :2,\n       averageAge :32.0\n   },\n   {  \n       country : England ,\n       count :16,\n       averageAge :27.0625\n   },\n   {  \n       country : Peru ,\n       count :8,\n       averageAge :23.625\n   },\n   {  \n       country : Bangladesh ,\n       count :3,\n       averageAge :27.66666666666667\n   }\n], meta :{  \n    Window :{  \n       Number :2,\n       Emit Time :1529458408036,\n       Expected Emit Time :1529458408023,\n       Name : Tumbling \n   },\n    Query :{  \n       ID : 448d228a-1eed-471f-8777-c800cc866535 ,\n       Receive Time :1529458398023,\n       Body : ...(query body)... \n   },\n    Sketch :{  \n       Was Estimated :false,\n       Uniques Estimate :98.0,\n       Family : TUPLE ,\n       Theta :1.0,\n       Standard Deviations :{  \n          1 :{  \n             upperBound :98.0,\n             lowerBound :98.0\n         },\n          2 :{  \n             upperBound :98.0,\n             lowerBound :98.0\n         },\n          3 :{  \n             upperBound :98.0,\n             lowerBound :98.0\n         }\n      }\n   }\n} records :[  \n   {  \n       country : Canada ,\n       count :121,\n       averageAge :27.97520661157025\n   },\n   {  \n       country : Haiti ,\n       count :3,\n       averageAge :39.0\n   },\n   {  \n       country : Cabuyao laguna ,\n       count :2,\n       averageAge :28.0\n   },\n   {  \n       country : USA ,\n       count :1,\n       averageAge :20.0\n   },\n   {  \n       country : England ,\n       count :23,\n       averageAge :40.869565217391305\n   }\n], meta :{  \n    Window :{  \n       Number :3,\n       Emit Time :1529458413031,\n       Expected Emit Time :1529458413023,\n       Name : Tumbling \n   },\n    Query :{  \n       ID : 448d228a-1eed-471f-8777-c800cc866535 ,\n       Receive Time :1529458398023,\n       Body : ...(query body)... \n   },\n    Sketch :{  \n       Was Estimated :false,\n       Uniques Estimate :104.0,\n       Family : TUPLE ,\n       Theta :1.0,\n       Standard Deviations :{  \n          1 :{  \n             upperBound :104.0,\n             lowerBound :104.0\n         },\n          2 :{  \n             upperBound :104.0,\n             lowerBound :104.0\n         },\n          3 :{  \n             upperBound :104.0,\n             lowerBound :104.0\n         }\n      }\n   }\n} records :[  \n   {  \n       country : Canada ,\n       count :117,\n       averageAge :21.82051282051282\n   },\n   {  \n       country : Azerbaijan ,\n       count :1,\n       averageAge :30.0\n   },\n   {  \n       country : England ,\n       count :13,\n       averageAge :30.923076923076923\n   },\n   {  \n       country : Congo ,\n       count :1,\n       averageAge :32.0\n   },\n   {  \n       country : Bangladesh ,\n       count :3,\n       averageAge :24.333333333333336\n   }\n], meta :{  \n    Window :{  \n       Number :4,\n       Emit Time :1529458418030,\n       Expected Emit Time :1529458418023,\n       Name : Tumbling \n   },\n    Query :{  \n       Finish Time :1529458418030,\n       ID : 448d228a-1eed-471f-8777-c800cc866535 ,\n       Receive Time :1529458398023,\n       Body : ...(query body)... \n   },\n    Sketch :{  \n       Was Estimated :false,\n       Uniques Estimate :108.0,\n       Family : TUPLE ,\n       Theta :1.0,\n       Standard Deviations :{  \n          1 :{  \n             upperBound :108.0,\n             lowerBound :108.0\n         },\n          2 :{  \n             upperBound :108.0,\n             lowerBound :108.0\n         },\n          3 :{  \n             upperBound :108.0,\n             lowerBound :108.0\n         }\n      }\n   }\n}", 
            "title": "Window - Tumbling Group-By"
        }, 
        {
            "location": "/ws/examples/#window-additive-tumbling", 
            "text": "SELECT COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nWINDOWING EVERY(5000, TIME, ALL)\nLIMIT 50;  The above query will run for 20 seconds and emit a result every 5 seconds. The result will contain the average age and the count of the records seen since the very beginning of the query. Results might look like this:  records :[  \n   {  \n       count :8493,\n       averageAge :28.8828796983622\n   }\n], meta :{  \n    Window :{  \n       Number :1,\n       Emit Time :1529522392188,\n       Expected Emit Time :1529522392089,\n       Name : Tumbling \n   },\n    Query :{  \n       ID : 12e48fbd-a20f-4f5e-8135-0f012d9ba3ef ,\n       Receive Time :1529522387089,\n       Body : ...(query body)... \n   }\n} records :[  \n   {  \n       count :17580,\n       averageAge :29.842629482071715\n   }\n], meta :{  \n    Window :{  \n       Number :2,\n       Emit Time :1529522397191,\n       Expected Emit Time :1529522397089,\n       Name : Tumbling \n   },\n    Query :{  \n       ID : 12e48fbd-a20f-4f5e-8135-0f012d9ba3ef ,\n       Receive Time :1529522387089,\n       Body : ...(query body)... \n   }\n} records :[  \n   {  \n       count :26317,\n       averageAge :29.86675792835957\n   }\n], meta :{  \n    Window :{  \n       Number :3,\n       Emit Time :1529522402185,\n       Expected Emit Time :1529522402089,\n       Name : Tumbling \n   },\n    Query :{  \n       ID : 12e48fbd-a20f-4f5e-8135-0f012d9ba3ef ,\n       Receive Time :1529522387089,\n       Body : ...(query body)... \n   }\n} records :[  \n   {  \n       count :35259,\n       averageAge :29.8303102557552\n   }\n], meta :{  \n    Window :{  \n       Number :4,\n       Emit Time :1529522407182,\n       Expected Emit Time :1529522407089,\n       Name : Tumbling \n   },\n    Query :{  \n       Finish Time :1529522407182,\n       ID : 12e48fbd-a20f-4f5e-8135-0f012d9ba3ef ,\n       Receive Time :1529522387089,\n       Body : ...(query body)... \n   }\n}", 
            "title": "Window - Additive Tumbling"
        }, 
        {
            "location": "/ws/examples/#sliding-window-of-size-1-with-max-duration", 
            "text": "SELECT *\nFROM STREAM(MAX, TIME)\nWHERE  browser-id  = '2siknmdd6kaqm'\nWINDOWING EVERY(1, RECORD, FIRST, 1, RECORD)  This is a query that will capture raw data, and has a sliding window of size 1. This query will return window results immediately whenever a single record that matches the filters flows through the system. The filters in this example will only match records from a particular browser.  This query will run for the maximum amount of time that the backend is configured to allow.  Results might look like this:  records :[  \n   {  \n       country : USA ,\n       event : page ,\n       browser-id : 2siknmdd6kaqm \n   }\n], meta :{  \n    Window :{  \n       Number :1,\n       Size :1,\n       Emit Time :1529521479235,\n       Name : Sliding \n   },\n    Query :{  \n       ID : 31d65a12-ed56-4cc8-81ec-6a8bfe9301ba ,\n       Receive Time :1529521475015,\n       Body : ...(query body)...  \n   }\n} records :[  \n   {  \n       country : USA ,\n       event : click ,\n       browser-id : 2siknmdd6kaqm \n   }\n], meta :{  \n    Window :{  \n       Number :6,\n       Size :1,\n       Emit Time :1529521764875,\n       Name : Sliding \n   },\n    Query :{  \n       ID : e9595eb4-ea95-418b-8cff-d00736bf216f ,\n       Receive Time :1529521757459,\n       Body : ...(query body)... \n   }\n}\n\n... (one result returned for each record found for as long as the backend is configured to allow) ...", 
            "title": "Sliding Window of Size 1 with Max Duration"
        }, 
        {
            "location": "/ws/api-json/", 
            "text": "Bullet JSON API\n\n\nThis section gives a comprehensive overview of the old Web Service API for launching Bullet JSON queries. This was \ndeprecated\n in favor of \nBQL\n in Bullet 1.0+.\n\n\nThe JSON API is the old Query format that was expected by the API and the Backend prior to Bullet 1.0. \nThe API\n is a more user-friendly API which can also be used - the Web Service prior to Bullet 1.0 will automatically detect the BQL query and convert the query to this JSON format before submitting it to the backend. With the addition of Post Aggregations and Expressions,\nit is a lot easier to use BQL rather than construct the JSON. The Bullet Web Service also provides \nan API\n to convert BQL to JSON if you so desire.\n\n\n\n\nFor info on how to use the UI, see the \nUI Usage section\n\n\nFor examples of specific queries see the \nExamples\n section\n\n\n\n\nConstituents of a Bullet Query\n\n\nThe main constituents of a Bullet JSON query are:\n\n\n\n\nfilters\n, which determine which records will be consumed by your query\n\n\nprojection\n, which determines which fields will be projected in the resulting output from Bullet\n\n\naggregation\n, which allows you to aggregate data and perform aggregation operations\n\n\npostAggregations\n, which allows you to perform post aggregations before the result is returned\n\n\nwindow\n, which can be used to return incremental results on \"windowed\" data\n\n\nduration\n, which determines the maximum duration of the query in milliseconds\n\n\n\n\nThe main constituents of a Bullet query listed above create the top level fields of the Bullet query:\n\n\n{\n    \nfilters\n: [{}, {}, ...],\n    \nprojection\n: {},\n    \naggregation\n: {},\n    \npostAggregations\n: [{}, {}, ...],\n    \nwindow\n: {},\n    \nduration\n: 20000\n}\n\n\n\n\nAccessing Complex Fields\n\n\nFields inside maps and lists can be accessed using the '.' notation in queries.\n\n\n\n\n\n\n\n\nComplex Field Type\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nMap of Primitive\n\n\nmyMap.key\n\n\n\n\n\n\nMap of Map of Primitive\n\n\nmyMap.myInnerMap.key\n\n\n\n\n\n\nList of Map/Primitive\n\n\nmyList.0\n\n\n\n\n\n\nList of Map of Primitive\n\n\nmyListOfMaps.4.key\n\n\n\n\n\n\n\n\nWe will now describe how to specify each of these top-level fields below:\n\n\nFilters\n\n\nBullet supports two kinds of filters:\n\n\n\n\nLogical filters\n\n\nRelational filters\n\n\n\n\nLogical Filters\n\n\nLogical filters allow you to combine other filter clauses with logical operations like \nAND\n, \nOR\n and \nNOT\n.\n\n\nThe current logical operators allowed in filters are:\n\n\n\n\n\n\n\n\nLogical Operator\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nAND\n\n\nAll filters must be true. The first false filter evaluated left to right will short-circuit the computation.\n\n\n\n\n\n\nOR\n\n\nAny filter must be true. The first true filter evaluated left to right will short-circuit the computation.\n\n\n\n\n\n\nNOT\n\n\nNegates the value of the first filter clause. The filter is satisfied iff the value is true.\n\n\n\n\n\n\n\n\nThe format for a \nsingle\n Logical filter is:\n\n\n{\n   \noperation\n: \nAND | OR | NOT\n\n   \nclauses\n: [\n      {\noperation\n: \n...\n, clauses: [{}, ...]},\n      {\nfield\n: \n...\n, \noperation\n: \n, values: [\n...\n]},\n      {\noperation\n: \n...\n, clauses: [{}, ...]}\n      ...\n   ]\n}\n\n\n\n\nAny other type of filter may be provided as a clause in clauses.\n\n\nNote that the \"filter\" field in the query is a \nlist\n of as many filters as you'd like.\n\n\nRelational Filters\n\n\nRelational filters allow you to specify conditions on a field, using a comparison operator and a list of constant values or other fields.\n\n\nThe current comparisons allowed in filters are:\n\n\n\n\n\n\n\n\nComparison\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\n==\n\n\nEqual to any value in values\n\n\n\n\n\n\n!=\n\n\nNot equal to any value in values\n\n\n\n\n\n\n=\n\n\nLess than or equal to any value in values\n\n\n\n\n\n\n=\n\n\nGreater than or equal to any value in values\n\n\n\n\n\n\n\n\nLess than any value in values\n\n\n\n\n\n\n\n\nGreater than any value in values\n\n\n\n\n\n\nRLIKE\n\n\nMatches using \nJava Regex notation\n, any Regex value in values\n\n\n\n\n\n\nSIZEIS\n\n\nIf a map, list, or string has a given size\n\n\n\n\n\n\nCONTAINSKEY\n\n\nIf a map field contains the given key. This map field can be a top level map field or a map field inside a list of maps\n\n\n\n\n\n\nCONTAINSVALUE\n\n\nIf a map field or a list field contains the given value. If the list contains maps instead of primitives, the values in the maps are used\n\n\n\n\n\n\n\n\nNote: These operators are all typed based on the type of the \nleft hand side\n from the Bullet record. If the elements on the right hand side cannot be\ncasted to the types on the LHS, those items will be ignored for the comparison.\n\n\nThe format for a Relational filter is:\n\n\n{\n    \noperation\n: \n== | != | \n= | \n= | \n | \n | RLIKE | SIZEIS | CONTAINSKEY | CONTAINSVALUE\n\n    \nfield\n: \nrecord_field_name | map_field.subfield\n,\n    \nvalues\n: [\n        { \nkind\n: \nVALUE\n, \ntype\n: \nBOOLEAN | INTEGER | LONG | FLOAT | DOUBLE | STRING | MAP | LIST\n, \nvalue\n: \nfoo\n},\n        { \nkind\n: \nFIELD\n, \ntype\n: \nBOOLEAN | INTEGER | LONG | FLOAT | DOUBLE | STRING | MAP | LIST\n, \nvalue\n: \nanother_record_field_name\n}\n    ]\n}\n\n\n\n\nNote that you may specify \nVALUE\n or \nKIND\n currently for the \nkind\n key in the entries in the \nvalues\n field above, denoting the type of value this is. The \ntype\n field is a \noptional\n and is provided to change the type of the provided \nkind\n (value or field) to the provided type. If you do not provide this type, the value or field provided here will be \ncasted\n to the type of the field (the LHS of the filter).\n\n\nAs a shortcut, you can also specify the following format for \nVALUE\n kind.\n\n\n{\n    \noperation\n: \n== | != | \n= | \n= | \n | \n | RLIKE | SIZEIS | CONTAINSKEY | CONTAINSVALUE\n\n    \nfield\n: \nrecord_field_name | map_field.subfield\n,\n    \nvalues\n: [\n        \nstring values\n,\n        \nthat go here\n,\n        \nwill be casted\n,\n        \nto the\n,\n        \ntype of field\n\n    ]\n}\n\n\n\n\nYou may \nnot\n mix and match both styles in the same filter.\n\n\nMultiple top level relational filters behave as if they are ANDed together.\n This is supported as a convenience to do a bunch of \nAND\ned relational filters without having to nest them in a logical clause.\n\n\nProjections\n\n\nProjections allow you to pull out only the fields needed and rename them (renaming is being supported in order to give\nbetter names to fields pulled out from maps). If projections are not specified, the entire record is returned. If you are querying\nfor raw records, you can use projections to help reduce the load on the system and network.\n\n\n{\n    \nfields\n: {\n        \nfieldA\n: \nnewNameA\n,\n        \nfieldB\n: \nnewNameB\n\n    }\n}\n\n\n\n\nAggregations\n\n\nAggregations allow you to perform some operation on the collected records. They take an optional size to restrict\nthe size of the aggregation (this applies for aggregations high cardinality aggregations and raw records).\n\n\nThe current aggregation types that are supported are:\n\n\n\n\n\n\n\n\nAggregation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nGROUP\n\n\nThe resulting output would be a record containing the result of an operation for each unique group in the specified fields\n\n\n\n\n\n\nCOUNT DISTINCT\n\n\nComputes the number of distinct elements in the fields. (May be approximate)\n\n\n\n\n\n\nLIMIT\n\n\nThe resulting output would be at most the number specified in size.\n\n\n\n\n\n\nDISTRIBUTION\n\n\nComputes distributions of the elements in the field. E.g. Find the median value or various percentile of a field, or get frequency or cumulative frequency distributions\n\n\n\n\n\n\nTOP K\n\n\nReturns the top K most frequently appearing values in the column\n\n\n\n\n\n\n\n\nThe current format for an aggregation is:\n\n\n{\n    \ntype\n: \nGROUP | COUNT DISTINCT | TOP | PERCENTILE | RAW\n,\n    \nsize\n: \na limit on the number of resulting records\n,\n    \nfields\n: {\n        \nfields\n: \nnewNameA\n,\n        \nthat go here\n: \nnewNameB\n,\n        \nare what the\n: \nnewNameC\n,\n        \naggregation type applies to\n: \nnewNameD\n\n    },\n    \nattributes\n: {\n        \nthese\n: \nchange\n,\n        \nper\n: [\n           \naggregation type\n\n        ]\n    }\n}\n\n\n\n\nYou can also use \nLIMIT\n as an alias for \nRAW\n. \nDISTINCT\n is also an alias for \nGROUP\n. These exist to make some queries read a bit better.\n\n\nCurrently we support GROUP aggregations on the following operations:\n\n\n\n\n\n\n\n\nOperation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nCOUNT\n\n\nComputes the number of the elements in the group\n\n\n\n\n\n\nSUM\n\n\nComputes the sum of the elements in the group\n\n\n\n\n\n\nMIN\n\n\nReturns the minimum of the elements in the group\n\n\n\n\n\n\nMAX\n\n\nReturns the maximum of the elements in the group\n\n\n\n\n\n\nAVG\n\n\nComputes the average of the elements in the group\n\n\n\n\n\n\n\n\nAttributes\n\n\nThe \nattributes\n section changes per aggregation \ntype\n.\n\n\nGROUP\n\n\nThe following attributes are supported for \nGROUP\n:\n\n\n    \nattributes\n: {\n        \noperations\n: [\n            {\n                \ntype\n: \nCOUNT\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nSUM\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nMIN\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nMAX\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nAVG\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            }\n        ]\n    }\n\n\n\n\nYou can perform \nSUM\n, \nMIN\n, \nMAX\n, \nAVG\n on non-numeric fields. Bullet will attempt to \ncast the field to a number first.\n If it cannot, that record with the field will be ignored for the operation. For the purposes of \nAVG\n, Bullet will\nperform the average across the numeric values for a field only.\n\n\nCOUNT DISTINCT\n\n\nThe following attributes are supported for \nCOUNT DISTINCT\n:\n\n\n    \nattributes\n: {\n        \nnewName\n: \nresultCountColumnName\n\n    }\n\n\n\n\nNote that the new names you specify in the fields map for aggregations do not apply. You must use the attributes here to give your resulting output count column a name.\n\n\nDISTRIBUTION\n\n\nThe following attributes are supported for \nDISTRIBUTION\n:\n\n\n    \nattributes\n: {\n        \ntype\n: \nQUANTILE | PMF | CDF\n,\n        \nnumberOfPoints\n: \na number of evenly generated points to generate\n,\n        \npoints\n: [ a, free, form, list, of, numbers ],\n        \nstart\n: \na start of the range to generate points\n,\n        \nend\n: \nthe end of the range to generate points\n,\n        \nincrement\n: \nthe increment between the generated points\n,\n    }\n\n\n\n\nYou \nmust\n specify one and only one field using the \nfields\n section in \naggregation\n. Any \nnewName\n you provide will be ignored.\n\n\nThe \ntype\n field picks the type of distribution to apply.\n\n\n\n\n\n\n\n\nType\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nQUANTILE\n\n\nLets you pick out percentiles of the numeric field you provide.\n\n\n\n\n\n\nPMF\n\n\nThe Probability Mass Function distribution lets you get frequency counts and probabilities of ranges or intervals of your numeric field\n\n\n\n\n\n\nCDF\n\n\nThe Cumulative Distribution Function distribution lets you get cumulative frequency counts instead and is otherwise similar to PMF\n\n\n\n\n\n\n\n\nDepending on what \ntype\n you have chosen, the rest of the attributes define \npoints in the domain\n of that distribution.\n\n\nFor \nQUANTILE\n, the points you define will be the \nvalues\n to get the percentiles from. These percentiles are represented as numbers between 0 and 1. This means that your points \nmust\n be between 0 and 1.\n\n\nFor \nPMF\n and \nCDF\n, the points you define will \npartition\n the range of your field values into intervals, with the first interval going from -\n to the first point and the last interval from your last point to +\n. This means that if you generate \nN\n points, you will receive \nN+1\n intervals. The points you define should be in the range of the values for your field to get a meaningful distribution. The domain for your points is therefore, all real numbers but you should narrow down to valid values for the field to get meaningful results.\n\n\nYou have three options to generate points.\n\n\n\n\n\n\n\n\nMethod\n\n\nKeys\n\n\n\n\n\n\n\n\n\n\nNumber of Points\n\n\nYou can use the \nnumberOfPoints\n key to provide a number of points to generate evenly distributed in the full range of your domain\n\n\n\n\n\n\nGenerate Points in a range\n\n\nYou can use \nstart\n, \nend\n and \nincrement\n (\nstart\n \n \nend\n, \nincrement\n \n 0) to specify numbers to generate points in a narrower region of your domain\n\n\n\n\n\n\nSpecify free-form points\n\n\nYou can specify a free-form \narray\n of numbers which will be used as the points\n\n\n\n\n\n\n\n\nNote that If you specify more than one way to generate points, the API will use \nnumberOfPoints\n, followed by \npoints\n, followed by \nstart\n, \nend\n and \nincrement\n and whichever creates valid points will be used first.\n\n\nFor \nPMF\n and \nCDF\n, no matter how you specify your points, the first interval will always be \n(-\n, first point)\n and the last interval will be \n[last point, +\n)\n. You will also get a probability of how likely a value will land in the interval per interval in addition to a frequency (or cumulative frequency) count.\n\n\nAs with \nGROUP\n, Bullet will attempt to \ncast\n your field into a numeric type and ignore it if it cannot.\n\n\nTOP K\n\n\nThe following attributes are supported for \nTOP K\n:\n\n\n  \nattributes\n: {\n      \nthreshold\n: \nrestrict results to having at least this count value\n,\n      \nnewName\n: \nresultCountColumnName\n\n  }\n\n\n\n\nNote that the \nK\n in \nTOP K\n is specified using the \nsize\n field in the \naggregation\n object.\n\n\nPost Aggregations\n\n\nPost Aggregations allow you to perform some final operations on the aggregated data before it is returned, as the name suggests. It is \noptional\n and it is performed for each window. For example, you can cast your result field into another type or perform some math.\n\n\n\n\n\n\n\n\nPost Aggregation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nORDER BY\n\n\nOrders your result by your specified fields in ascending or descending order\n\n\n\n\n\n\nCOMPUTATION\n\n\nSpecify an expression (can be nested expressions) to do math with or cast fields in your result\n\n\n\n\n\n\n\n\nThe \n\"postAggregations\"\n field takes a list of these Post Aggregation entries. The \norder\n of the various post aggregations in this list determines how they are evaluated. Post aggregations can refer to previous results of post aggregations in the list to chain them.\n\n\nORDER BY\n\n\nThis orders result records based on given fields (in ascending order by default). To sort the records in descending order, use the \nDESC\n \ndirection\n. You can specify any fields in each record or from previous post aggregations. Note that the ordering is fully typed, so the types of the fields will be used. If multiple fields are specified, ties will be broken from the list of fields from left to right.\n\n\n{\n  \ntype\n: \nORDERBY\n,\n  \nfields\n: [\nA\n, \nB\n],\n  \ndirection\n: \nDESC\n\n}\n\n\n\n\nCOMPUTATION\n\n\nThis lets you perform arithmetic on the results in a fully nested way. We currently support \n+\n, \n-\n, \n*\n and \n/\n as operations. The format for this is:\n\n\n{\n  \ntype\n: \nCOMPUTATION\n,\n  \nexpression\n: {}\n}\n\n\n\n\nExpressions\n\n\nFor future extensibility, the \nexpression\n in the post aggregation is free form. Currently, we support binary arithmetic operations that can be nested (implying parentheses). This forms a tree of expressions. The leaves of this tree resolve atomic values such as fields or constants. So, there are two kinds of expressions.\n\n\nBinary Expressions\n\n\n{\n  \noperation\n: \n+\n,\n  \nleft\n: {},\n  \nright\n: {},\n  \ntype\n: \nINTEGER | FLOAT | BOOLEAN | DOUBLE | LONG | STRING\n\n}\n\n\n\n\n, where \nleft\n and \nright\n are themselves expressions and \ntype\n is used for force cast the result to the given type.\n\n\nUnary Expressions\n\n\n{\n  \nvalue\n: {\n    \nkind\n: \nFIELD | VALUE\n,\n    \nvalue\n: \nfoo.bar\n,\n    \ntype\n: \nINTEGER | FLOAT | BOOLEAN | DOUBLE | LONG | STRING\n\n  }\n}\n\n\n\n\nThese is the same definition value used for filtering mentioned above and can be used to extract fields from the record as your chosen type or use constants as your chosen type.\n\n\nIf casting \nfails\n in any of the expressions, the expression is ignored.\n\n\nPutting all these together, here is a complete example of post aggregation. This first force computes a new field C, which is the result of doing \n(CAST(foo.bar, LONG) + CAST((CAST(1.2, DOUBLE)/CAST(1, INTEGER)), FLOAT)\n or (C: foo.bar + (1.2/1) for each record in the result window and then orders the result by foo.baz first then by the new the field C.\n\n\nPost Aggregation Example\n\n\n{\n   \npostAggregations\n:[\n      {\n         \ntype\n:\nCOMPUTATION\n,\n         \nexpression\n:{\n            \noperation\n:\n+\n,\n            \nleft\n:{\n               \nvalue\n:{\n                  \nkind\n:\nFIELD\n,\n                  \nvalue\n:\nfoo.bar\n,\n                  \ntype\n:\nLONG\n\n               }\n            },\n            \nright\n:{\n               \noperation\n:\n/\n,\n               \nleft\n:{\n                  \nvalue\n:{\n                     \nkind\n:\nVALUE\n,\n                     \nvalue\n:\n1.2\n,\n                     \ntype\n:\nDOUBLE\n,\n                  }\n               },\n               \nright\n:{\n                  \nvalue\n:{\n                     \nkind\n:\nVALUE\n,\n                     \nvalue\n:\n1\n,\n                     \ntype\n:\nINTEGER\n\n                  }\n               },\n               \ntype\n:\nFLOAT\n\n            },\n            \nnewName\n:\nC\n\n         }\n      },\n      {\n         \ntype\n:\nORDERBY\n,\n         \n fields\n:[\n            \nfoo.baz\n, \nC\n\n         ],\n         \ndirection\n:\nASC\n\n      }\n   ]\n}\n\n\n\n\nWindow\n\n\nThe \"window\" field is \noptional\n and allows you to instruct Bullet to return incremental results. For example you might want to return the COUNT of a field and return that count every 2 seconds.\n\n\nIf \"window\" is omitted Bullet will emit only a single result at the very end of the query.\n\n\nAn example window might look like this:\n\n\nwindow\n: { \nemit\n: { \ntype\n: \nTIME/RECORD\n, \nevery\n: 5000 },\n            \ninclude\n: { \ntype\n: \nTIME/RECORD/ALL\n, \nfirst\n: 5000 } },\n\n\n\n\n\n\n\n\n\n\nField\n\n\nSubField\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nemit\n\n\n\n\nThis object specifies when a window should be emitted and the current results sent back to the user\n\n\n\n\n\n\n\n\ntype\n\n\nMust be \"TIME\" or \"RECORD\" - specifying if the window should be emitted after X number of milliseconds, or X number of records\n\n\n\n\n\n\n\n\nevery\n\n\nThe number of milliseconds or records (determined by \"type\" above) that will be contained in the emitted window\n\n\n\n\n\n\ninclude\n\n\n\n\nThis object specifies what will be included in the emitted window\n\n\n\n\n\n\n\n\ntype\n\n\nMust be \"TIME\", \"RECORD\" or \"ALL\" - specifying if the window should include X number of milliseconds, X number of records, or all results since the beginning of the whole query\n\n\n\n\n\n\n\n\nfirst\n\n\nSpecifies the number of records/milliseconds at the beginning of this window to include in the emitted result - it should be omitted if \"type\" is \"ALL\"\n\n\n\n\n\n\n\n\nNOTE: Not all windowing types are supported at this time.\n\n\nCurrently Bullet supports the following window types\n:\n\n\n\n\nTime-Based Tumbling Windows\n\n\nAdditive Tumbling Windows\n\n\nReactive Record-Based Windows\n\n\nNo Window\n\n\n\n\nSupport for more windows will be added in the future.\n\n\nEach currently supported window type will be described below:\n\n\nTime-Based Tumbling Windows\n\n\nCurrently time-based tumbling windows \nmust\n have emit == include. In other words, only the entire window can be emitted, and windows must be adjacent.\n\n\n\n\nThe above example windowing would be specified with the window:\n\n\nwindow\n: { \nemit\n: { \ntype\n: \nTIME\n, \nevery\n: 3000 },\n            \ninclude\n: { \ntype\n: \nTIME\n, \nfirst\n: 3000 } },\n\n\n\n\nAny aggregation can be done in each window, or the raw records themselves can be returned as specified in the \"aggregation\" object.\n\n\nIn this example the first window would include 3 records, the second would include 4 records, the third would include 3 records and the fourth would include 2 records.\n\n\nAdditive Tumbling Windows\n\n\nAdditive tumbling windows emit with the same logic as time-based tumbling windows, but include ALL results from the beginning of the query:\n\n\n\n\nThe above example would be specified with the window:\n\n\nwindow\n: { \nemit\n: { \ntype\n: \nTIME\n, \nevery\n: 3000 },\n            \ninclude\n: { \ntype\n: \nALL\n } },\n\n\n\n\nIn this example the first window would include 3 records, the second would include 7 records, the third would include 10 records and the fourth would include 12 records.\n\n\nSliding Windows\n\n\nSliding windows emit based on the arrival of an event, rather than after a certain period of time. In general sliding windows often do some aggregation on the previous X records, or on all records that arrived in the last X seconds. At this time, Bullet only supports sliding windows on the previous X records. It does not support sliding windows with any aggregation but \nRAW\n at this time. If you set X to 1, the query will effectively return each record as they arrive to the user.\n\n\n\n\nThe above example would be specified with the window (you may replace every and last with higher values):\n\n\nwindow\n: { \nemit\n: { \ntype\n: \nRECORD\n, \nevery\n: 1 },\n            \ninclude\n: { \ntype\n: \nRECORD\n, \nlast\n: 1 } },\n\n\n\n\nNo Window\n\n\nIf the \"window\" field is optional. If it is  omitted, the query will only emit when the entire query is finished.\n\n\nResults\n\n\nBullet results are JSON objects with two fields:\n\n\n\n\n\n\n\n\nField\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nrecords\n\n\nThis field contains the list of matching records\n\n\n\n\n\n\nmeta\n\n\nThis field is a map that contains meta information about the query, such as the time the query was received, error data, etc. These are configurable at launch time.\n\n\n\n\n\n\n\n\nFor a detailed description of how to perform these queries and see example results, see \nExamples\n.", 
            "title": "JSON (DEPRECATED)"
        }, 
        {
            "location": "/ws/api-json/#bullet-json-api", 
            "text": "This section gives a comprehensive overview of the old Web Service API for launching Bullet JSON queries. This was  deprecated  in favor of  BQL  in Bullet 1.0+.  The JSON API is the old Query format that was expected by the API and the Backend prior to Bullet 1.0.  The API  is a more user-friendly API which can also be used - the Web Service prior to Bullet 1.0 will automatically detect the BQL query and convert the query to this JSON format before submitting it to the backend. With the addition of Post Aggregations and Expressions,\nit is a lot easier to use BQL rather than construct the JSON. The Bullet Web Service also provides  an API  to convert BQL to JSON if you so desire.   For info on how to use the UI, see the  UI Usage section  For examples of specific queries see the  Examples  section", 
            "title": "Bullet JSON API"
        }, 
        {
            "location": "/ws/api-json/#constituents-of-a-bullet-query", 
            "text": "The main constituents of a Bullet JSON query are:   filters , which determine which records will be consumed by your query  projection , which determines which fields will be projected in the resulting output from Bullet  aggregation , which allows you to aggregate data and perform aggregation operations  postAggregations , which allows you to perform post aggregations before the result is returned  window , which can be used to return incremental results on \"windowed\" data  duration , which determines the maximum duration of the query in milliseconds   The main constituents of a Bullet query listed above create the top level fields of the Bullet query:  {\n     filters : [{}, {}, ...],\n     projection : {},\n     aggregation : {},\n     postAggregations : [{}, {}, ...],\n     window : {},\n     duration : 20000\n}", 
            "title": "Constituents of a Bullet Query"
        }, 
        {
            "location": "/ws/api-json/#accessing-complex-fields", 
            "text": "Fields inside maps and lists can be accessed using the '.' notation in queries.     Complex Field Type  Example      Map of Primitive  myMap.key    Map of Map of Primitive  myMap.myInnerMap.key    List of Map/Primitive  myList.0    List of Map of Primitive  myListOfMaps.4.key     We will now describe how to specify each of these top-level fields below:", 
            "title": "Accessing Complex Fields"
        }, 
        {
            "location": "/ws/api-json/#filters", 
            "text": "Bullet supports two kinds of filters:   Logical filters  Relational filters", 
            "title": "Filters"
        }, 
        {
            "location": "/ws/api-json/#logical-filters", 
            "text": "Logical filters allow you to combine other filter clauses with logical operations like  AND ,  OR  and  NOT .  The current logical operators allowed in filters are:     Logical Operator  Meaning      AND  All filters must be true. The first false filter evaluated left to right will short-circuit the computation.    OR  Any filter must be true. The first true filter evaluated left to right will short-circuit the computation.    NOT  Negates the value of the first filter clause. The filter is satisfied iff the value is true.     The format for a  single  Logical filter is:  {\n    operation :  AND | OR | NOT \n    clauses : [\n      { operation :  ... , clauses: [{}, ...]},\n      { field :  ... ,  operation :  , values: [ ... ]},\n      { operation :  ... , clauses: [{}, ...]}\n      ...\n   ]\n}  Any other type of filter may be provided as a clause in clauses.  Note that the \"filter\" field in the query is a  list  of as many filters as you'd like.", 
            "title": "Logical Filters"
        }, 
        {
            "location": "/ws/api-json/#relational-filters", 
            "text": "Relational filters allow you to specify conditions on a field, using a comparison operator and a list of constant values or other fields.  The current comparisons allowed in filters are:     Comparison  Meaning      ==  Equal to any value in values    !=  Not equal to any value in values    =  Less than or equal to any value in values    =  Greater than or equal to any value in values     Less than any value in values     Greater than any value in values    RLIKE  Matches using  Java Regex notation , any Regex value in values    SIZEIS  If a map, list, or string has a given size    CONTAINSKEY  If a map field contains the given key. This map field can be a top level map field or a map field inside a list of maps    CONTAINSVALUE  If a map field or a list field contains the given value. If the list contains maps instead of primitives, the values in the maps are used     Note: These operators are all typed based on the type of the  left hand side  from the Bullet record. If the elements on the right hand side cannot be\ncasted to the types on the LHS, those items will be ignored for the comparison.  The format for a Relational filter is:  {\n     operation :  == | != |  = |  = |   |   | RLIKE | SIZEIS | CONTAINSKEY | CONTAINSVALUE \n     field :  record_field_name | map_field.subfield ,\n     values : [\n        {  kind :  VALUE ,  type :  BOOLEAN | INTEGER | LONG | FLOAT | DOUBLE | STRING | MAP | LIST ,  value :  foo },\n        {  kind :  FIELD ,  type :  BOOLEAN | INTEGER | LONG | FLOAT | DOUBLE | STRING | MAP | LIST ,  value :  another_record_field_name }\n    ]\n}  Note that you may specify  VALUE  or  KIND  currently for the  kind  key in the entries in the  values  field above, denoting the type of value this is. The  type  field is a  optional  and is provided to change the type of the provided  kind  (value or field) to the provided type. If you do not provide this type, the value or field provided here will be  casted  to the type of the field (the LHS of the filter).  As a shortcut, you can also specify the following format for  VALUE  kind.  {\n     operation :  == | != |  = |  = |   |   | RLIKE | SIZEIS | CONTAINSKEY | CONTAINSVALUE \n     field :  record_field_name | map_field.subfield ,\n     values : [\n         string values ,\n         that go here ,\n         will be casted ,\n         to the ,\n         type of field \n    ]\n}  You may  not  mix and match both styles in the same filter.  Multiple top level relational filters behave as if they are ANDed together.  This is supported as a convenience to do a bunch of  AND ed relational filters without having to nest them in a logical clause.", 
            "title": "Relational Filters"
        }, 
        {
            "location": "/ws/api-json/#projections", 
            "text": "Projections allow you to pull out only the fields needed and rename them (renaming is being supported in order to give\nbetter names to fields pulled out from maps). If projections are not specified, the entire record is returned. If you are querying\nfor raw records, you can use projections to help reduce the load on the system and network.  {\n     fields : {\n         fieldA :  newNameA ,\n         fieldB :  newNameB \n    }\n}", 
            "title": "Projections"
        }, 
        {
            "location": "/ws/api-json/#aggregations", 
            "text": "Aggregations allow you to perform some operation on the collected records. They take an optional size to restrict\nthe size of the aggregation (this applies for aggregations high cardinality aggregations and raw records).  The current aggregation types that are supported are:     Aggregation  Meaning      GROUP  The resulting output would be a record containing the result of an operation for each unique group in the specified fields    COUNT DISTINCT  Computes the number of distinct elements in the fields. (May be approximate)    LIMIT  The resulting output would be at most the number specified in size.    DISTRIBUTION  Computes distributions of the elements in the field. E.g. Find the median value or various percentile of a field, or get frequency or cumulative frequency distributions    TOP K  Returns the top K most frequently appearing values in the column     The current format for an aggregation is:  {\n     type :  GROUP | COUNT DISTINCT | TOP | PERCENTILE | RAW ,\n     size :  a limit on the number of resulting records ,\n     fields : {\n         fields :  newNameA ,\n         that go here :  newNameB ,\n         are what the :  newNameC ,\n         aggregation type applies to :  newNameD \n    },\n     attributes : {\n         these :  change ,\n         per : [\n            aggregation type \n        ]\n    }\n}  You can also use  LIMIT  as an alias for  RAW .  DISTINCT  is also an alias for  GROUP . These exist to make some queries read a bit better.  Currently we support GROUP aggregations on the following operations:     Operation  Meaning      COUNT  Computes the number of the elements in the group    SUM  Computes the sum of the elements in the group    MIN  Returns the minimum of the elements in the group    MAX  Returns the maximum of the elements in the group    AVG  Computes the average of the elements in the group", 
            "title": "Aggregations"
        }, 
        {
            "location": "/ws/api-json/#attributes", 
            "text": "The  attributes  section changes per aggregation  type .", 
            "title": "Attributes"
        }, 
        {
            "location": "/ws/api-json/#group", 
            "text": "The following attributes are supported for  GROUP :       attributes : {\n         operations : [\n            {\n                 type :  COUNT ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  SUM ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  MIN ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  MAX ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  AVG ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            }\n        ]\n    }  You can perform  SUM ,  MIN ,  MAX ,  AVG  on non-numeric fields. Bullet will attempt to  cast the field to a number first.  If it cannot, that record with the field will be ignored for the operation. For the purposes of  AVG , Bullet will\nperform the average across the numeric values for a field only.", 
            "title": "GROUP"
        }, 
        {
            "location": "/ws/api-json/#count-distinct", 
            "text": "The following attributes are supported for  COUNT DISTINCT :       attributes : {\n         newName :  resultCountColumnName \n    }  Note that the new names you specify in the fields map for aggregations do not apply. You must use the attributes here to give your resulting output count column a name.", 
            "title": "COUNT DISTINCT"
        }, 
        {
            "location": "/ws/api-json/#distribution", 
            "text": "The following attributes are supported for  DISTRIBUTION :       attributes : {\n         type :  QUANTILE | PMF | CDF ,\n         numberOfPoints :  a number of evenly generated points to generate ,\n         points : [ a, free, form, list, of, numbers ],\n         start :  a start of the range to generate points ,\n         end :  the end of the range to generate points ,\n         increment :  the increment between the generated points ,\n    }  You  must  specify one and only one field using the  fields  section in  aggregation . Any  newName  you provide will be ignored.  The  type  field picks the type of distribution to apply.     Type  Meaning      QUANTILE  Lets you pick out percentiles of the numeric field you provide.    PMF  The Probability Mass Function distribution lets you get frequency counts and probabilities of ranges or intervals of your numeric field    CDF  The Cumulative Distribution Function distribution lets you get cumulative frequency counts instead and is otherwise similar to PMF     Depending on what  type  you have chosen, the rest of the attributes define  points in the domain  of that distribution.  For  QUANTILE , the points you define will be the  values  to get the percentiles from. These percentiles are represented as numbers between 0 and 1. This means that your points  must  be between 0 and 1.  For  PMF  and  CDF , the points you define will  partition  the range of your field values into intervals, with the first interval going from -  to the first point and the last interval from your last point to + . This means that if you generate  N  points, you will receive  N+1  intervals. The points you define should be in the range of the values for your field to get a meaningful distribution. The domain for your points is therefore, all real numbers but you should narrow down to valid values for the field to get meaningful results.  You have three options to generate points.     Method  Keys      Number of Points  You can use the  numberOfPoints  key to provide a number of points to generate evenly distributed in the full range of your domain    Generate Points in a range  You can use  start ,  end  and  increment  ( start     end ,  increment    0) to specify numbers to generate points in a narrower region of your domain    Specify free-form points  You can specify a free-form  array  of numbers which will be used as the points     Note that If you specify more than one way to generate points, the API will use  numberOfPoints , followed by  points , followed by  start ,  end  and  increment  and whichever creates valid points will be used first.  For  PMF  and  CDF , no matter how you specify your points, the first interval will always be  (- , first point)  and the last interval will be  [last point, + ) . You will also get a probability of how likely a value will land in the interval per interval in addition to a frequency (or cumulative frequency) count.  As with  GROUP , Bullet will attempt to  cast  your field into a numeric type and ignore it if it cannot.", 
            "title": "DISTRIBUTION"
        }, 
        {
            "location": "/ws/api-json/#top-k", 
            "text": "The following attributes are supported for  TOP K :     attributes : {\n       threshold :  restrict results to having at least this count value ,\n       newName :  resultCountColumnName \n  }  Note that the  K  in  TOP K  is specified using the  size  field in the  aggregation  object.", 
            "title": "TOP K"
        }, 
        {
            "location": "/ws/api-json/#post-aggregations", 
            "text": "Post Aggregations allow you to perform some final operations on the aggregated data before it is returned, as the name suggests. It is  optional  and it is performed for each window. For example, you can cast your result field into another type or perform some math.     Post Aggregation  Meaning      ORDER BY  Orders your result by your specified fields in ascending or descending order    COMPUTATION  Specify an expression (can be nested expressions) to do math with or cast fields in your result     The  \"postAggregations\"  field takes a list of these Post Aggregation entries. The  order  of the various post aggregations in this list determines how they are evaluated. Post aggregations can refer to previous results of post aggregations in the list to chain them.", 
            "title": "Post Aggregations"
        }, 
        {
            "location": "/ws/api-json/#order-by", 
            "text": "This orders result records based on given fields (in ascending order by default). To sort the records in descending order, use the  DESC   direction . You can specify any fields in each record or from previous post aggregations. Note that the ordering is fully typed, so the types of the fields will be used. If multiple fields are specified, ties will be broken from the list of fields from left to right.  {\n   type :  ORDERBY ,\n   fields : [ A ,  B ],\n   direction :  DESC \n}", 
            "title": "ORDER BY"
        }, 
        {
            "location": "/ws/api-json/#computation", 
            "text": "This lets you perform arithmetic on the results in a fully nested way. We currently support  + ,  - ,  *  and  /  as operations. The format for this is:  {\n   type :  COMPUTATION ,\n   expression : {}\n}", 
            "title": "COMPUTATION"
        }, 
        {
            "location": "/ws/api-json/#expressions", 
            "text": "For future extensibility, the  expression  in the post aggregation is free form. Currently, we support binary arithmetic operations that can be nested (implying parentheses). This forms a tree of expressions. The leaves of this tree resolve atomic values such as fields or constants. So, there are two kinds of expressions.", 
            "title": "Expressions"
        }, 
        {
            "location": "/ws/api-json/#binary-expressions", 
            "text": "{\n   operation :  + ,\n   left : {},\n   right : {},\n   type :  INTEGER | FLOAT | BOOLEAN | DOUBLE | LONG | STRING \n}  , where  left  and  right  are themselves expressions and  type  is used for force cast the result to the given type.", 
            "title": "Binary Expressions"
        }, 
        {
            "location": "/ws/api-json/#unary-expressions", 
            "text": "{\n   value : {\n     kind :  FIELD | VALUE ,\n     value :  foo.bar ,\n     type :  INTEGER | FLOAT | BOOLEAN | DOUBLE | LONG | STRING \n  }\n}  These is the same definition value used for filtering mentioned above and can be used to extract fields from the record as your chosen type or use constants as your chosen type.  If casting  fails  in any of the expressions, the expression is ignored.  Putting all these together, here is a complete example of post aggregation. This first force computes a new field C, which is the result of doing  (CAST(foo.bar, LONG) + CAST((CAST(1.2, DOUBLE)/CAST(1, INTEGER)), FLOAT)  or (C: foo.bar + (1.2/1) for each record in the result window and then orders the result by foo.baz first then by the new the field C.", 
            "title": "Unary Expressions"
        }, 
        {
            "location": "/ws/api-json/#post-aggregation-example", 
            "text": "{\n    postAggregations :[\n      {\n          type : COMPUTATION ,\n          expression :{\n             operation : + ,\n             left :{\n                value :{\n                   kind : FIELD ,\n                   value : foo.bar ,\n                   type : LONG \n               }\n            },\n             right :{\n                operation : / ,\n                left :{\n                   value :{\n                      kind : VALUE ,\n                      value : 1.2 ,\n                      type : DOUBLE ,\n                  }\n               },\n                right :{\n                   value :{\n                      kind : VALUE ,\n                      value : 1 ,\n                      type : INTEGER \n                  }\n               },\n                type : FLOAT \n            },\n             newName : C \n         }\n      },\n      {\n          type : ORDERBY ,\n           fields :[\n             foo.baz ,  C \n         ],\n          direction : ASC \n      }\n   ]\n}", 
            "title": "Post Aggregation Example"
        }, 
        {
            "location": "/ws/api-json/#window", 
            "text": "The \"window\" field is  optional  and allows you to instruct Bullet to return incremental results. For example you might want to return the COUNT of a field and return that count every 2 seconds.  If \"window\" is omitted Bullet will emit only a single result at the very end of the query.  An example window might look like this:  window : {  emit : {  type :  TIME/RECORD ,  every : 5000 },\n             include : {  type :  TIME/RECORD/ALL ,  first : 5000 } },     Field  SubField  Meaning      emit   This object specifies when a window should be emitted and the current results sent back to the user     type  Must be \"TIME\" or \"RECORD\" - specifying if the window should be emitted after X number of milliseconds, or X number of records     every  The number of milliseconds or records (determined by \"type\" above) that will be contained in the emitted window    include   This object specifies what will be included in the emitted window     type  Must be \"TIME\", \"RECORD\" or \"ALL\" - specifying if the window should include X number of milliseconds, X number of records, or all results since the beginning of the whole query     first  Specifies the number of records/milliseconds at the beginning of this window to include in the emitted result - it should be omitted if \"type\" is \"ALL\"     NOTE: Not all windowing types are supported at this time.  Currently Bullet supports the following window types :   Time-Based Tumbling Windows  Additive Tumbling Windows  Reactive Record-Based Windows  No Window   Support for more windows will be added in the future.  Each currently supported window type will be described below:", 
            "title": "Window"
        }, 
        {
            "location": "/ws/api-json/#time-based-tumbling-windows", 
            "text": "Currently time-based tumbling windows  must  have emit == include. In other words, only the entire window can be emitted, and windows must be adjacent.   The above example windowing would be specified with the window:  window : {  emit : {  type :  TIME ,  every : 3000 },\n             include : {  type :  TIME ,  first : 3000 } },  Any aggregation can be done in each window, or the raw records themselves can be returned as specified in the \"aggregation\" object.  In this example the first window would include 3 records, the second would include 4 records, the third would include 3 records and the fourth would include 2 records.", 
            "title": "Time-Based Tumbling Windows"
        }, 
        {
            "location": "/ws/api-json/#additive-tumbling-windows", 
            "text": "Additive tumbling windows emit with the same logic as time-based tumbling windows, but include ALL results from the beginning of the query:   The above example would be specified with the window:  window : {  emit : {  type :  TIME ,  every : 3000 },\n             include : {  type :  ALL  } },  In this example the first window would include 3 records, the second would include 7 records, the third would include 10 records and the fourth would include 12 records.", 
            "title": "Additive Tumbling Windows"
        }, 
        {
            "location": "/ws/api-json/#sliding-windows", 
            "text": "Sliding windows emit based on the arrival of an event, rather than after a certain period of time. In general sliding windows often do some aggregation on the previous X records, or on all records that arrived in the last X seconds. At this time, Bullet only supports sliding windows on the previous X records. It does not support sliding windows with any aggregation but  RAW  at this time. If you set X to 1, the query will effectively return each record as they arrive to the user.   The above example would be specified with the window (you may replace every and last with higher values):  window : {  emit : {  type :  RECORD ,  every : 1 },\n             include : {  type :  RECORD ,  last : 1 } },", 
            "title": "Sliding Windows"
        }, 
        {
            "location": "/ws/api-json/#no-window", 
            "text": "If the \"window\" field is optional. If it is  omitted, the query will only emit when the entire query is finished.", 
            "title": "No Window"
        }, 
        {
            "location": "/ws/api-json/#results", 
            "text": "Bullet results are JSON objects with two fields:     Field  Contents      records  This field contains the list of matching records    meta  This field is a map that contains meta information about the query, such as the time the query was received, error data, etc. These are configurable at launch time.     For a detailed description of how to perform these queries and see example results, see  Examples .", 
            "title": "Results"
        }, 
        {
            "location": "/ui/setup/", 
            "text": "The UI Layer\n\n\nThe Bullet UI lets you easily create and work with Bullet queries and results for your custom data. It stores all created queries, results and other metadata in the local browser storage or \nIndexedDB\n.\n\n\n\n\nReally!? Browser Storage only!?\n\n\nWe're serious about the no persistence thing with Bullet! And while we're at it, we are also not interested in supporting old browsers. Joking aside though, we wanted to keep Bullet as light and simple as possible to start with. We can look into extending support from the server-side by adding a database or the like if needed. In practice, we have found that this isn't as important as it initially seems.\n\n\n\n\nPrerequisites\n\n\nIn order for your UI to work with Bullet, you should have:\n\n\n\n\nAn instance of the \nbackend\n set up\n\n\nAn instance of the \nWeb Service\n set up\n\n\nYou should also have a Web Service serving your schema (either by using the \nfile based serving\n from the Web Service or your own somewhere else)\n\n\n\n\nInstallation\n\n\nWe are considering various packaging options at the moment like Docker etc. In the meantime, the following two options are available:\n\n\nGitHub Releases\n\n\n\n\nHead to the \nReleases page\n and grab the latest release\n\n\nDownload the bullet-ui-vX.X.X.tar.gz archive\n\n\nUnarchive it into your web server where you wish to run the UI.\n\n\nInstall \nNode\n (recommend using \nnvm\n to manage Node versions) on the web server\n\n\n\n\nBuild from source\n\n\n\n\nInstall \nNode\n (recommend using \nnvm\n to manage Node versions).\n\n\nInstall \nYarn\n. Use NPM to install it with \nnpm install -g yarn\n\n\nInstall \nEmber\n. \nsudo npm install -g ember-cli\n (sudo required only if not using nvm)\n\n\ngit clone git@github.com:bullet-db/bullet-ui.git\n\n\ncd bullet-ui\n\n\nyarn\n\n\nember build --environment production\n\n\n\n\nThe entire application with all its assets and dependencies are compiled and placed into dist/. You could point a web server directly at this folder but you will \nonly\n be able to use the default configuration (see \nbelow\n).\n\n\nRunning\n\n\nThere is a Node.js server endpoint defined at \nserver/index.js\n to serve the UI. This dynamically injects the settings (see configuration \nbelow\n) into the served UI based on the environment variable NODE_ENV. You should not need to worry about if you only have one environment.\n\n\nThe entry-point for the UI is the \nExpress\n endpoint defined as the main in package.json that simply adds the server/index.js as a middleware.\n\n\nRegardless of which \ninstallation\n option you chose, you need the following folder structure in order to run the UI:\n\n\ndist/*\nconfig/env-settings.json\nserver/index.js\nexpress-server.js\n\n\n\n\nYou can use node to launch the UI from the top-level of the folder structure above.\n\n\nTo launch the UI with the default settings (without specifying proper API endpoints you will not be able to create or run a query):\n\n\nPORT=8800 node express-server.js\n\n\n\n\nTo launch with custom settings:\n\n\nNODE_ENV=\nyour_property_name_from_env-settings.json\n PORT=8800 node express-server.js\n\n\n\n\nVisit localhost:8800 to see your UI that should be configured with the right settings.\n\n\nConfiguration\n\n\nThe configuration for the UI lets you have different instances of Bullet for different environments (e.g. CI, Staging, Production). These settings can be found in \nenv-settings.json\n.\n\n\n\n\n\n\n\n\nSetting\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nqueryHost\n\n\nThe end point (port included) of your Web Service machine that is talking to the Bullet backend\n\n\n\n\n\n\nqueryNamespace\n\n\nAny qualifiers you have after your host and port on your Web Service running on your \nqueryHost\n\n\n\n\n\n\nqueryPath\n\n\nThe path fragment after the \nqueryNamespace\n on your Web Service running on your \nqueryHost\n for the WebSocket endpoint\n\n\n\n\n\n\nvalidationPath\n\n\nThe path fragment after the \nqueryNamespace\n on your Web Service running on your \nqueryHost\n for the Query Validation endpoint\n\n\n\n\n\n\nqueryStompRequestChannel\n\n\nThe fragment after this is the Stomp Request channel as configured in your Web Service for the WebSocket endpoint\n\n\n\n\n\n\nqueryStompResponseChannel\n\n\nThe fragment after this is the Stomp Response channel as configured in your Web Service for the WebSocket endpoint\n\n\n\n\n\n\nschemaHost\n\n\nThe end point (port included) of your Web Service machine that is serving your schema in the JSON API format (see \nWeb Service setup\n for details.)\n\n\n\n\n\n\nschemaNamespace\n\n\nThe path fragment on your schema Web Service running on the \nschemaHost\n. There is no \nschemaPath\n because it \nmust\n be \ncolumns\n in order for the UI to be able fetch the column resource (the fields in your schema).\n\n\n\n\n\n\nmodelVersion\n\n\nThis is used an indicator to apply changes to the stored queries, results etc. It is monotonically increasing. On startup, changes specified in \nmigrations\n will be applied if the old modelVersion is not present or is \n than this number. This is generally incremented by the UI once backwards-incompatible changes are made.\n\n\n\n\n\n\nmigrations\n\n\nis an object that currently supports one key: \ndeletions\n of type string. The value can be set to either \nresult\n or \nquery\n. The former wipes all existing results. The latter wipes everything. See \nmodelVersion\n above.\n\n\n\n\n\n\nhelpLinks\n\n\nIs a list of objects, where each object is a help link. These links populate the \"Help\" drop-down on the UI's top navbar. You can add links to explain your data for example\n\n\n\n\n\n\ndefaultQuery\n\n\nCan either be a \nAPI Query\n or a URL from which one could be fetched dynamically. The UI makes this the query created on every newly created Query. You could use this as a way to have user specific (for example, cookie based) filters created for your users or customize an aggregation when they create a new query in the UI. Note that Builder Query do not support all API queries yet but whatever query you specify here (as long as it's a valid query) will be supported in the BQL query page in the UI. If it is not possible to convert your query into a Builder query, a default one will be used instead.\n\n\n\n\n\n\nbugLink\n\n\nIs a URL that by default points to the issues page for the UI GitHub repository. You can change it to point to your own custom JIRA queue or something else\n\n\n\n\n\n\ndefaultValues\n\n\nIs an object that lets you configures defaults for various query parameters and lets you tie your custom backend settings to the UI\n\n\n\n\n\n\n\n\nThese are the properties in the \ndefaultValues\n object. The Validated column denotes if the value is used when validating a query for correctness and the In Help column denotes if the value is displayed in the popover help messages in the UI.\n\n\n\n\n\n\n\n\nDefault Values\n\n\nValidated\n\n\nIn Help\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\naggregationMaxSize\n\n\nYes\n\n\nYes\n\n\nThe size used when doing a Count Distinct, Distinct, Group By, or Distribution query. Set this to your max aggregations size in your backend configuration\n\n\n\n\n\n\nrawMaxSize\n\n\nYes\n\n\nYes\n\n\nThe maximum size for a Raw query. Set this to your max raw aggregation size in your backend configuration\n\n\n\n\n\n\ndurationMaxSecs\n\n\nYes\n\n\nYes\n\n\nThe maximum duration for a query. Set this to the seconds version of the milliseconds max duration in your backend configuration\n\n\n\n\n\n\ndistributionNumberOfPoints\n\n\nYes\n\n\nNo\n\n\nThe default value filled in for the Number of Points field for all Distribution aggregations\n\n\n\n\n\n\ndistributionQuantilePoints\n\n\nNo\n\n\nNo\n\n\nThe default value filled in for the Points field for Quantile Distribution aggregations\n\n\n\n\n\n\ndistributionQuantileStart\n\n\nNo\n\n\nNo\n\n\nThe default value filled in for the Start field for Quantile Distribution aggregations\n\n\n\n\n\n\ndistributionQuantileEnd\n\n\nNo\n\n\nNo\n\n\nThe default value filled in for the End field for Quantile Distribution aggregations\n\n\n\n\n\n\ndistributionQuantileIncrement\n\n\nNo\n\n\nNo\n\n\nThe default value filled in for the Increment field for Quantile Distribution aggregations\n\n\n\n\n\n\nwindowEmitFrequencyMinSecs\n\n\nYes\n\n\nNo\n\n\nThe minimum time interval at which a time based window can be returned in seconds. Set this to the minimum window emit frequency from your backend configuration\n\n\n\n\n\n\neveryForRecordBasedWindow\n\n\nNo\n\n\nNo\n\n\nThe default value for the number of records in a window for a record based window\n\n\n\n\n\n\neveryForTimeBasedWindow\n\n\nNo\n\n\nNo\n\n\nThe default value for the number of records in a window for a time based window in milliseconds\n\n\n\n\n\n\nsketches.countDistinctMaxEntries\n\n\nNo\n\n\nYes\n\n\nThe maximum entries configured for your Count Distinct sketch in your backend configuration\n\n\n\n\n\n\nsketches.groupByMaxEntries\n\n\nNo\n\n\nYes\n\n\nThe maximum entries configured for your Group sketch in your backend configuration\n\n\n\n\n\n\nsketches.distributionMaxEntries\n\n\nNo\n\n\nYes\n\n\nThe maximum entries configured for your Distribution sketch in your backend configuration\n\n\n\n\n\n\nsketches.distributionMaxNumberOfPoints\n\n\nYes\n\n\nYes\n\n\nThe maximum number of points allowed for Distribution aggregations in your backend configuration\n\n\n\n\n\n\nsketches.topKMaxEntries\n\n\nNo\n\n\nYes\n\n\nThe maximum entries configured for your Top K sketch in your backend configuration\n\n\n\n\n\n\nsketches.topKErrorType\n\n\nNo\n\n\nYes\n\n\nThe ErrorType used for your Top K sketch in your backend configuration. You should set this to the full String rather than \nNFN\n or \nNFP\n\n\n\n\n\n\nmetadataKeyMapping.querySection\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Query Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.windowSection\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Window Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.sketchSection\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Theta Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.theta\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Theta Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.uniquesEstimate\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Uniques Estimate Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.queryCreationTime\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Query Creation Time Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.queryTerminationTime\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Query Termination Time Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.estimatedResult\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Estimated Result Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.standardDeviations\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Standard Deviations Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.normalizedRankError\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Normalized Rank Error Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.maximumCountError\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Maximum Count Error Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.itemsSeen\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Items Seen Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.minimumValue\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Minimum Value Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.maximumValue\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Maximum Value Concept in your backend configuration\n\n\n\n\n\n\n\n\nYou can specify values for each property above in the \nenv-settings.json\n file. These will be used when running a custom instance of the UI (see \nabove\n).\n\n\nThe \ndefault\n property in the \nenv-settings.json\n that loads default settings for the UI that can be selectively overridden based on which environment you are running on. All settings explained above have default values\nthat are the same as the \ndefault backend settings\n.\n\n\n{\n  \ndefault\n: {\n    \nqueryHost\n: \nhttp://localhost:5555\n,\n    \nqueryNamespace\n: \napi/bullet/queries\n,\n    \nqueryPath\n: \nws-query\n,\n    \nvalidationPath\n: \nvalidate-query\n,\n    \nqueryStompRequestChannel\n: \n/server/request\n,\n    \nqueryStompResponseChannel\n: \n/client/response\n,\n    \nschemaHost\n: \nhttp://localhost:5555\n,\n    \nschemaNamespace\n: \napi/bullet\n,\n    \nhelpLinks\n: [\n      {\n        \nname\n: \nTutorials\n,\n        \nlink\n: \nhttps://bullet-db.github.io/ui/usage\n\n      }\n    ],\n    \nbugLink\n: \nhttps://github.com/bullet-db/bullet-ui/issues\n,\n    \nmodelVersion\n: 4,\n    \nmigrations\n: {\n      \ndeletions\n: \nquery\n\n    },\n    \ndefaultQuery\n: \nSELECT COUNT(*) FROM STREAM(60000, TIME) WINDOWING TUMBLING(2000, TIME);\n,\n    \ndefaultValues\n: {\n      \naggregationMaxSize\n: 500,\n      \nrawMaxSize\n: 100,\n      \ndurationMaxSecs\n: 9007199254740,\n      \ndistributionNumberOfPoints\n: 11,\n      \ndistributionQuantilePoints\n: \n0, 0.25, 0.5, 0.75, 0.9, 1\n,\n      \ndistributionQuantileStart\n: 0,\n      \ndistributionQuantileEnd\n: 1,\n      \ndistributionQuantileIncrement\n: 0.1,\n      \nwindowEmitFrequencyMinSecs\n: 1,\n      \neveryForRecordBasedWindow\n: 1,\n      \neveryForTimeBasedWindow\n: 2000,\n      \nsketches\n: {\n        \ncountDistinctMaxEntries\n: 16384,\n        \ngroupByMaxEntries\n: 512,\n        \ndistributionMaxEntries\n: 1024,\n        \ndistributionMaxNumberOfPoints\n: 100,\n        \ntopKMaxEntries\n: 1024,\n        \ntopKErrorType\n: \nNo False Negatives\n\n      },\n      \nmetadataKeyMapping\n: {\n        \nquerySection\n: \nQuery\n,\n        \nwindowSection\n: \nWindow\n,\n        \nsketchSection\n: \nSketch\n,\n        \ntheta\n: \nTheta\n,\n        \nuniquesEstimate\n: \nUniques Estimate\n,\n        \nqueryCreationTime\n: \nReceive Time\n,\n        \nqueryTerminationTime\n: \nFinish Time\n,\n        \nestimatedResult\n: \nWas Estimated\n,\n        \nstandardDeviations\n: \nStandard Deviations\n,\n        \nnormalizedRankError\n: \nNormalized Rank Error\n,\n        \nmaximumCountError\n: \nMaximum Count Error\n,\n        \nitemsSeen\n: \nItems Seen\n,\n        \nminimumValue\n: \nMinimum Value\n,\n        \nmaximumValue\n: \nMaximum Value\n,\n        \nwindowNumber\n: \nNumber\n,\n        \nwindowSize\n: \nSize\n,\n        \nwindowEmitTime\n: \nEmit Time\n,\n        \nexpectedEmitTime\n: \nExpected Emit Time\n\n      }\n    }\n  }\n}\n\n\n\n\nYou can add more properties for each environment you have the UI running on and \noverride\n the properties in the \ndefault\n object. See \nbelow\n for an example.\n\n\n\n\nCORS\n\n\nAll your Web Service endpoints must support CORS (return the right headers) in order for the UI to be able to communicate with it. The Bullet Web Service already does this for the Query and Schema endpoints.\n\n\n\n\nExample\n\n\nTo cement all this, if you wanted an instance of the UI in your CI environment, you could add another property to the \nenv-settings.json\n file.\n\n\n{\n    \nci\n: {\n        \nqueryHost\n: \nhttp://bullet-ws.dev.domain.com:4080\n,\n        \nschemaHost\n: \nhttp://bullet-ws.dev.domain.com:4080\n,\n        \nhelpLinks\n: [\n          {\n            \nname\n: \nCustom Documentation\n,\n            \nlink\n: \nhttp://data.docs.domain.com\n\n          }\n        ],\n        \ndefaultValues\n : {\n            \ndurationMaxSecs\n: 300,\n            \nsketches\n: {\n                \ncountDistinctMaxEntries\n: 32768,\n                \ndistributionMaxNumberOfPoints\n: 50\n            }\n        },\n        \ndefaultQuery\n: \nhttp://bullet-ws.dev.domain.com:4080/custom-endpoint/api/defaultQuery\n\n    }\n}\n\n\n\n\nYour UI on your CI environment will:\n\n\n\n\nTalk using Websockets to \nhttp://bullet-ws.dev.domain.com:4080/api/bullet/ws-query\n for UI created Bullet queries\n\n\nGET the schema from \nhttp://bullet-ws.dev.domain.com:4080/api/bullet/columns\n\n\nValidate queries in the BQL page with \nhttp://bullet-ws.dev.domain.com:4080/api/bullet/validate-query\n\n\nPopulate an additional link on the Help drop-down pointing to \nhttp://data.docs.domain.com\n\n\nAllow queries to run as long as 300 seconds\n\n\nUse 32768 in the help menu for the max number of unique elements that can be counted exactly\n\n\nAllow only 50 points to be generated for Distribution queries\n\n\nGET and cache a default query from \nhttp://bullet-ws.dev.domain.com:4080/custom-endpoint/api/defaultQuery\n\n\n\n\nYou would make express use these settings by running\n\n\nNODE_ENV=ci PORT=8800 node express-server.js", 
            "title": "Setup"
        }, 
        {
            "location": "/ui/setup/#the-ui-layer", 
            "text": "The Bullet UI lets you easily create and work with Bullet queries and results for your custom data. It stores all created queries, results and other metadata in the local browser storage or  IndexedDB .   Really!? Browser Storage only!?  We're serious about the no persistence thing with Bullet! And while we're at it, we are also not interested in supporting old browsers. Joking aside though, we wanted to keep Bullet as light and simple as possible to start with. We can look into extending support from the server-side by adding a database or the like if needed. In practice, we have found that this isn't as important as it initially seems.", 
            "title": "The UI Layer"
        }, 
        {
            "location": "/ui/setup/#prerequisites", 
            "text": "In order for your UI to work with Bullet, you should have:   An instance of the  backend  set up  An instance of the  Web Service  set up  You should also have a Web Service serving your schema (either by using the  file based serving  from the Web Service or your own somewhere else)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/ui/setup/#installation", 
            "text": "We are considering various packaging options at the moment like Docker etc. In the meantime, the following two options are available:", 
            "title": "Installation"
        }, 
        {
            "location": "/ui/setup/#github-releases", 
            "text": "Head to the  Releases page  and grab the latest release  Download the bullet-ui-vX.X.X.tar.gz archive  Unarchive it into your web server where you wish to run the UI.  Install  Node  (recommend using  nvm  to manage Node versions) on the web server", 
            "title": "GitHub Releases"
        }, 
        {
            "location": "/ui/setup/#build-from-source", 
            "text": "Install  Node  (recommend using  nvm  to manage Node versions).  Install  Yarn . Use NPM to install it with  npm install -g yarn  Install  Ember .  sudo npm install -g ember-cli  (sudo required only if not using nvm)  git clone git@github.com:bullet-db/bullet-ui.git  cd bullet-ui  yarn  ember build --environment production   The entire application with all its assets and dependencies are compiled and placed into dist/. You could point a web server directly at this folder but you will  only  be able to use the default configuration (see  below ).", 
            "title": "Build from source"
        }, 
        {
            "location": "/ui/setup/#running", 
            "text": "There is a Node.js server endpoint defined at  server/index.js  to serve the UI. This dynamically injects the settings (see configuration  below ) into the served UI based on the environment variable NODE_ENV. You should not need to worry about if you only have one environment.  The entry-point for the UI is the  Express  endpoint defined as the main in package.json that simply adds the server/index.js as a middleware.  Regardless of which  installation  option you chose, you need the following folder structure in order to run the UI:  dist/*\nconfig/env-settings.json\nserver/index.js\nexpress-server.js  You can use node to launch the UI from the top-level of the folder structure above.  To launch the UI with the default settings (without specifying proper API endpoints you will not be able to create or run a query):  PORT=8800 node express-server.js  To launch with custom settings:  NODE_ENV= your_property_name_from_env-settings.json  PORT=8800 node express-server.js  Visit localhost:8800 to see your UI that should be configured with the right settings.", 
            "title": "Running"
        }, 
        {
            "location": "/ui/setup/#configuration", 
            "text": "The configuration for the UI lets you have different instances of Bullet for different environments (e.g. CI, Staging, Production). These settings can be found in  env-settings.json .     Setting  Meaning      queryHost  The end point (port included) of your Web Service machine that is talking to the Bullet backend    queryNamespace  Any qualifiers you have after your host and port on your Web Service running on your  queryHost    queryPath  The path fragment after the  queryNamespace  on your Web Service running on your  queryHost  for the WebSocket endpoint    validationPath  The path fragment after the  queryNamespace  on your Web Service running on your  queryHost  for the Query Validation endpoint    queryStompRequestChannel  The fragment after this is the Stomp Request channel as configured in your Web Service for the WebSocket endpoint    queryStompResponseChannel  The fragment after this is the Stomp Response channel as configured in your Web Service for the WebSocket endpoint    schemaHost  The end point (port included) of your Web Service machine that is serving your schema in the JSON API format (see  Web Service setup  for details.)    schemaNamespace  The path fragment on your schema Web Service running on the  schemaHost . There is no  schemaPath  because it  must  be  columns  in order for the UI to be able fetch the column resource (the fields in your schema).    modelVersion  This is used an indicator to apply changes to the stored queries, results etc. It is monotonically increasing. On startup, changes specified in  migrations  will be applied if the old modelVersion is not present or is   than this number. This is generally incremented by the UI once backwards-incompatible changes are made.    migrations  is an object that currently supports one key:  deletions  of type string. The value can be set to either  result  or  query . The former wipes all existing results. The latter wipes everything. See  modelVersion  above.    helpLinks  Is a list of objects, where each object is a help link. These links populate the \"Help\" drop-down on the UI's top navbar. You can add links to explain your data for example    defaultQuery  Can either be a  API Query  or a URL from which one could be fetched dynamically. The UI makes this the query created on every newly created Query. You could use this as a way to have user specific (for example, cookie based) filters created for your users or customize an aggregation when they create a new query in the UI. Note that Builder Query do not support all API queries yet but whatever query you specify here (as long as it's a valid query) will be supported in the BQL query page in the UI. If it is not possible to convert your query into a Builder query, a default one will be used instead.    bugLink  Is a URL that by default points to the issues page for the UI GitHub repository. You can change it to point to your own custom JIRA queue or something else    defaultValues  Is an object that lets you configures defaults for various query parameters and lets you tie your custom backend settings to the UI     These are the properties in the  defaultValues  object. The Validated column denotes if the value is used when validating a query for correctness and the In Help column denotes if the value is displayed in the popover help messages in the UI.     Default Values  Validated  In Help  Meaning      aggregationMaxSize  Yes  Yes  The size used when doing a Count Distinct, Distinct, Group By, or Distribution query. Set this to your max aggregations size in your backend configuration    rawMaxSize  Yes  Yes  The maximum size for a Raw query. Set this to your max raw aggregation size in your backend configuration    durationMaxSecs  Yes  Yes  The maximum duration for a query. Set this to the seconds version of the milliseconds max duration in your backend configuration    distributionNumberOfPoints  Yes  No  The default value filled in for the Number of Points field for all Distribution aggregations    distributionQuantilePoints  No  No  The default value filled in for the Points field for Quantile Distribution aggregations    distributionQuantileStart  No  No  The default value filled in for the Start field for Quantile Distribution aggregations    distributionQuantileEnd  No  No  The default value filled in for the End field for Quantile Distribution aggregations    distributionQuantileIncrement  No  No  The default value filled in for the Increment field for Quantile Distribution aggregations    windowEmitFrequencyMinSecs  Yes  No  The minimum time interval at which a time based window can be returned in seconds. Set this to the minimum window emit frequency from your backend configuration    everyForRecordBasedWindow  No  No  The default value for the number of records in a window for a record based window    everyForTimeBasedWindow  No  No  The default value for the number of records in a window for a time based window in milliseconds    sketches.countDistinctMaxEntries  No  Yes  The maximum entries configured for your Count Distinct sketch in your backend configuration    sketches.groupByMaxEntries  No  Yes  The maximum entries configured for your Group sketch in your backend configuration    sketches.distributionMaxEntries  No  Yes  The maximum entries configured for your Distribution sketch in your backend configuration    sketches.distributionMaxNumberOfPoints  Yes  Yes  The maximum number of points allowed for Distribution aggregations in your backend configuration    sketches.topKMaxEntries  No  Yes  The maximum entries configured for your Top K sketch in your backend configuration    sketches.topKErrorType  No  Yes  The ErrorType used for your Top K sketch in your backend configuration. You should set this to the full String rather than  NFN  or  NFP    metadataKeyMapping.querySection  No  Yes  The name of the Metadata key for the Query Concept in your backend configuration    metadataKeyMapping.windowSection  No  Yes  The name of the Metadata key for the Window Concept in your backend configuration    metadataKeyMapping.sketchSection  No  Yes  The name of the Metadata key for the Theta Concept in your backend configuration    metadataKeyMapping.theta  No  Yes  The name of the Metadata key for the Theta Concept in your backend configuration    metadataKeyMapping.uniquesEstimate  No  Yes  The name of the Metadata key for the Uniques Estimate Concept in your backend configuration    metadataKeyMapping.queryCreationTime  No  Yes  The name of the Metadata key for the Query Creation Time Concept in your backend configuration    metadataKeyMapping.queryTerminationTime  No  Yes  The name of the Metadata key for the Query Termination Time Concept in your backend configuration    metadataKeyMapping.estimatedResult  No  Yes  The name of the Metadata key for the Estimated Result Concept in your backend configuration    metadataKeyMapping.standardDeviations  No  Yes  The name of the Metadata key for the Standard Deviations Concept in your backend configuration    metadataKeyMapping.normalizedRankError  No  Yes  The name of the Metadata key for the Normalized Rank Error Concept in your backend configuration    metadataKeyMapping.maximumCountError  No  Yes  The name of the Metadata key for the Maximum Count Error Concept in your backend configuration    metadataKeyMapping.itemsSeen  No  Yes  The name of the Metadata key for the Items Seen Concept in your backend configuration    metadataKeyMapping.minimumValue  No  Yes  The name of the Metadata key for the Minimum Value Concept in your backend configuration    metadataKeyMapping.maximumValue  No  Yes  The name of the Metadata key for the Maximum Value Concept in your backend configuration     You can specify values for each property above in the  env-settings.json  file. These will be used when running a custom instance of the UI (see  above ).  The  default  property in the  env-settings.json  that loads default settings for the UI that can be selectively overridden based on which environment you are running on. All settings explained above have default values\nthat are the same as the  default backend settings .  {\n   default : {\n     queryHost :  http://localhost:5555 ,\n     queryNamespace :  api/bullet/queries ,\n     queryPath :  ws-query ,\n     validationPath :  validate-query ,\n     queryStompRequestChannel :  /server/request ,\n     queryStompResponseChannel :  /client/response ,\n     schemaHost :  http://localhost:5555 ,\n     schemaNamespace :  api/bullet ,\n     helpLinks : [\n      {\n         name :  Tutorials ,\n         link :  https://bullet-db.github.io/ui/usage \n      }\n    ],\n     bugLink :  https://github.com/bullet-db/bullet-ui/issues ,\n     modelVersion : 4,\n     migrations : {\n       deletions :  query \n    },\n     defaultQuery :  SELECT COUNT(*) FROM STREAM(60000, TIME) WINDOWING TUMBLING(2000, TIME); ,\n     defaultValues : {\n       aggregationMaxSize : 500,\n       rawMaxSize : 100,\n       durationMaxSecs : 9007199254740,\n       distributionNumberOfPoints : 11,\n       distributionQuantilePoints :  0, 0.25, 0.5, 0.75, 0.9, 1 ,\n       distributionQuantileStart : 0,\n       distributionQuantileEnd : 1,\n       distributionQuantileIncrement : 0.1,\n       windowEmitFrequencyMinSecs : 1,\n       everyForRecordBasedWindow : 1,\n       everyForTimeBasedWindow : 2000,\n       sketches : {\n         countDistinctMaxEntries : 16384,\n         groupByMaxEntries : 512,\n         distributionMaxEntries : 1024,\n         distributionMaxNumberOfPoints : 100,\n         topKMaxEntries : 1024,\n         topKErrorType :  No False Negatives \n      },\n       metadataKeyMapping : {\n         querySection :  Query ,\n         windowSection :  Window ,\n         sketchSection :  Sketch ,\n         theta :  Theta ,\n         uniquesEstimate :  Uniques Estimate ,\n         queryCreationTime :  Receive Time ,\n         queryTerminationTime :  Finish Time ,\n         estimatedResult :  Was Estimated ,\n         standardDeviations :  Standard Deviations ,\n         normalizedRankError :  Normalized Rank Error ,\n         maximumCountError :  Maximum Count Error ,\n         itemsSeen :  Items Seen ,\n         minimumValue :  Minimum Value ,\n         maximumValue :  Maximum Value ,\n         windowNumber :  Number ,\n         windowSize :  Size ,\n         windowEmitTime :  Emit Time ,\n         expectedEmitTime :  Expected Emit Time \n      }\n    }\n  }\n}  You can add more properties for each environment you have the UI running on and  override  the properties in the  default  object. See  below  for an example.   CORS  All your Web Service endpoints must support CORS (return the right headers) in order for the UI to be able to communicate with it. The Bullet Web Service already does this for the Query and Schema endpoints.", 
            "title": "Configuration"
        }, 
        {
            "location": "/ui/setup/#example", 
            "text": "To cement all this, if you wanted an instance of the UI in your CI environment, you could add another property to the  env-settings.json  file.  {\n     ci : {\n         queryHost :  http://bullet-ws.dev.domain.com:4080 ,\n         schemaHost :  http://bullet-ws.dev.domain.com:4080 ,\n         helpLinks : [\n          {\n             name :  Custom Documentation ,\n             link :  http://data.docs.domain.com \n          }\n        ],\n         defaultValues  : {\n             durationMaxSecs : 300,\n             sketches : {\n                 countDistinctMaxEntries : 32768,\n                 distributionMaxNumberOfPoints : 50\n            }\n        },\n         defaultQuery :  http://bullet-ws.dev.domain.com:4080/custom-endpoint/api/defaultQuery \n    }\n}  Your UI on your CI environment will:   Talk using Websockets to  http://bullet-ws.dev.domain.com:4080/api/bullet/ws-query  for UI created Bullet queries  GET the schema from  http://bullet-ws.dev.domain.com:4080/api/bullet/columns  Validate queries in the BQL page with  http://bullet-ws.dev.domain.com:4080/api/bullet/validate-query  Populate an additional link on the Help drop-down pointing to  http://data.docs.domain.com  Allow queries to run as long as 300 seconds  Use 32768 in the help menu for the max number of unique elements that can be counted exactly  Allow only 50 points to be generated for Distribution queries  GET and cache a default query from  http://bullet-ws.dev.domain.com:4080/custom-endpoint/api/defaultQuery   You would make express use these settings by running  NODE_ENV=ci PORT=8800 node express-server.js", 
            "title": "Example"
        }, 
        {
            "location": "/ui/usage/", 
            "text": "Navigating the UI\n\n\nAll videos in this page are running on the topology set up in the \nQuick Start on Storm\n (producing \n200 data records every second.\n).\n\n\nClicking the \n icon will display useful information.\n\n\nA simple first query\n\n\nThe default new query will get a raw record with max result count 1 - it will return when a single record is found.\n\n\nResults\n\n\nSince there is no projection in this query, the results are shown as a JSON. You can click the Show as Table button to switch the mode.\n\n\nExample: Picking a random record from the stream\n\n\n\n\n\nFiltering and projecting data\n\n\nThe \nFilters\n section allows you to \npick a slice of data\n from the data stream.\n\n\nThe \nOutput Data\n section allows you to retrieve a subset of fields, and optionally rename them. You can also aggregate data, or choose to see raw data records.\n\n\nExample: Finding and picking out fields from events that have probability \n 0.5\n\n\n\n\n\nStream Raw Events\n\n\nA simple but useful query is a query with a filter and a \nSliding Window of size 1\n. This query will run for the extent of your duration and stream back events that match your filters as they arrive:\n\n\n\n\n\nBe careful\n when you use this query to ensure that your filter is sufficient to avoid returning TOO many results too fast. If this occurs Bullet will kill your query because of rate limiting (the default rate limit is 500 records per second).\n\n\nComplex Filtering\n\n\nThe querybuilder allows you create nested filters (basic relational filters, ANDs and ORs).\n\n\nThe querybuilder is also type aware: Numeric fields only allow numeric values, String fields only allow String operation, etc.\n\n\nExample: Finding and picking out the first and second events in each period that also have probability \n 0.5\n\n\n\n\n\n\n\nWhat's the .* next to a field?\n\n\nIf you have a map that is not enumerated (the keys are not known upfront), there will be \ntwo\n selections for the field in the dropdowns. If you want to apply operations to the nested keys, you can choose the field with the \n.*\n. This will display a free-form subfield selection input where you can specify the key. If you want to apply operations on the entire map, you will need to choose the field without the \n.*\n\n\n\n\nCount Distinct\n\n\nCount Distinct will count the number of distinct elements in a field exactly up to a threshold that is established when the backend is launched (16,384 in the example below).\n\n\nAfter this threshold the count will be approximate. As this example demonstrates, information about the precision of the count can be found in the Metadata:\n\n\n\n\n\n\n\nHow Can Bullet Count Distinct Elements So Fast??\n\n\nBullet uses \nData Sketches\n to preform the Count Distinct operation extremely quickly and using a configurable amount of memory. The size and precision of the Sketches used is configurable when the backend is launched. Data Sketches provide an estimate of computationally difficult measurements with provable error bounds. Information about the precision of the estimate (such as the Standard Deviations) is available in the Metadata.\n\n\n\n\nGroup all\n\n\nChoosing the Grouped Data option with no fields will result in the metrics being applied to all the data that matches your filters (or the whole set if you have no filters).\n\n\nExample: Counting, summing and averaging on the whole dataset\n\n\nThe metrics you apply on fields are all numeric presently. If you apply a metric on a non-numeric field, Bullet will try to \ntype-cast\n your field into number and if it's not possible, the result will be \nnull\n. The result will also be \nnull\n if the field was not present or no data matched your filters.\n\n\n\n\n\n\n\nAre Grouped Data metrics approximate?\n\n\nNo, the results are all exact. See below to see what is approximated when you have too many unique group combinations.\n\n\n\n\nGroup by\n\n\nYou can also choose Group fields and perform metrics per group.\n\n\nExample: Grouping by tuple_number\n\n\n\n\n\n\n\nWhat happens if I group by uuid?\n\n\nTry it out! If the number of unique group values exceeds the \nmaximum configured\n (we used 1024 for this example), you will receive a \nuniform sample\n across your unique group values. The results for your metrics however, are \nnot sampled\n. It is the groups that are sampled on. This means that is \nno\n guarantee of order if you were expecting the \nmost popular\n groups or similar. You should use the Top K query in that scenario.\n\n\n\n\n\n\nWhy no Count Distinct after Grouping\n\n\nAt this time, we do not support counting distinct values per field because with the current implementation of Grouping, it would involve storing DataSketches within DataSketches. We are considering this in a future release however.\n\n\n\n\n\n\nAha, sorting by tuple_number didn't sort properly!\n\n\nGood job, eagle eyes! Unfortunately, whenever we group on fields, those fields become strings under the current implementation. Rather than convert them back at the end, we have currently decided to leave it as is. This means that in your results, if you try and sort by a grouped field, it will perform a lexicographical sort even if it was originally a number.\n\n\nHowever, this also means that you can actually group by any field - including non primitives such as maps and lists! The field will be converted to a string and that string will be used as the field's representation for uniqueness and grouping purposes.\n\n\n\n\nDistributions\n\n\nIn this example, we find distributions of the \nduration\n  field. This field is generated randomly from 0 to 10,049, with a tendency to have values that are closer to 0. We should be able to see this using Bullet.\n\n\nThe \"distribution\" option allows you to pick a type of distribution:\n\n\n\n\nQuantiles\n lets you get various percentiles (e.g. 25th, 99th) of your numeric field\n\n\nFrequencies\n lets you break up the range of values of your field into intervals and get a count of how many values fell into each interval.\n\n\nCumulative Frequencies\n does the same as \nFrequencies\n but each interval includes the counts of all the intervals prior to it.\n\n\n\n\nBoth \nFrequencies\n and \nCumulative Frequencies\n also give you a probability of how likely a value is to fall into the interval.\n\n\nYou can read much more about this in the UI help by clicking the \nNeed more help?\n link.\n\n\nExact\n\n\nExample: Finding the various percentiles of duration\n\n\n\n\n\n\n\nExample: Finding some frequency counts of duration values in an interval\n\n\n\n\n\nTry it out youtself and see what \nCumulative Frequencies\n does!\n\n\nApproximate\n\n\nExample: Approximate quantile distribution\n\n\n\n\n\n\n\nNormalized Rank Error\n\n\nTo understand what this means, refer to the \nexplanation here\n. You can also refer to the help in the Result Metadata section.\n\n\n\n\nTop K\n\n\nTop K lets you get the most \nfrequent items\n or the \nheavy hitters\n for the values in a set of a fields.\n\n\nExact\n\n\nThis example gets the Top 3 most popular \ntype\n values (there are only 6 but this illustrates the idea).\n\n\n\n\n\nApproximate\n\n\nBy adding \nduration\n into the fields, the number of unique values for \n(type, duration)\n is increased. However, because \nduration\n has a tendency to have low values, we will have some \nfrequent items\n. The counts are now estimated.\n\n\n\n\n\n\n\nMaximum Count Error\n\n\nThe \nmaximum_count_error\n value for this query was \n3\n. This means that the difference between the upper bound and the lower bound of each count estimate is \n3\n. Bullet returns the upper bound as the estimate so subtracting \n3\n from each count gives you the lower bound of the count. Note that some counts are closer to each other than the count error. For instance, \n(quux, 1)\n and \n(bar, 1)\n have counts \n79\n and \n78\n but their true counts could be from \n76 to 79\n and \n75 to 78\n respectively. This means that \n(bar, 1)\n could well be the most frequent item for this query.\n\n\n\n\nCharting\n\n\nThis example shows how to get a basic chart in Bullet. The charting and pivoting modes are only enabled for queries that are \nnot\n Count Distinct or Group without Group Fields. This is because these results only have a single row and it does not make sense to graph them. They are enabled for all other queries.\n\n\nThe charting example below shows how to get a quick chart of a \nGroup\n query with 3 metrics.\n\n\n\n\n\nTumbling Windows\n\n\nTime-Based Tumbling Windows\n will return results every X seconds:\n\n\n\n\n\nThis example groups-by \"type\" and computes a couple metrics for each 2 second window.\n\n\nAdditive Tumbling Windows\n\n\nAdditive tumbling windows\n will also return results every X seconds, but the results will contain all the data collected since the beginning of the query:\n\n\n\n\n\nIn this example we compute bucket'ed frequency for the \"gaussian\" field. As the query runs you can see the gaussian curve form.\n\n\nPivoting\n\n\nIf the regular chart option is insufficient for your result (for instance, you have too many groups and metrics or you want to post-aggregate your results or remove outliers etc), then there is a advanced Pivot mode available when you are in the Chart option.\n\n\nThe Pivot option provides a drag-and-drop interface to drag fields to breakdown and aggregate by their values. Operations such as finding standard deviations, variance, etc are available as well as easily viewing them as tables and charts.\n\n\nThe following example shows a \nGroup\n query with multiple groups and metrics and some interactions with the Pivot table.\n\n\n\n\n\n\n\nRaw data does have a regular chart mode option\n\n\nThis is deliberate since the Chart option tries to infer your independent and dependent columns. When you fetch raw data, this is prone to errors so only the Pivot option is allowed.", 
            "title": "Usage"
        }, 
        {
            "location": "/ui/usage/#navigating-the-ui", 
            "text": "All videos in this page are running on the topology set up in the  Quick Start on Storm  (producing  200 data records every second. ).  Clicking the   icon will display useful information.", 
            "title": "Navigating the UI"
        }, 
        {
            "location": "/ui/usage/#a-simple-first-query", 
            "text": "The default new query will get a raw record with max result count 1 - it will return when a single record is found.", 
            "title": "A simple first query"
        }, 
        {
            "location": "/ui/usage/#results", 
            "text": "Since there is no projection in this query, the results are shown as a JSON. You can click the Show as Table button to switch the mode.  Example: Picking a random record from the stream", 
            "title": "Results"
        }, 
        {
            "location": "/ui/usage/#filtering-and-projecting-data", 
            "text": "The  Filters  section allows you to  pick a slice of data  from the data stream.  The  Output Data  section allows you to retrieve a subset of fields, and optionally rename them. You can also aggregate data, or choose to see raw data records.  Example: Finding and picking out fields from events that have probability   0.5", 
            "title": "Filtering and projecting data"
        }, 
        {
            "location": "/ui/usage/#stream-raw-events", 
            "text": "A simple but useful query is a query with a filter and a  Sliding Window of size 1 . This query will run for the extent of your duration and stream back events that match your filters as they arrive:   Be careful  when you use this query to ensure that your filter is sufficient to avoid returning TOO many results too fast. If this occurs Bullet will kill your query because of rate limiting (the default rate limit is 500 records per second).", 
            "title": "Stream Raw Events"
        }, 
        {
            "location": "/ui/usage/#complex-filtering", 
            "text": "The querybuilder allows you create nested filters (basic relational filters, ANDs and ORs).  The querybuilder is also type aware: Numeric fields only allow numeric values, String fields only allow String operation, etc.  Example: Finding and picking out the first and second events in each period that also have probability   0.5    What's the .* next to a field?  If you have a map that is not enumerated (the keys are not known upfront), there will be  two  selections for the field in the dropdowns. If you want to apply operations to the nested keys, you can choose the field with the  .* . This will display a free-form subfield selection input where you can specify the key. If you want to apply operations on the entire map, you will need to choose the field without the  .*", 
            "title": "Complex Filtering"
        }, 
        {
            "location": "/ui/usage/#count-distinct", 
            "text": "Count Distinct will count the number of distinct elements in a field exactly up to a threshold that is established when the backend is launched (16,384 in the example below).  After this threshold the count will be approximate. As this example demonstrates, information about the precision of the count can be found in the Metadata:    How Can Bullet Count Distinct Elements So Fast??  Bullet uses  Data Sketches  to preform the Count Distinct operation extremely quickly and using a configurable amount of memory. The size and precision of the Sketches used is configurable when the backend is launched. Data Sketches provide an estimate of computationally difficult measurements with provable error bounds. Information about the precision of the estimate (such as the Standard Deviations) is available in the Metadata.", 
            "title": "Count Distinct"
        }, 
        {
            "location": "/ui/usage/#group-all", 
            "text": "Choosing the Grouped Data option with no fields will result in the metrics being applied to all the data that matches your filters (or the whole set if you have no filters).  Example: Counting, summing and averaging on the whole dataset  The metrics you apply on fields are all numeric presently. If you apply a metric on a non-numeric field, Bullet will try to  type-cast  your field into number and if it's not possible, the result will be  null . The result will also be  null  if the field was not present or no data matched your filters.    Are Grouped Data metrics approximate?  No, the results are all exact. See below to see what is approximated when you have too many unique group combinations.", 
            "title": "Group all"
        }, 
        {
            "location": "/ui/usage/#group-by", 
            "text": "You can also choose Group fields and perform metrics per group.  Example: Grouping by tuple_number    What happens if I group by uuid?  Try it out! If the number of unique group values exceeds the  maximum configured  (we used 1024 for this example), you will receive a  uniform sample  across your unique group values. The results for your metrics however, are  not sampled . It is the groups that are sampled on. This means that is  no  guarantee of order if you were expecting the  most popular  groups or similar. You should use the Top K query in that scenario.    Why no Count Distinct after Grouping  At this time, we do not support counting distinct values per field because with the current implementation of Grouping, it would involve storing DataSketches within DataSketches. We are considering this in a future release however.    Aha, sorting by tuple_number didn't sort properly!  Good job, eagle eyes! Unfortunately, whenever we group on fields, those fields become strings under the current implementation. Rather than convert them back at the end, we have currently decided to leave it as is. This means that in your results, if you try and sort by a grouped field, it will perform a lexicographical sort even if it was originally a number.  However, this also means that you can actually group by any field - including non primitives such as maps and lists! The field will be converted to a string and that string will be used as the field's representation for uniqueness and grouping purposes.", 
            "title": "Group by"
        }, 
        {
            "location": "/ui/usage/#distributions", 
            "text": "In this example, we find distributions of the  duration   field. This field is generated randomly from 0 to 10,049, with a tendency to have values that are closer to 0. We should be able to see this using Bullet.  The \"distribution\" option allows you to pick a type of distribution:   Quantiles  lets you get various percentiles (e.g. 25th, 99th) of your numeric field  Frequencies  lets you break up the range of values of your field into intervals and get a count of how many values fell into each interval.  Cumulative Frequencies  does the same as  Frequencies  but each interval includes the counts of all the intervals prior to it.   Both  Frequencies  and  Cumulative Frequencies  also give you a probability of how likely a value is to fall into the interval.  You can read much more about this in the UI help by clicking the  Need more help?  link.", 
            "title": "Distributions"
        }, 
        {
            "location": "/ui/usage/#exact", 
            "text": "Example: Finding the various percentiles of duration    Example: Finding some frequency counts of duration values in an interval   Try it out youtself and see what  Cumulative Frequencies  does!", 
            "title": "Exact"
        }, 
        {
            "location": "/ui/usage/#approximate", 
            "text": "Example: Approximate quantile distribution    Normalized Rank Error  To understand what this means, refer to the  explanation here . You can also refer to the help in the Result Metadata section.", 
            "title": "Approximate"
        }, 
        {
            "location": "/ui/usage/#top-k", 
            "text": "Top K lets you get the most  frequent items  or the  heavy hitters  for the values in a set of a fields.", 
            "title": "Top K"
        }, 
        {
            "location": "/ui/usage/#exact_1", 
            "text": "This example gets the Top 3 most popular  type  values (there are only 6 but this illustrates the idea).", 
            "title": "Exact"
        }, 
        {
            "location": "/ui/usage/#approximate_1", 
            "text": "By adding  duration  into the fields, the number of unique values for  (type, duration)  is increased. However, because  duration  has a tendency to have low values, we will have some  frequent items . The counts are now estimated.    Maximum Count Error  The  maximum_count_error  value for this query was  3 . This means that the difference between the upper bound and the lower bound of each count estimate is  3 . Bullet returns the upper bound as the estimate so subtracting  3  from each count gives you the lower bound of the count. Note that some counts are closer to each other than the count error. For instance,  (quux, 1)  and  (bar, 1)  have counts  79  and  78  but their true counts could be from  76 to 79  and  75 to 78  respectively. This means that  (bar, 1)  could well be the most frequent item for this query.", 
            "title": "Approximate"
        }, 
        {
            "location": "/ui/usage/#charting", 
            "text": "This example shows how to get a basic chart in Bullet. The charting and pivoting modes are only enabled for queries that are  not  Count Distinct or Group without Group Fields. This is because these results only have a single row and it does not make sense to graph them. They are enabled for all other queries.  The charting example below shows how to get a quick chart of a  Group  query with 3 metrics.", 
            "title": "Charting"
        }, 
        {
            "location": "/ui/usage/#tumbling-windows", 
            "text": "Time-Based Tumbling Windows  will return results every X seconds:   This example groups-by \"type\" and computes a couple metrics for each 2 second window.", 
            "title": "Tumbling Windows"
        }, 
        {
            "location": "/ui/usage/#additive-tumbling-windows", 
            "text": "Additive tumbling windows  will also return results every X seconds, but the results will contain all the data collected since the beginning of the query:   In this example we compute bucket'ed frequency for the \"gaussian\" field. As the query runs you can see the gaussian curve form.", 
            "title": "Additive Tumbling Windows"
        }, 
        {
            "location": "/ui/usage/#pivoting", 
            "text": "If the regular chart option is insufficient for your result (for instance, you have too many groups and metrics or you want to post-aggregate your results or remove outliers etc), then there is a advanced Pivot mode available when you are in the Chart option.  The Pivot option provides a drag-and-drop interface to drag fields to breakdown and aggregate by their values. Operations such as finding standard deviations, variance, etc are available as well as easily viewing them as tables and charts.  The following example shows a  Group  query with multiple groups and metrics and some interactions with the Pivot table.    Raw data does have a regular chart mode option  This is deliberate since the Chart option tries to infer your independent and dependent columns. When you fetch raw data, this is prone to errors so only the Pivot option is allowed.", 
            "title": "Pivoting"
        }, 
        {
            "location": "/releases/", 
            "text": "Releases\n\n\nThis sections gathers all the relevant releases of the components of Bullet that we maintain in one place. It may not include the very few initial releases of these components if they were largely irrelevant. Full release notes can be found by clicking on the actual releases.\n\n\nBullet is still in active development. We welcome all contributions. Feel free to raise any issues/questions/bugs and whatever else on the relevant issues section for each component. Please include as many details as you can.\n\n\nAPI Documentation\n\n\nAPI (Java and Scala) docs can also be found for the releases below.\n\n\nDownload\n\n\nFor downloading any artifact listed below manually, you should preferably use the \nJCenter mirror here\n. For resolving artifacts in your build tool, follow the direcions in each of the components' Package Manager Setup sections.\n\n\n\n\nBullet Core\n\n\nThe core Bullet logic (a library) that can be used to implement Bullet on different Stream Processors (like Flink, Storm, Kafka Streams etc.). This core library can also be reused in other Bullet components that wish to depend on core Bullet concepts. This actually lived inside the \nBullet Storm\n package prior to version \n0.5.0\n. Starting with 0.5.0, Bullet Storm only includes the logic to implement Bullet on Storm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-core\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-core/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\nAPIDocs\n\n\n\n\n\n\n\n\n\n\n2021-01-04\n\n\n1.2.0\n\n\nStorage layer updates and extensions\n\n\nJavaDocs\n\n\n\n\n\n\n2020-10-30\n\n\n1.1.0\n\n\nTernary Logic, Bullet Record 1.1\n\n\nJavaDocs\n\n\n\n\n\n\n2020-10-02\n\n\n1.0.0\n\n\nMajor release - Expressions, Storage, Async queries, No JSON queries\n\n\nJavaDocs\n\n\n\n\n\n\n2019-02-01\n\n\n0.6.6\n\n\nQueryManager partition leak cleanup\n\n\nJavaDocs\n\n\n\n\n\n\n2018-12-20\n\n\n0.6.5\n\n\nQueryManager logging fixes\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-21\n\n\n0.6.4\n\n\nExtended field extraction in Projections\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-21\n\n\n0.6.3\n\n\nExtended field extraction Filters and Aggregations\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-19\n\n\n0.6.2\n\n\nQuery Manager helpers\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-16\n\n\n0.6.1\n\n\nQuery Categorizer category\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-06\n\n\n0.6.0\n\n\nQuery Partitioning, Validator and other improvements\n\n\nJavaDocs\n\n\n\n\n\n\n2018-10-21\n\n\n0.5.2\n\n\nAutoCloseable Pubsub Components, HttpClient 4.3.6\n\n\nJavaDocs\n\n\n\n\n\n\n2018-09-25\n\n\n0.5.1\n\n\nBetter Order By, Smaller Serializations, Transient Fields\n\n\nJavaDocs\n\n\n\n\n\n\n2018-09-14\n\n\n0.5.0\n\n\nPost Aggregations - ORDER BY, COMPUTATION, Casting in Filters\n\n\nJavaDocs\n\n\n\n\n\n\n2018-09-05\n\n\n0.4.3\n\n\nSliding Windows, SIZEIS, CONTAINSKEY, CONTAINSVALUE, filtering against other fields\n\n\nJavaDocs\n\n\n\n\n\n\n2018-06-26\n\n\n0.4.2\n\n\nFixes a bug with unclosed connections in the RESTPubSub\n\n\nJavaDocs\n\n\n\n\n\n\n2018-06-22\n\n\n0.4.1\n\n\nAdded RESTPublisher HTTP Timeout Setting\n\n\n\n\n\n\n\n\n2018-06-18\n\n\n0.4.0\n\n\nAdded support for Integer and Float data types, and configurable BulletRecordProvider class used to instantiate BulletRecords in bullet-core\n\n\n\n\n\n\n\n\n2018-04-11\n\n\n0.3.4\n\n\nPre-Start delaying and Buffering changes - queries are now buffered at the start of a query instead of start of each window\n\n\n\n\n\n\n\n\n2018-03-30\n\n\n0.3.3\n\n\nBug fix for com.yahoo.bullet.core.querying.Querier#isClosedForPartition\n\n\n\n\n\n\n\n\n2018-03-20\n\n\n0.3.2\n\n\nAdded headers to RESTPubSub http requests\n\n\n\n\n\n\n\n\n2018-03-16\n\n\n0.3.1\n\n\nAdded RESTPubSub implementation\n\n\n\n\n\n\n\n\n2018-02-22\n\n\n0.3.0\n\n\nSupports windowing / incremental updates\n\n\n\n\n\n\n\n\n2017-10-04\n\n\n0.2.5\n\n\nSupports an in-memory BufferingSubscriber implementation for reliable subscribing\n\n\n\n\n\n\n\n\n2017-10-03\n\n\n0.2.4\n\n\nHelpers added to Config, PubSubMessage, Metadata and JSONFormatter. FAIL signal in Metadata. PubSubMessage is JSON serializable\n\n\n\n\n\n\n\n\n2017-09-20\n\n\n0.2.3\n\n\nPubSub is no longer required to be Serializable. Makes PubSubMessage fully serializable. Utility classes and checked exceptions for PubSub\n\n\n\n\n\n\n\n\n2017-08-30\n\n\n0.2.2\n\n\nHelper methods to PubSubMessage and Config\n\n\n\n\n\n\n\n\n2017-08-23\n\n\n0.2.1\n\n\nRemoves PubSubConfig, adds defaults methods to Publisher/Subscriber interfaces and improves PubSubException\n\n\n\n\n\n\n\n\n2017-08-16\n\n\n0.2.0\n\n\nPubSub interfaces and classes to implement custom communication between API and backend\n\n\n\n\n\n\n\n\n2017-06-27\n\n\n0.1.2\n\n\nChanges to the BulletConfig interface previously used in Bullet Storm. Users now use BulletStormConfig instead but YAML config is the same\n\n\n\n\n\n\n\n\n2017-06-27\n\n\n0.1.1\n\n\nFirst stable release containing the core of Bullet as a library including parsing, implementing queries, creating results, DataSketches etc\n\n\n\n\n\n\n\n\n\n\nBullet Storm\n\n\nThe implementation of Bullet on Storm. Due to major API changes between Storm \n= 0.10 and Storm 1.0, Bullet Storm used to \nbuild two artifacts\n. The \nartifactId\n changes from \nbullet-storm\n (for 1.0+) to \nbullet-storm-0.10\n. All releases for both versions include migration and testing of the code on \nboth\n platforms. Feature parity depends on what was new in Storm 1.0. For example, the Resource Aware Scheduler or RAS, is only present in Storm 1.0+. So, bullet-storm-0.10 removes certain CPU and memory related settings specific to RAS in its configuration. There are also minor changes to the Metrics API in Storm. In terms of Bullet itself, there should be no differences.\n\n\n\n\nStorm DRPC PubSub \n\n\nThe DRPC PubSub is part of this artifact and is fully released and available for use starting with versions 0.6.2 and above. It is only meant to be used if you're using Storm as your Backend.\n\n\n\n\n\n\nStorm 0.10\n\n\nWe will no longer support Storm 0.10 since Storm 2.0 is now stable starting with Bullet on Storm 1.0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorm-1.0+ Repository\n\n\nhttps://github.com/bullet-db/bullet-storm\n\n\n\n\n\n\nStorm-0.10- Repository\n\n\nhttps://github.com/bullet-db/bullet-storm/tree/storm-0.10\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-storm/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nStorm 1.0+\n\n\nStorm 0.10\n\n\nHighlights\n\n\nAPIDocs\n\n\n\n\n\n\n\n\n\n\n2021-01-12\n\n\n1.0.0\n\n\n-\n\n\nBullet Core 1.1, Replay, Storage\n\n\nJavaDocs\n\n\n\n\n\n\n2019-02-07\n\n\n0.9.1\n\n\n0.9.1\n\n\nBullet DSL 0.1.2 and packaging fixes\n\n\nJavaDocs\n\n\n\n\n\n\n2019-02-07\n\n\n0.9.0\n\n\n0.9.0\n\n\nBullet DSL support!\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-26\n\n\n0.8.5\n\n\n0.8.5\n\n\nExtended field notation and updates bullet-core to 0.6.4\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-20\n\n\n0.8.4\n\n\n0.8.4\n\n\nPartitioning and updates bullet-core to 0.6.2\n\n\nJavaDocs\n\n\n\n\n\n\n2018-06-18\n\n\n0.8.3\n\n\n0.8.3\n\n\nUsing new bullet-record and bullet-core supporting Integer and Float data types\n\n\nJavaDocs\n\n\n\n\n\n\n2018-04-12\n\n\n0.8.2\n\n\n0.8.2\n\n\nDelaying query start in Join bolt\n\n\n\n\n\n\n\n\n2018-04-04\n\n\n0.8.1\n\n\n0.8.1\n\n\nFixed bug in Joinbolt\n\n\n\n\n\n\n\n\n2018-03-30\n\n\n0.8.0\n\n\n0.8.0\n\n\nSupports windowing / incremental updates\n\n\n\n\n\n\n\n\n2017-11-07\n\n\n0.7.0\n\n\n0.7.0\n\n\nMerge Query and Metadata Streams\n\n\n\n\n\n\n\n\n2017-10-24\n\n\n0.6.2\n\n\n0.6.2\n\n\nAdds a fat jar for using the DRPC PubSub in the Web Service\n\n\n\n\n\n\n\n\n2017-10-18\n\n\n0.6.1\n\n\n0.6.1\n\n\nDRPC PubSub\n\n\n\n\n\n\n\n\n2017-08-30\n\n\n0.6.0\n\n\n0.6.0\n\n\nNew PubSub architecture, removes DRPC components and settings\n\n\n\n\n\n\n\n\n2017-06-27\n\n\n0.5.0\n\n\n0.5.0\n\n\nPulled out Bullet Core. BulletConfig to BulletStormConfig\n\n\n\n\n\n\n\n\n2017-06-09\n\n\n0.4.3\n\n\n0.4.3\n\n\nAdding rounding for DISTRIBUTION. Latency metric\n\n\n\n\n\n\n\n\n2017-04-28\n\n\n0.4.2\n\n\n0.4.2\n\n\nStrict JSON output and fix for no data distributions\n\n\n\n\n\n\n\n\n2017-04-26\n\n\n0.4.1\n\n\n0.4.1\n\n\nResult Metadata Concept name mismatch fix\n\n\n\n\n\n\n\n\n2017-04-21\n\n\n0.4.0\n\n\n0.4.0\n\n\nDISTRIBUTION and TOP K release. Configuration renames.\n\n\n\n\n\n\n\n\n2017-03-13\n\n\n0.3.1\n\n\n0.3.1\n\n\nExtra records accepted after query expiry bug fix\n\n\n\n\n\n\n\n\n2017-02-27\n\n\n0.3.0\n\n\n0.3.0\n\n\nMetrics interface, config namespace, NPE bug fix\n\n\n\n\n\n\n\n\n2017-02-15\n\n\n0.2.1\n\n\n0.2.1\n\n\nAcking support, Max size and other bug fixes\n\n\n\n\n\n\n\n\n2017-01-26\n\n\n0.2.0\n\n\n0.2.0\n\n\nGROUP (DISTINCT, SUM, COUNT, MIN, MAX, AVG)\n\n\n\n\n\n\n\n\n2017-01-09\n\n\n0.1.0\n\n\n0.1.0\n\n\nCOUNT DISTINCT and micro-batching\n\n\n\n\n\n\n\n\n\n\nBullet Spark\n\n\nThe implementation of Bullet on Spark Streaming.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-spark\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-spark/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\nAPIDocs\n\n\n\n\n\n\n\n\n\n\n2021-02-12\n\n\n1.0.0\n\n\nBullet Core 1.2, DSL\n\n\nSparkDocs\n\n\n\n\n\n\n2019-02-07\n\n\n0.2.2\n\n\nFixes a NPE in JoinStreaming for very short queries\n\n\nSparkDocs\n\n\n\n\n\n\n2018-11-26\n\n\n0.2.1\n\n\nUses bullet-core 0.6.4 and supports extended field notation in queries\n\n\nSparkDocs\n\n\n\n\n\n\n2018-11-16\n\n\n0.2.0\n\n\nUses bullet-core 0.6.1 and adds partitioning support\n\n\nSparkDocs\n\n\n\n\n\n\n2018-06-18\n\n\n0.1.2\n\n\nUses SimpleBulletRecord to avoid some Spark serialization issues with Avro\n\n\nSparkDocs\n\n\n\n\n\n\n2018-06-08\n\n\n0.1.1\n\n\nAdds a command flag to pass custom setting file\n\n\n\n\n\n\n\n\n2018-05-25\n\n\n0.1.0\n\n\nThe first release\n\n\n\n\n\n\n\n\n\n\nBullet Web Service\n\n\nThe Web Service implementation that can serve a static schema from a file and talk to the backend using the PubSub.\n\n\n\n\nWAR to JAR\n\n\nStarting with 0.1.1 and above, this artifact no longer produces a WAR file that is meant to be run in a servlet container and instead switches to an executable Java application using Spring Boot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-service\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-service/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\nAPIDocs\n\n\n\n\n\n\n\n\n\n\n2021-01-12\n\n\n1.0.0\n\n\nAsync queries, Storage, Metrics, BQL only 1.0, Bullet Core 1.0\n\n\nJavaDocs\n\n\n\n\n\n\n2019-03-07\n\n\n0.5.0\n\n\nQueryManager API updates\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-28\n\n\n0.4.3\n\n\nUpdates bullet-bql to 0.2.1\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-26\n\n\n0.4.2\n\n\nBQL to JSON endpoint, dead backend reaper, new types in Schema, bullet-core 0.6.4\n\n\nJavaDocs\n\n\n\n\n\n\n2018-09-06\n\n\n0.4.1\n\n\nMax Queries limit and bullet-bql 0.1.2\n\n\nJavaDocs\n\n\n\n\n\n\n2018-07-17\n\n\n0.4.0\n\n\nEnhanced Web Service to support BQL queries\n\n\nJavaDocs\n\n\n\n\n\n\n2018-06-25\n\n\n0.3.0\n\n\nUpgrades to Netty-less Bullet Core for the RESTPubsub\n\n\n\n\n\n\n\n\n2018-06-14\n\n\n0.2.2\n\n\nAdding settings to configure Websocket\n\n\n\n\n\n\n\n\n2018-04-02\n\n\n0.2.1\n\n\nMoved and renamed settings\n\n\n\n\n\n\n\n\n2018-03-30\n\n\n0.2.0\n\n\nSupporting windowing / incremental updates\n\n\n\n\n\n\n\n\n2017-10-19\n\n\n0.1.1\n\n\nNew PubSub architecture. Switching to Spring Boot and executable JAR instead of WAR\n\n\n\n\n\n\n\n\n2016-12-16\n\n\n0.0.1\n\n\nThe first release with support for DRPC and the file-based schema\n\n\n\n\n\n\n\n\n\n\n\n\nWant to directly download jars?\n\n\nHead over to the JCenter download page to \ndirectly download all Bullet Storm, Core, Service, Record artifacts\n.\n\n\n\n\nBullet UI\n\n\nThe Bullet UI that lets you build, run, save and visualize results from Bullet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-ui\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-ui/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2021-02-18\n\n\n1.0.1\n\n\nStomp Websocket disconnect on query end\n\n\n\n\n\n\n2021-01-12\n\n\n1.0.0\n\n\nEmber 3 Octane, BQL support, new filter operators\n\n\n\n\n\n\n2019-03-18\n\n\n0.6.2\n\n\nLogo update\n\n\n\n\n\n\n2018-10-05\n\n\n0.6.1\n\n\nTimeseries Graphing, Bar, Pie Charts and FontAwesome\n\n\n\n\n\n\n2018-07-20\n\n\n0.6.0\n\n\nSupports adding a full default starting query\n\n\n\n\n\n\n2018-06-18\n\n\n0.5.0\n\n\nSupports windowing, uses IndexedDB and Ember 3!\n\n\n\n\n\n\n2017-08-22\n\n\n0.4.0\n\n\nQuery sharing, collapsible Raw view, and unsaved/error indicators. Settings rename and other bug fixes\n\n\n\n\n\n\n2017-05-22\n\n\n0.3.2\n\n\nExporting to TSV in Pivot table. Fixes unselectability bug in Raw view\n\n\n\n\n\n\n2017-05-15\n\n\n0.3.1\n\n\nAdds styles to the Pivot table. Fixes some minor UI interactions\n\n\n\n\n\n\n2017-05-10\n\n\n0.3.0\n\n\nAdds Charting and Pivoting support. Migrations enhanced. Support for overriding nested default settings\n\n\n\n\n\n\n2017-05-03\n\n\n0.2.2\n\n\nFixes maxlength of the input for points\n\n\n\n\n\n\n2017-05-02\n\n\n0.2.1\n\n\nFixes a bug with a dependency that broke sorting the Filters\n\n\n\n\n\n\n2017-05-01\n\n\n0.2.0\n\n\nRelease for Top K and Distribution. Supports Bullet Storm 0.4.2+\n\n\n\n\n\n\n2017-02-21\n\n\n0.1.0\n\n\nThe first release with support for all features included in Bullet Storm 0.2.1+\n\n\n\n\n\n\n\n\nBullet Record\n\n\nThe AVRO and other containers that you need to convert your data into to be consumed by Bullet. Also manages the typing in Bullet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-record\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-record/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\nAPIDocs\n\n\n\n\n\n\n\n\n\n\n2020-10-30\n\n\n1.1.0\n\n\nTernary logic\n\n\nJavaDocs\n\n\n\n\n\n\n2020-06-04\n\n\n1.0.0\n\n\nType System, Typed records, Schemas, extended Types\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-21\n\n\n0.3.0\n\n\nMore setters in BulletRecord including a forceSet\n\n\nJavaDocs\n\n\n\n\n\n\n2018-10-30\n\n\n0.2.2\n\n\nExtract from Lists and Map of Maps\n\n\nJavaDocs\n\n\n\n\n\n\n2018-08-14\n\n\n0.2.1\n\n\nSupports List of Primitive types\n\n\nJavaDocs\n\n\n\n\n\n\n2018-06-14\n\n\n0.2.0\n\n\nMakes BulletRecord pluggable, adds simple record and avro record implementations\n\n\nJavaDocs\n\n\n\n\n\n\n2017-05-19\n\n\n0.1.2\n\n\nReduces the memory footprint needed to serialize itself by a factor of 128 for small records\n\n\n\n\n\n\n\n\n2017-04-17\n\n\n0.1.1\n\n\nHelper methods to remove, rename, check presence and count fields in the Record\n\n\n\n\n\n\n\n\n2017-02-09\n\n\n0.1.0\n\n\nMap constructor\n\n\n\n\n\n\n\n\n\n\nBullet DSL\n\n\nA DSL to plug data sources into the Bullet Backend and Web Service.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-dsl\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-dsl/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\nAPIDocs\n\n\n\n\n\n\n\n\n\n\n2021-02-17\n\n\n1.1.0\n\n\nJSONBulletRecordConverter\n\n\nJavaDocs\n\n\n\n\n\n\n2021-02-11\n\n\n1.0.1\n\n\nBullet Core 1.2, Unsets default connector/converter\n\n\nJavaDocs\n\n\n\n\n\n\n2020-10-30\n\n\n1.0.0\n\n\nBullet Core 1.1, Types to match Bullet Record 1.1\n\n\nJavaDocs\n\n\n\n\n\n\n2019-02-07\n\n\n0.1.1\n\n\nInterface consolidation, IdentityDeserializer\n\n\nJavaDocs\n\n\n\n\n\n\n2019-02-05\n\n\n0.1.0\n\n\nBullet DSL, Fat jar, Interface refactors\n\n\nJavaDocs\n\n\n\n\n\n\n2019-01-08\n\n\n0.0.1\n\n\nFirst release\n\n\nJavaDocs\n\n\n\n\n\n\n\n\nBullet Kafka\n\n\nA PubSub implementation using Kafka as the backing PubSub. Can be used with any Bullet Backend.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-kafka\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-kafka/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\nAPIDocs\n\n\n\n\n\n\n\n\n\n\n2021-02-17\n\n\n1.0.1\n\n\nBullet Core 1.2\n\n\nJavaDocs\n\n\n\n\n\n\n2020-10-30\n\n\n1.0.0\n\n\nBullet Core 1.1\n\n\nJavaDocs\n\n\n\n\n\n\n2018-12-17\n\n\n0.3.3\n\n\nRemoves adding unnecessary properties to Producers/Consumers\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-26\n\n\n0.3.2\n\n\nUses bullet-core-0.6.4\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-26\n\n\n0.3.1\n\n\nUses bullet-core-0.6.0 and adds Validator\n\n\nJavaDocs\n\n\n\n\n\n\n2018-02-27\n\n\n0.3.0\n\n\nUses bullet-core-0.3.0 - windows / incremental updates\n\n\nJavaDocs\n\n\n\n\n\n\n2017-10-19\n\n\n0.2.0\n\n\nRefactors and re-releases. Pass-through settings to Kafka. Manual offset committing bug fix\n\n\n\n\n\n\n\n\n2017-09-27\n\n\n0.1.2\n\n\nFixes a bug with config loading\n\n\n\n\n\n\n\n\n2017-09-22\n\n\n0.1.1\n\n\nFirst release using the PubSub interfaces\n\n\n\n\n\n\n\n\n\n\nBullet Pulsar\n\n\nA PubSub implementation using Pulsar as the backing PubSub. Can be used with any Bullet Backend.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-pulsar\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-pulsar/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\nAPIDocs\n\n\n\n\n\n\n\n\n\n\n2020-10-30\n\n\n1.0.0\n\n\nBullet Core 1.1\n\n\nJavaDocs\n\n\n\n\n\n\n2018-12-10\n\n\n0.1.0\n\n\nFirst release using the PubSub interfaces\n\n\nJavaDocs\n\n\n\n\n\n\n\n\nBullet BQL\n\n\nA library facilitating the conversion from Bullet BQL queries to Bullet queries. This is the interface to the API.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-bql\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-bql/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\nAPIDocs\n\n\n\n\n\n\n\n\n\n\n2021-01-04\n\n\n1.1.0\n\n\nUpdates Bullet Core to 1.2.0\n\n\nJavaDocs\n\n\n\n\n\n\n2021-01-04\n\n\n1.0.0\n\n\nExpressions, Schema integration, native queries instead of JSON\n\n\nJavaDocs\n\n\n\n\n\n\n2018-11-28\n\n\n0.2.1\n\n\nExtended field access notation\n\n\nJavaDocs\n\n\n\n\n\n\n2018-09-28\n\n\n0.2.0\n\n\nAdds Post Aggregations and uses bullet-core-0.5.1\n\n\nJavaDocs\n\n\n\n\n\n\n2018-09-06\n\n\n0.1.2\n\n\nSupports CONTAINSKEY, CONTAINSVALUE, SIZEOF, comparing to other fields. Fixes some bugs\n\n\nJavaDocs\n\n\n\n\n\n\n2018-07-17\n\n\n0.1.1\n\n\nStops publishing fat jar and marks slf4j dependency provided\n\n\nJavaDocs\n\n\n\n\n\n\n2018-07-05\n\n\n0.1.0\n\n\nFirst release", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#releases", 
            "text": "This sections gathers all the relevant releases of the components of Bullet that we maintain in one place. It may not include the very few initial releases of these components if they were largely irrelevant. Full release notes can be found by clicking on the actual releases.  Bullet is still in active development. We welcome all contributions. Feel free to raise any issues/questions/bugs and whatever else on the relevant issues section for each component. Please include as many details as you can.", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#api-documentation", 
            "text": "API (Java and Scala) docs can also be found for the releases below.", 
            "title": "API Documentation"
        }, 
        {
            "location": "/releases/#download", 
            "text": "For downloading any artifact listed below manually, you should preferably use the  JCenter mirror here . For resolving artifacts in your build tool, follow the direcions in each of the components' Package Manager Setup sections.", 
            "title": "Download"
        }, 
        {
            "location": "/releases/#bullet-core", 
            "text": "The core Bullet logic (a library) that can be used to implement Bullet on different Stream Processors (like Flink, Storm, Kafka Streams etc.). This core library can also be reused in other Bullet components that wish to depend on core Bullet concepts. This actually lived inside the  Bullet Storm  package prior to version  0.5.0 . Starting with 0.5.0, Bullet Storm only includes the logic to implement Bullet on Storm.           Repository  https://github.com/bullet-db/bullet-core    Issues  https://github.com/bullet-db/bullet-core/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Core"
        }, 
        {
            "location": "/releases/#releases_1", 
            "text": "Date  Release  Highlights  APIDocs      2021-01-04  1.2.0  Storage layer updates and extensions  JavaDocs    2020-10-30  1.1.0  Ternary Logic, Bullet Record 1.1  JavaDocs    2020-10-02  1.0.0  Major release - Expressions, Storage, Async queries, No JSON queries  JavaDocs    2019-02-01  0.6.6  QueryManager partition leak cleanup  JavaDocs    2018-12-20  0.6.5  QueryManager logging fixes  JavaDocs    2018-11-21  0.6.4  Extended field extraction in Projections  JavaDocs    2018-11-21  0.6.3  Extended field extraction Filters and Aggregations  JavaDocs    2018-11-19  0.6.2  Query Manager helpers  JavaDocs    2018-11-16  0.6.1  Query Categorizer category  JavaDocs    2018-11-06  0.6.0  Query Partitioning, Validator and other improvements  JavaDocs    2018-10-21  0.5.2  AutoCloseable Pubsub Components, HttpClient 4.3.6  JavaDocs    2018-09-25  0.5.1  Better Order By, Smaller Serializations, Transient Fields  JavaDocs    2018-09-14  0.5.0  Post Aggregations - ORDER BY, COMPUTATION, Casting in Filters  JavaDocs    2018-09-05  0.4.3  Sliding Windows, SIZEIS, CONTAINSKEY, CONTAINSVALUE, filtering against other fields  JavaDocs    2018-06-26  0.4.2  Fixes a bug with unclosed connections in the RESTPubSub  JavaDocs    2018-06-22  0.4.1  Added RESTPublisher HTTP Timeout Setting     2018-06-18  0.4.0  Added support for Integer and Float data types, and configurable BulletRecordProvider class used to instantiate BulletRecords in bullet-core     2018-04-11  0.3.4  Pre-Start delaying and Buffering changes - queries are now buffered at the start of a query instead of start of each window     2018-03-30  0.3.3  Bug fix for com.yahoo.bullet.core.querying.Querier#isClosedForPartition     2018-03-20  0.3.2  Added headers to RESTPubSub http requests     2018-03-16  0.3.1  Added RESTPubSub implementation     2018-02-22  0.3.0  Supports windowing / incremental updates     2017-10-04  0.2.5  Supports an in-memory BufferingSubscriber implementation for reliable subscribing     2017-10-03  0.2.4  Helpers added to Config, PubSubMessage, Metadata and JSONFormatter. FAIL signal in Metadata. PubSubMessage is JSON serializable     2017-09-20  0.2.3  PubSub is no longer required to be Serializable. Makes PubSubMessage fully serializable. Utility classes and checked exceptions for PubSub     2017-08-30  0.2.2  Helper methods to PubSubMessage and Config     2017-08-23  0.2.1  Removes PubSubConfig, adds defaults methods to Publisher/Subscriber interfaces and improves PubSubException     2017-08-16  0.2.0  PubSub interfaces and classes to implement custom communication between API and backend     2017-06-27  0.1.2  Changes to the BulletConfig interface previously used in Bullet Storm. Users now use BulletStormConfig instead but YAML config is the same     2017-06-27  0.1.1  First stable release containing the core of Bullet as a library including parsing, implementing queries, creating results, DataSketches etc", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-storm", 
            "text": "The implementation of Bullet on Storm. Due to major API changes between Storm  = 0.10 and Storm 1.0, Bullet Storm used to  build two artifacts . The  artifactId  changes from  bullet-storm  (for 1.0+) to  bullet-storm-0.10 . All releases for both versions include migration and testing of the code on  both  platforms. Feature parity depends on what was new in Storm 1.0. For example, the Resource Aware Scheduler or RAS, is only present in Storm 1.0+. So, bullet-storm-0.10 removes certain CPU and memory related settings specific to RAS in its configuration. There are also minor changes to the Metrics API in Storm. In terms of Bullet itself, there should be no differences.   Storm DRPC PubSub   The DRPC PubSub is part of this artifact and is fully released and available for use starting with versions 0.6.2 and above. It is only meant to be used if you're using Storm as your Backend.    Storm 0.10  We will no longer support Storm 0.10 since Storm 2.0 is now stable starting with Bullet on Storm 1.0.            Storm-1.0+ Repository  https://github.com/bullet-db/bullet-storm    Storm-0.10- Repository  https://github.com/bullet-db/bullet-storm/tree/storm-0.10    Issues  https://github.com/bullet-db/bullet-storm/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Storm"
        }, 
        {
            "location": "/releases/#releases_2", 
            "text": "Date  Storm 1.0+  Storm 0.10  Highlights  APIDocs      2021-01-12  1.0.0  -  Bullet Core 1.1, Replay, Storage  JavaDocs    2019-02-07  0.9.1  0.9.1  Bullet DSL 0.1.2 and packaging fixes  JavaDocs    2019-02-07  0.9.0  0.9.0  Bullet DSL support!  JavaDocs    2018-11-26  0.8.5  0.8.5  Extended field notation and updates bullet-core to 0.6.4  JavaDocs    2018-11-20  0.8.4  0.8.4  Partitioning and updates bullet-core to 0.6.2  JavaDocs    2018-06-18  0.8.3  0.8.3  Using new bullet-record and bullet-core supporting Integer and Float data types  JavaDocs    2018-04-12  0.8.2  0.8.2  Delaying query start in Join bolt     2018-04-04  0.8.1  0.8.1  Fixed bug in Joinbolt     2018-03-30  0.8.0  0.8.0  Supports windowing / incremental updates     2017-11-07  0.7.0  0.7.0  Merge Query and Metadata Streams     2017-10-24  0.6.2  0.6.2  Adds a fat jar for using the DRPC PubSub in the Web Service     2017-10-18  0.6.1  0.6.1  DRPC PubSub     2017-08-30  0.6.0  0.6.0  New PubSub architecture, removes DRPC components and settings     2017-06-27  0.5.0  0.5.0  Pulled out Bullet Core. BulletConfig to BulletStormConfig     2017-06-09  0.4.3  0.4.3  Adding rounding for DISTRIBUTION. Latency metric     2017-04-28  0.4.2  0.4.2  Strict JSON output and fix for no data distributions     2017-04-26  0.4.1  0.4.1  Result Metadata Concept name mismatch fix     2017-04-21  0.4.0  0.4.0  DISTRIBUTION and TOP K release. Configuration renames.     2017-03-13  0.3.1  0.3.1  Extra records accepted after query expiry bug fix     2017-02-27  0.3.0  0.3.0  Metrics interface, config namespace, NPE bug fix     2017-02-15  0.2.1  0.2.1  Acking support, Max size and other bug fixes     2017-01-26  0.2.0  0.2.0  GROUP (DISTINCT, SUM, COUNT, MIN, MAX, AVG)     2017-01-09  0.1.0  0.1.0  COUNT DISTINCT and micro-batching", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-spark", 
            "text": "The implementation of Bullet on Spark Streaming.           Repository  https://github.com/bullet-db/bullet-spark    Issues  https://github.com/bullet-db/bullet-spark/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Spark"
        }, 
        {
            "location": "/releases/#releases_3", 
            "text": "Date  Release  Highlights  APIDocs      2021-02-12  1.0.0  Bullet Core 1.2, DSL  SparkDocs    2019-02-07  0.2.2  Fixes a NPE in JoinStreaming for very short queries  SparkDocs    2018-11-26  0.2.1  Uses bullet-core 0.6.4 and supports extended field notation in queries  SparkDocs    2018-11-16  0.2.0  Uses bullet-core 0.6.1 and adds partitioning support  SparkDocs    2018-06-18  0.1.2  Uses SimpleBulletRecord to avoid some Spark serialization issues with Avro  SparkDocs    2018-06-08  0.1.1  Adds a command flag to pass custom setting file     2018-05-25  0.1.0  The first release", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-web-service", 
            "text": "The Web Service implementation that can serve a static schema from a file and talk to the backend using the PubSub.   WAR to JAR  Starting with 0.1.1 and above, this artifact no longer produces a WAR file that is meant to be run in a servlet container and instead switches to an executable Java application using Spring Boot.            Repository  https://github.com/bullet-db/bullet-service    Issues  https://github.com/bullet-db/bullet-service/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Web Service"
        }, 
        {
            "location": "/releases/#releases_4", 
            "text": "Date  Release  Highlights  APIDocs      2021-01-12  1.0.0  Async queries, Storage, Metrics, BQL only 1.0, Bullet Core 1.0  JavaDocs    2019-03-07  0.5.0  QueryManager API updates  JavaDocs    2018-11-28  0.4.3  Updates bullet-bql to 0.2.1  JavaDocs    2018-11-26  0.4.2  BQL to JSON endpoint, dead backend reaper, new types in Schema, bullet-core 0.6.4  JavaDocs    2018-09-06  0.4.1  Max Queries limit and bullet-bql 0.1.2  JavaDocs    2018-07-17  0.4.0  Enhanced Web Service to support BQL queries  JavaDocs    2018-06-25  0.3.0  Upgrades to Netty-less Bullet Core for the RESTPubsub     2018-06-14  0.2.2  Adding settings to configure Websocket     2018-04-02  0.2.1  Moved and renamed settings     2018-03-30  0.2.0  Supporting windowing / incremental updates     2017-10-19  0.1.1  New PubSub architecture. Switching to Spring Boot and executable JAR instead of WAR     2016-12-16  0.0.1  The first release with support for DRPC and the file-based schema       Want to directly download jars?  Head over to the JCenter download page to  directly download all Bullet Storm, Core, Service, Record artifacts .", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-ui", 
            "text": "The Bullet UI that lets you build, run, save and visualize results from Bullet.           Repository  https://github.com/bullet-db/bullet-ui    Issues  https://github.com/bullet-db/bullet-ui/issues    Last Tag     Latest Artifact", 
            "title": "Bullet UI"
        }, 
        {
            "location": "/releases/#releases_5", 
            "text": "Date  Release  Highlights      2021-02-18  1.0.1  Stomp Websocket disconnect on query end    2021-01-12  1.0.0  Ember 3 Octane, BQL support, new filter operators    2019-03-18  0.6.2  Logo update    2018-10-05  0.6.1  Timeseries Graphing, Bar, Pie Charts and FontAwesome    2018-07-20  0.6.0  Supports adding a full default starting query    2018-06-18  0.5.0  Supports windowing, uses IndexedDB and Ember 3!    2017-08-22  0.4.0  Query sharing, collapsible Raw view, and unsaved/error indicators. Settings rename and other bug fixes    2017-05-22  0.3.2  Exporting to TSV in Pivot table. Fixes unselectability bug in Raw view    2017-05-15  0.3.1  Adds styles to the Pivot table. Fixes some minor UI interactions    2017-05-10  0.3.0  Adds Charting and Pivoting support. Migrations enhanced. Support for overriding nested default settings    2017-05-03  0.2.2  Fixes maxlength of the input for points    2017-05-02  0.2.1  Fixes a bug with a dependency that broke sorting the Filters    2017-05-01  0.2.0  Release for Top K and Distribution. Supports Bullet Storm 0.4.2+    2017-02-21  0.1.0  The first release with support for all features included in Bullet Storm 0.2.1+", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-record", 
            "text": "The AVRO and other containers that you need to convert your data into to be consumed by Bullet. Also manages the typing in Bullet.           Repository  https://github.com/bullet-db/bullet-record    Issues  https://github.com/bullet-db/bullet-record/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Record"
        }, 
        {
            "location": "/releases/#releases_6", 
            "text": "Date  Release  Highlights  APIDocs      2020-10-30  1.1.0  Ternary logic  JavaDocs    2020-06-04  1.0.0  Type System, Typed records, Schemas, extended Types  JavaDocs    2018-11-21  0.3.0  More setters in BulletRecord including a forceSet  JavaDocs    2018-10-30  0.2.2  Extract from Lists and Map of Maps  JavaDocs    2018-08-14  0.2.1  Supports List of Primitive types  JavaDocs    2018-06-14  0.2.0  Makes BulletRecord pluggable, adds simple record and avro record implementations  JavaDocs    2017-05-19  0.1.2  Reduces the memory footprint needed to serialize itself by a factor of 128 for small records     2017-04-17  0.1.1  Helper methods to remove, rename, check presence and count fields in the Record     2017-02-09  0.1.0  Map constructor", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-dsl", 
            "text": "A DSL to plug data sources into the Bullet Backend and Web Service.           Repository  https://github.com/bullet-db/bullet-dsl    Issues  https://github.com/bullet-db/bullet-dsl/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet DSL"
        }, 
        {
            "location": "/releases/#releases_7", 
            "text": "Date  Release  Highlights  APIDocs      2021-02-17  1.1.0  JSONBulletRecordConverter  JavaDocs    2021-02-11  1.0.1  Bullet Core 1.2, Unsets default connector/converter  JavaDocs    2020-10-30  1.0.0  Bullet Core 1.1, Types to match Bullet Record 1.1  JavaDocs    2019-02-07  0.1.1  Interface consolidation, IdentityDeserializer  JavaDocs    2019-02-05  0.1.0  Bullet DSL, Fat jar, Interface refactors  JavaDocs    2019-01-08  0.0.1  First release  JavaDocs", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-kafka", 
            "text": "A PubSub implementation using Kafka as the backing PubSub. Can be used with any Bullet Backend.           Repository  https://github.com/bullet-db/bullet-kafka    Issues  https://github.com/bullet-db/bullet-kafka/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Kafka"
        }, 
        {
            "location": "/releases/#releases_8", 
            "text": "Date  Release  Highlights  APIDocs      2021-02-17  1.0.1  Bullet Core 1.2  JavaDocs    2020-10-30  1.0.0  Bullet Core 1.1  JavaDocs    2018-12-17  0.3.3  Removes adding unnecessary properties to Producers/Consumers  JavaDocs    2018-11-26  0.3.2  Uses bullet-core-0.6.4  JavaDocs    2018-11-26  0.3.1  Uses bullet-core-0.6.0 and adds Validator  JavaDocs    2018-02-27  0.3.0  Uses bullet-core-0.3.0 - windows / incremental updates  JavaDocs    2017-10-19  0.2.0  Refactors and re-releases. Pass-through settings to Kafka. Manual offset committing bug fix     2017-09-27  0.1.2  Fixes a bug with config loading     2017-09-22  0.1.1  First release using the PubSub interfaces", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-pulsar", 
            "text": "A PubSub implementation using Pulsar as the backing PubSub. Can be used with any Bullet Backend.           Repository  https://github.com/bullet-db/bullet-pulsar    Issues  https://github.com/bullet-db/bullet-pulsar/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Pulsar"
        }, 
        {
            "location": "/releases/#releases_9", 
            "text": "Date  Release  Highlights  APIDocs      2020-10-30  1.0.0  Bullet Core 1.1  JavaDocs    2018-12-10  0.1.0  First release using the PubSub interfaces  JavaDocs", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-bql", 
            "text": "A library facilitating the conversion from Bullet BQL queries to Bullet queries. This is the interface to the API.           Repository  https://github.com/bullet-db/bullet-bql    Issues  https://github.com/bullet-db/bullet-bql/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet BQL"
        }, 
        {
            "location": "/releases/#releases_10", 
            "text": "Date  Release  Highlights  APIDocs      2021-01-04  1.1.0  Updates Bullet Core to 1.2.0  JavaDocs    2021-01-04  1.0.0  Expressions, Schema integration, native queries instead of JSON  JavaDocs    2018-11-28  0.2.1  Extended field access notation  JavaDocs    2018-09-28  0.2.0  Adds Post Aggregations and uses bullet-core-0.5.1  JavaDocs    2018-09-06  0.1.2  Supports CONTAINSKEY, CONTAINSVALUE, SIZEOF, comparing to other fields. Fixes some bugs  JavaDocs    2018-07-17  0.1.1  Stops publishing fat jar and marks slf4j dependency provided  JavaDocs    2018-07-05  0.1.0  First release", 
            "title": "Releases"
        }, 
        {
            "location": "/about/contributing/", 
            "text": "Contributing\n\n\nWe welcome all contributions! We also welcome all usage experiences, stories, annoyances and whatever else you want to say. Head on over to our \nContact Us page\n and let us know!\n\n\nContributor License Agreement (CLA)\n\n\nBullet is hosted under the \nBullet Github Organization\n, a subsidiary of the \nYahoo Github Organization\n. In order to contribute to any Yahoo project, you will need to submit a CLA. When you submit a Pull Request to any Bullet repository, a CLABot will ask  you to sign the CLA if you haven't signed one already. Read the \nhuman-readable summary\n of the CLA.\n\n\nFuture plans\n\n\nOur issues are tracked through GitHub on the appropriate repo issues pages. You are welcome to take a look and contribute. Also, feel free to \ncontact us\n with any ideas/suggestions/PRs for features mentioned here or anything else you think about!\n\n\nYou can get a list of all the \nopen issues here\n.", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#contributing", 
            "text": "We welcome all contributions! We also welcome all usage experiences, stories, annoyances and whatever else you want to say. Head on over to our  Contact Us page  and let us know!", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#contributor-license-agreement-cla", 
            "text": "Bullet is hosted under the  Bullet Github Organization , a subsidiary of the  Yahoo Github Organization . In order to contribute to any Yahoo project, you will need to submit a CLA. When you submit a Pull Request to any Bullet repository, a CLABot will ask  you to sign the CLA if you haven't signed one already. Read the  human-readable summary  of the CLA.", 
            "title": "Contributor License Agreement (CLA)"
        }, 
        {
            "location": "/about/contributing/#future-plans", 
            "text": "Our issues are tracked through GitHub on the appropriate repo issues pages. You are welcome to take a look and contribute. Also, feel free to  contact us  with any ideas/suggestions/PRs for features mentioned here or anything else you think about!  You can get a list of all the  open issues here .", 
            "title": "Future plans"
        }, 
        {
            "location": "/about/contact/", 
            "text": "Contact Us\n\n\nIssues\n\n\nIf you have any issues with any of the particular Bullet sub-components, feel free to create issues.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorm\n\n\nhttps://github.com/bullet-db/bullet-storm/issues\n\n\n\n\n\n\nSpark\n\n\nhttps://github.com/bullet-db/bullet-spark/issues\n\n\n\n\n\n\nWeb Service\n\n\nhttps://github.com/bullet-db/bullet-service/issues\n\n\n\n\n\n\nUI\n\n\nhttps://github.com/bullet-db/bullet-ui/issues\n\n\n\n\n\n\nRecord\n\n\nhttps://github.com/bullet-db/bullet-record/issues\n\n\n\n\n\n\nCore\n\n\nhttps://github.com/bullet-db/bullet-core/issues\n\n\n\n\n\n\nDSL\n\n\nhttps://github.com/bullet-db/bullet-dsl/issues\n\n\n\n\n\n\nBQL\n\n\nhttps://github.com/bullet-db/bullet-bql/issues\n\n\n\n\n\n\nKafka PubSub\n\n\nhttps://github.com/bullet-db/bullet-kafka/issues\n\n\n\n\n\n\nPulsar PubSub\n\n\nhttps://github.com/bullet-db/bullet-pulsar/issues\n\n\n\n\n\n\nDocumentation\n\n\nhttps://github.com/bullet-db/bullet-db.github.io/issues\n\n\n\n\n\n\n\n\nMailing Lists\n\n\nIf you have a general question, comment, or observation meant for general visibility, reach out to the Users list. If you want to keep it to just the developers, reach to that list instead.\n\n\n\n\n\n\n\n\n\n\nMail\n\n\nForum\n\n\n\n\n\n\n\n\n\n\nUsers\n\n\nbullet-users@googlegroups.com\n\n\nhttps://groups.google.com/d/forum/bullet-users\n\n\n\n\n\n\nDevelopers\n\n\nbullet-dev@googlegroups.com\n\n\nhttps://groups.google.com/d/forum/bullet-dev", 
            "title": "Contact Us"
        }, 
        {
            "location": "/about/contact/#contact-us", 
            "text": "", 
            "title": "Contact Us"
        }, 
        {
            "location": "/about/contact/#issues", 
            "text": "If you have any issues with any of the particular Bullet sub-components, feel free to create issues.           Storm  https://github.com/bullet-db/bullet-storm/issues    Spark  https://github.com/bullet-db/bullet-spark/issues    Web Service  https://github.com/bullet-db/bullet-service/issues    UI  https://github.com/bullet-db/bullet-ui/issues    Record  https://github.com/bullet-db/bullet-record/issues    Core  https://github.com/bullet-db/bullet-core/issues    DSL  https://github.com/bullet-db/bullet-dsl/issues    BQL  https://github.com/bullet-db/bullet-bql/issues    Kafka PubSub  https://github.com/bullet-db/bullet-kafka/issues    Pulsar PubSub  https://github.com/bullet-db/bullet-pulsar/issues    Documentation  https://github.com/bullet-db/bullet-db.github.io/issues", 
            "title": "Issues"
        }, 
        {
            "location": "/about/contact/#mailing-lists", 
            "text": "If you have a general question, comment, or observation meant for general visibility, reach out to the Users list. If you want to keep it to just the developers, reach to that list instead.      Mail  Forum      Users  bullet-users@googlegroups.com  https://groups.google.com/d/forum/bullet-users    Developers  bullet-dev@googlegroups.com  https://groups.google.com/d/forum/bullet-dev", 
            "title": "Mailing Lists"
        }
    ]
}