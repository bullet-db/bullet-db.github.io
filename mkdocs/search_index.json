{
    "docs": [
        {
            "location": "/", 
            "text": "A real-time query engine for very large data streams\n\n\n\n\n\n\nNO persistence layer\n\n\n\n\n\n\nLight-weight, cheap and fast\n\n\n\n\n\n\nMulti-tenant\n\n\n\n\n\n\nPluggable to any data source\n\n\n\n\n\n\nProvides a UI and Web Service\n\n\n\n\n\n\nFilter raw data or aggregate data\n\n\n\n\n\n\nCan be run on storm or spark streaming\n\n\n\n\n\n\nA look-forward query system - operates on data that arrive after the query is submitted\n\n\n\n\n\n\nBig-data scale-tested - used in production at Yahoo and tested running 500+ queries simultaneously on up to 2,000,000 rps\n\n\n\n\n\n\nHow is Bullet useful\n\n\nHow Bullet is used is largely determined by the data source it consumes. Depending on what kind of data you put Bullet on, the types of queries you run on it and your use-cases will change. As a look-forward query system with no persistence, you will not be able to repeat your queries on the same data. The next time you run your query, it will operate on the different data that arrives after that submission. If this usage pattern is what you need and you are looking for a light-weight system that can tap into your streaming data, then Bullet is for you!\n\n\nExample: How Bullet is used at Yahoo\n\n\nBullet is used in production internally at Yahoo by having it sit on a subset of raw user engagement events from Yahoo sites and apps. This lets Yahoo developers automatically validate their instrumentation code \nend-to-end\n in their Continuous Delivery pipelines. Validating instrumentation is critical since it powers pretty much all decisions and products including machine learning, corporate KPIs, analytics, personalization, targeting.\n\n\nThis instance of Bullet also powers other use-cases such as letting analysts validate assumptions about data, product managers verify launches instantly, debug issues and outages, or simply explore and play around with the data.\n\n\n\n\nBlog post\n\n\nHere is a \nlink to our blog post\n condensing most of this information if you want to take a look.\n\n\n\n\n\n\nQuick Start\n\n\nSee \nQuick Start\n to set up Bullet locally using Spark Streaming. You will generate some synthetic streaming data that you can then query with Bullet.\n\n\nSetup Bullet on your streaming data\n\n\nTo set up Bullet on a real data stream, you need:\n\n\n\n\nTo setup the Bullet Backend on a stream processing framework. Currently, we support \nBullet on Storm\n and \nBullet on Spark\n.\n\n\nPlug in your source of data. See \nGetting your data into Bullet\n for details\n\n\nConsume your data stream\n\n\n\n\n\n\nThe \nWeb Service\n set up to convey queries and return results back from the backend\n\n\nTo choose a \nPubSub implementation\n that connects the Web Service and the Backend. We currently support \nKafka\n and a \nREST PubSub\n on any Backend and \nStorm DRPC\n for the Storm Backend.\n\n\nThe optional \nUI\n set up to talk to your Web Service. You can skip the UI if all your access is programmatic\n\n\n\n\n\n\nSchema in the UI\n\n\nThe UI also needs an endpoint that provides your data schema to help with query building. The Web Service you set up provides a simple file based schema endpoint that you can point the UI to if that is sufficient for your needs.\n\n\n\n\n\n\nQuerying in Bullet\n\n\nBullet queries allow you to filter, project and aggregate data. You can also specify a window to get incremental results. Bullet lets you fetch raw (the individual data records) as well as aggregated data.\n\n\n\n\n\n\nSee the \nUI Usage section\n for using the UI to build Bullet queries. This is the same UI you will build in the Quick Starts.\n\n\n\n\n\n\nSee the API section (\nJSON\n, \nBQL\n) for building Bullet API queries\n\n\n\n\n\n\nFor examples using the API, see \nExamples\n. These are actual albeit cleansed queries sourced from the instance at Yahoo.\n\n\n\n\n\n\nTermination conditions\n\n\nA Bullet query terminates and returns whatever has been collected so far when:\n\n\n\n\nA maximum duration is reached. In other words, a query runs for a defined time window.\n\n\nA maximum number of records is reached (only applicable for queries that are fetching raw data records and not aggregating).\n\n\n\n\nFilters\n\n\nBullet supports two kinds of filters:\n\n\n\n\n\n\n\n\nFilter Type\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nLogical filter\n\n\nAllow you to combine filter clauses (Logical or Relational) with logical operations like AND, OR and NOTs\n\n\n\n\n\n\nRelational filters\n\n\nAllow you to use comparison operations like equals, not equals, greater than, less than, regex like etc, on fields\n\n\n\n\n\n\n\n\nProjections\n\n\nProjections allow you to pull out only the fields needed and rename them when you are querying for raw data records.\n\n\nAggregations\n\n\nAggregations allow you to perform some operation on the collected records.\n\n\nThe current aggregation types that are supported are:\n\n\n\n\n\n\n\n\nAggregation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nGROUP\n\n\nThe resulting output would be a record containing the result of an operation for each unique value combination in your specified fields\n\n\n\n\n\n\nCOUNT DISTINCT\n\n\nComputes the number of distinct elements in the fields. (May be approximate)\n\n\n\n\n\n\nLIMIT or RAW\n\n\nThe resulting output would be at most the number specified in size.\n\n\n\n\n\n\nDISTRIBUTION\n\n\nComputes distributions of the elements in the field. E.g. Find the median value or various percentile of a field, or get frequency or cumulative frequency distributions\n\n\n\n\n\n\nTOP K\n\n\nReturns the top K most frequently appearing values in the column\n\n\n\n\n\n\n\n\nCurrently we support \nGROUP\n aggregations with the following operations:\n\n\n\n\n\n\n\n\nOperation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nCOUNT\n\n\nComputes the number of the elements in the group\n\n\n\n\n\n\nSUM\n\n\nComputes the sum of the non-null values in the provided field for all elements in the group\n\n\n\n\n\n\nMIN\n\n\nReturns the minimum of the non-null values in the provided field for all the elements in the group\n\n\n\n\n\n\nMAX\n\n\nReturns the maximum of the non-null values in the provided field for all the elements in the group\n\n\n\n\n\n\nAVG\n\n\nComputes the average of the non-null values in the provided field for all the elements in the group\n\n\n\n\n\n\n\n\nWindows\n\n\nWindows in a Bullet query allow you to specify how often you'd like Bullet to return results.\n\n\nFor example, you could launch a query for 2 minutes, and have Bullet return a COUNT DISTINCT on a particular field every 3 seconds:\n\n\n\n\nSee documentation on the Web Service API for more info.\n\n\nResults\n\n\nThe Bullet Web Service returns your query result as well as associated metadata information in a structured JSON format. The UI can display the results in different formats.\n\n\n\n\nApproximate computation\n\n\nIt is often intractable to perform aggregations on an unbounded stream of data and still support arbitrary queries. However, it is possible if an exact answer is not required and the approximate answer's error is exactly quantifiable. There are stochastic algorithms and data structures that let us do this. We use \nData Sketches\n to perform aggregations such as counting uniques, and will be using Sketches to implement some future aggregations.\n\n\nSketches let us be exact in our computation up to configured thresholds and approximate after. The error is very controllable and quantifiable. All Bullet queries that use Sketches return the error bounds with Standard Deviations as part of the results so you can quantify the error exactly. Using Sketches lets us address otherwise hard to solve problems in sub-linear space. We uses Sketches to compute \nCOUNT DISTINCT\n, \nGROUP\n, \nDISTRIBUTION\n and \nTOP K\n queries.\n\n\nWe also use Sketches as a way to control high cardinality grouping (group by a natural key column or related) and rely on the Sketching data structure to drop excess groups. It is up to you setting up Bullet to determine to set Sketch sizes large or small enough for to satisfy the queries that will be performed on that instance of Bullet.\n\n\nArchitecture\n\n\nEnd-to-End Architecture\n\n\n\n\nThe image above shows how the various pieces of the Bullet interact at a high-level. All these layers are modular and pluggable. You can choose an implementation for the Backend and the PubSub (or create your own). The core of Bullet is abstracted into a \nlibrary\n that can be reused to implement the Backend, Web Service and PubSub layers in a platform agnostic manner.\n\n\n\n\nBackend\n\n\n\n\nThe Bullet Backend can be split into three main conceptual sub-systems:\n\n\n\n\nRequest Processor - receives queries, adds metadata and sends it to the rest of the system\n\n\nData Processor - reads data from a input stream, converts it to an unified data format and matches it against queries\n\n\nCombiner - combines results for different queries, performs final aggregations and returns results\n\n\n\n\nThe core of Bullet querying is not tied to the Backend and lives in a core library. This allows you implement the flow shown above in any stream processor you like.\n\n\nImplementations of \nBullet on Storm\n and \nBullet on Spark\n are currently supported.\n\n\nPubSub\n\n\nThe PubSub is responsible for transmitting queries from the API to the Backend and returning results back from the Backend to the clients. It decouples whatever particular Backend you are using with the API.\nWe currently support three different PubSub implementations:\n\n\n\n\nKafka\n\n\nREST\n\n\nStorm DRPC\n (only for non-windowed queries)\n\n\n\n\nYou can also very easily \nimplement your own\n by defining a few interfaces that we provide.\n\n\nWeb Service and UI\n\n\nThe rest of the pieces are just the standard other two pieces in a full-stack application:\n\n\n\n\nA Web Service that talks to the backend using the PubSub layer\n\n\nA UI that talks to this Web Service\n\n\n\n\nThe \nBullet Web Service\n is built using \nSpring Boot\n in Java and the \nUI\n is built in \nEmber\n.\n\n\nThe Web Service can be deployed as a standalone Java application (a JAR file) or easily rebuilt as a WAR to deploy your favorite servlet container like \nJetty\n. The UI is a client-side application that can be served using \nNode.js\n\n\n\n\nWant to know more?\n\n\nIn practice, the backend is implemented using the basic components that the Stream processing framework provides. See \nStorm Architecture\n and \nSpark Architecture\n for details.", 
            "title": "Home"
        }, 
        {
            "location": "/#how-is-bullet-useful", 
            "text": "How Bullet is used is largely determined by the data source it consumes. Depending on what kind of data you put Bullet on, the types of queries you run on it and your use-cases will change. As a look-forward query system with no persistence, you will not be able to repeat your queries on the same data. The next time you run your query, it will operate on the different data that arrives after that submission. If this usage pattern is what you need and you are looking for a light-weight system that can tap into your streaming data, then Bullet is for you!", 
            "title": "How is Bullet useful"
        }, 
        {
            "location": "/#example-how-bullet-is-used-at-yahoo", 
            "text": "Bullet is used in production internally at Yahoo by having it sit on a subset of raw user engagement events from Yahoo sites and apps. This lets Yahoo developers automatically validate their instrumentation code  end-to-end  in their Continuous Delivery pipelines. Validating instrumentation is critical since it powers pretty much all decisions and products including machine learning, corporate KPIs, analytics, personalization, targeting.  This instance of Bullet also powers other use-cases such as letting analysts validate assumptions about data, product managers verify launches instantly, debug issues and outages, or simply explore and play around with the data.   Blog post  Here is a  link to our blog post  condensing most of this information if you want to take a look.", 
            "title": "Example: How Bullet is used at Yahoo"
        }, 
        {
            "location": "/#quick-start", 
            "text": "See  Quick Start  to set up Bullet locally using Spark Streaming. You will generate some synthetic streaming data that you can then query with Bullet.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/#setup-bullet-on-your-streaming-data", 
            "text": "To set up Bullet on a real data stream, you need:   To setup the Bullet Backend on a stream processing framework. Currently, we support  Bullet on Storm  and  Bullet on Spark .  Plug in your source of data. See  Getting your data into Bullet  for details  Consume your data stream    The  Web Service  set up to convey queries and return results back from the backend  To choose a  PubSub implementation  that connects the Web Service and the Backend. We currently support  Kafka  and a  REST PubSub  on any Backend and  Storm DRPC  for the Storm Backend.  The optional  UI  set up to talk to your Web Service. You can skip the UI if all your access is programmatic    Schema in the UI  The UI also needs an endpoint that provides your data schema to help with query building. The Web Service you set up provides a simple file based schema endpoint that you can point the UI to if that is sufficient for your needs.", 
            "title": "Setup Bullet on your streaming data"
        }, 
        {
            "location": "/#querying-in-bullet", 
            "text": "Bullet queries allow you to filter, project and aggregate data. You can also specify a window to get incremental results. Bullet lets you fetch raw (the individual data records) as well as aggregated data.    See the  UI Usage section  for using the UI to build Bullet queries. This is the same UI you will build in the Quick Starts.    See the API section ( JSON ,  BQL ) for building Bullet API queries    For examples using the API, see  Examples . These are actual albeit cleansed queries sourced from the instance at Yahoo.", 
            "title": "Querying in Bullet"
        }, 
        {
            "location": "/#termination-conditions", 
            "text": "A Bullet query terminates and returns whatever has been collected so far when:   A maximum duration is reached. In other words, a query runs for a defined time window.  A maximum number of records is reached (only applicable for queries that are fetching raw data records and not aggregating).", 
            "title": "Termination conditions"
        }, 
        {
            "location": "/#filters", 
            "text": "Bullet supports two kinds of filters:     Filter Type  Meaning      Logical filter  Allow you to combine filter clauses (Logical or Relational) with logical operations like AND, OR and NOTs    Relational filters  Allow you to use comparison operations like equals, not equals, greater than, less than, regex like etc, on fields", 
            "title": "Filters"
        }, 
        {
            "location": "/#projections", 
            "text": "Projections allow you to pull out only the fields needed and rename them when you are querying for raw data records.", 
            "title": "Projections"
        }, 
        {
            "location": "/#aggregations", 
            "text": "Aggregations allow you to perform some operation on the collected records.  The current aggregation types that are supported are:     Aggregation  Meaning      GROUP  The resulting output would be a record containing the result of an operation for each unique value combination in your specified fields    COUNT DISTINCT  Computes the number of distinct elements in the fields. (May be approximate)    LIMIT or RAW  The resulting output would be at most the number specified in size.    DISTRIBUTION  Computes distributions of the elements in the field. E.g. Find the median value or various percentile of a field, or get frequency or cumulative frequency distributions    TOP K  Returns the top K most frequently appearing values in the column     Currently we support  GROUP  aggregations with the following operations:     Operation  Meaning      COUNT  Computes the number of the elements in the group    SUM  Computes the sum of the non-null values in the provided field for all elements in the group    MIN  Returns the minimum of the non-null values in the provided field for all the elements in the group    MAX  Returns the maximum of the non-null values in the provided field for all the elements in the group    AVG  Computes the average of the non-null values in the provided field for all the elements in the group", 
            "title": "Aggregations"
        }, 
        {
            "location": "/#windows", 
            "text": "Windows in a Bullet query allow you to specify how often you'd like Bullet to return results.  For example, you could launch a query for 2 minutes, and have Bullet return a COUNT DISTINCT on a particular field every 3 seconds:   See documentation on the Web Service API for more info.", 
            "title": "Windows"
        }, 
        {
            "location": "/#results", 
            "text": "The Bullet Web Service returns your query result as well as associated metadata information in a structured JSON format. The UI can display the results in different formats.", 
            "title": "Results"
        }, 
        {
            "location": "/#approximate-computation", 
            "text": "It is often intractable to perform aggregations on an unbounded stream of data and still support arbitrary queries. However, it is possible if an exact answer is not required and the approximate answer's error is exactly quantifiable. There are stochastic algorithms and data structures that let us do this. We use  Data Sketches  to perform aggregations such as counting uniques, and will be using Sketches to implement some future aggregations.  Sketches let us be exact in our computation up to configured thresholds and approximate after. The error is very controllable and quantifiable. All Bullet queries that use Sketches return the error bounds with Standard Deviations as part of the results so you can quantify the error exactly. Using Sketches lets us address otherwise hard to solve problems in sub-linear space. We uses Sketches to compute  COUNT DISTINCT ,  GROUP ,  DISTRIBUTION  and  TOP K  queries.  We also use Sketches as a way to control high cardinality grouping (group by a natural key column or related) and rely on the Sketching data structure to drop excess groups. It is up to you setting up Bullet to determine to set Sketch sizes large or small enough for to satisfy the queries that will be performed on that instance of Bullet.", 
            "title": "Approximate computation"
        }, 
        {
            "location": "/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/#end-to-end-architecture", 
            "text": "The image above shows how the various pieces of the Bullet interact at a high-level. All these layers are modular and pluggable. You can choose an implementation for the Backend and the PubSub (or create your own). The core of Bullet is abstracted into a  library  that can be reused to implement the Backend, Web Service and PubSub layers in a platform agnostic manner.", 
            "title": "End-to-End Architecture"
        }, 
        {
            "location": "/#backend", 
            "text": "The Bullet Backend can be split into three main conceptual sub-systems:   Request Processor - receives queries, adds metadata and sends it to the rest of the system  Data Processor - reads data from a input stream, converts it to an unified data format and matches it against queries  Combiner - combines results for different queries, performs final aggregations and returns results   The core of Bullet querying is not tied to the Backend and lives in a core library. This allows you implement the flow shown above in any stream processor you like.  Implementations of  Bullet on Storm  and  Bullet on Spark  are currently supported.", 
            "title": "Backend"
        }, 
        {
            "location": "/#pubsub", 
            "text": "The PubSub is responsible for transmitting queries from the API to the Backend and returning results back from the Backend to the clients. It decouples whatever particular Backend you are using with the API.\nWe currently support three different PubSub implementations:   Kafka  REST  Storm DRPC  (only for non-windowed queries)   You can also very easily  implement your own  by defining a few interfaces that we provide.", 
            "title": "PubSub"
        }, 
        {
            "location": "/#web-service-and-ui", 
            "text": "The rest of the pieces are just the standard other two pieces in a full-stack application:   A Web Service that talks to the backend using the PubSub layer  A UI that talks to this Web Service   The  Bullet Web Service  is built using  Spring Boot  in Java and the  UI  is built in  Ember .  The Web Service can be deployed as a standalone Java application (a JAR file) or easily rebuilt as a WAR to deploy your favorite servlet container like  Jetty . The UI is a client-side application that can be served using  Node.js   Want to know more?  In practice, the backend is implemented using the basic components that the Stream processing framework provides. See  Storm Architecture  and  Spark Architecture  for details.", 
            "title": "Web Service and UI"
        }, 
        {
            "location": "/quick-start/spark/", 
            "text": "Quick Start on Spark\n\n\nIn this section we will setup a mock instance of Bullet to play around with. We will use \nBullet Spark\n to run the backend of Bullet on the \nSpark\n framework. And we will use the \nBullet Kafka PubSub\n.\n\n\nAt the end of this section, you will have:\n\n\n\n\nLaunched the Bullet backend on Spark\n\n\nSetup the \nWeb Service\n\n\nSetup the \nUI\n to talk to the Web Service\n\n\n\n\nPrerequisites\n\n\n\n\nYou will need to be on an Unix-based system (Mac OS X, Ubuntu ...) with \ncurl\n installed\n\n\nYou will need \nJDK 8\n installed\n\n\n\n\nInstall Script\n\n\nSimply run:\n\n\ncurl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-spark.sh | bash\n\n\n\n\nThis will setup a local Spark and Kafka cluster, a Bullet running on it, the Bullet Web Service and a Bullet UI for you. Once everything has launched, you should be able to go to the Bullet UI running locally at \nhttp://localhost:8800\n. You can then \ncontinue this guide from here\n.\n\n\n\n\nWant to DIY?\n\n\nIf you want to manually run all the commands or if the script died while doing something above (might want to perform the \nteardown\n first), you can continue below.\n\n\n\n\nManual Installation\n\n\nStep 1: Setup directories and examples\n\n\nexport BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/spark\nmkdir -p $BULLET_HOME/pubsub\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME\ncurl -LO https://github.com/bullet-db/bullet-db.github.io/releases/download/v0.5.2/examples_artifacts.tar.gz\ntar -xzf examples_artifacts.tar.gz\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-examples\n\n\n\n\nSetup Kafka\n\n\nFor this instance of Bullet we will use the Kafka PubSub implementation found in \nbullet-spark\n. So we will first download and run Kafka, and setup a couple Kafka topics.\n\n\nStep 2: Download and Install Kafka\n\n\ncd $BULLET_HOME/pubsub\ncurl -Lo bullet-kafka.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-kafka/0.3.0/bullet-kafka-0.3.0-fat.jar\ncurl -LO https://archive.apache.org/dist/kafka/0.11.0.1/kafka_2.12-0.11.0.1.tgz\ntar -xzf kafka_2.12-0.11.0.1.tgz\nexport KAFKA_DIR=$BULLET_HOME/pubsub/kafka_2.12-0.11.0.1\n\n\n\n\nStep 3: Start Zookeeper\n\n\n$KAFKA_DIR/bin/zookeeper-server-start.sh $KAFKA_DIR/config/zookeeper.properties \n\n\n\n\n\nStep 4: Start Kafka\n\n\nGive Zookeeper ~5-10 seconds to start up, then start Kafka:\n\n\n$KAFKA_DIR/bin/kafka-server-start.sh $KAFKA_DIR/config/server.properties \n\n\n\n\n\nStep 5: Create Kafka Topics\n\n\nThe Bullet Kafka PubSub uses two topics. One to send messages from the Web Service to the Backend, and one to send messages from the Backend to the Web Service. So we will create a Kafka topic called \"bullet.requests\" and another called \"bullet.responses\".\n\n\n$KAFKA_DIR/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic bullet.requests\n$KAFKA_DIR/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic bullet.responses\n\n\n\n\nSetup Bullet Backend on Spark\n\n\nWe will run the bullet-spark backend using \nSpark 2.2.1\n.\n\n\nStep 6: Install Spark 2.2.1\n\n\nexport BULLET_SPARK=$BULLET_HOME/backend/spark\ncd $BULLET_SPARK\ncurl -O http://www-eu.apache.org/dist/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz\ntar -xzf spark-2.2.1-bin-hadoop2.7.tgz\n\n\n\n\nStep 7: Setup Bullet-Spark and Example Data Producer\n\n\ncp $BULLET_HOME/bullet-examples/backend/spark/* $BULLET_SPARK\ncurl -Lo bullet-spark.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-spark/0.1.2/bullet-spark-0.1.2-standalone.jar\n\n\n\n\nStep 8: Launch the Bullet Spark Backend\n\n\nRun this multi-line command (new lines are escaped):\n\n\n$BULLET_SPARK/spark-2.2.1-bin-hadoop2.7/bin/spark-submit \\\n    --master local[10]  \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --jars $BULLET_HOME/pubsub/bullet-kafka.jar,$BULLET_SPARK/bullet-spark-example.jar \\\n    $BULLET_SPARK/bullet-spark.jar \\\n    --bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml \n log.txt \n\n\n\n\n\n\nThe Backend will usually be up and running usually within 5-10 seconds. Once it is running you can get information about the Spark job in the Spark UI, which can be seen in your browser at \nhttp://localhost:4040\n by default. The Web Service will now be hooked up through the Kafka PubSub to the Spark backend. To test it you can now run a Bullet query by hitting the Web Service directly:\n\n\nSetup Web Service\n\n\nStep 9: Install the Bullet Web Service\n\n\ncd $BULLET_HOME/service\ncurl -Lo bullet-service.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/0.2.1/bullet-service-0.2.1-embedded.jar\ncp $BULLET_EXAMPLES/web-service/example_kafka_pubsub_config.yaml $BULLET_HOME/service/\ncp $BULLET_EXAMPLES/web-service/example_columns.json $BULLET_HOME/service/\n\n\n\n\nStep 10: Launch the Web Service\n\n\nRun this multi-line command (new lines are escaped):\n\n\njava -Dloader.path=$BULLET_HOME/pubsub/bullet-kafka.jar -jar bullet-service.jar \\\n    --bullet.pubsub.config=$BULLET_HOME/service/example_kafka_pubsub_config.yaml \\\n    --bullet.schema.file=$BULLET_HOME/service/example_columns.json \\\n    --server.port=9999  \\\n    --logging.path=. \\\n    --logging.file=log.txt \n log.txt \n\n\n\n\n\nStep 11: Test the Web Service (optional)\n\n\nWe can check that the Web Service is up and running by getting the example columns through the API:\n\n\ncurl -s http://localhost:9999/api/bullet/columns\n\n\n\n\ncurl -s -H 'Content-Type: text/plain' -X POST -d '{\naggregation\n: {\nsize\n: 1}}' http://localhost:9999/api/bullet/sse-query\n\n\n\n\nThis query will return a result JSON containing a \"records\" field containing a single record, and a \"meta\" field with some meta information.\n\n\n\n\nWhat is this data?\n\n\nThis data is randomly generated by the \ncustom Spark Streaming Receiver\n that generates toy data to demo Bullet. In practice, your producer would read from an actual data source such as Kafka etc.\n\n\n\n\nYou can also check the status of the Web Service by looking at the Web Service log: $BULLET_HOME/service/log.txt\n\n\nSetting up the Bullet UI\n\n\nStep 12: Install Node\n\n\ncd $BULLET_HOME/ui\ncurl -s https://raw.githubusercontent.com/creationix/nvm/v0.33.1/install.sh | bash\nsource ~/.bashrc\nnvm install v6.9.4\nnvm use v6.9.4\n\n\n\n\nStep 13: Install the Bullet UI\n\n\ncurl -LO https://github.com/bullet-db/bullet-ui/releases/download/v0.5.0/bullet-ui-v0.5.0.tar.gz\ntar -xzf bullet-ui-v0.5.0.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/\n\n\n\n\nStep 14: Launch the UI\n\n\nPORT=8800 node express-server.js \n\n\n\n\n\nVisit \nhttp://localhost:8800\n to query your topology with the UI. See \nUI usage\n for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.\n\n\n\n\nRunning it remotely?\n\n\nIf you access the UI from another machine than where your UI is actually running, you will need to edit \nconfig/env-settings.json\n. Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change \nlocalhost\n in your env-settings.json to point to the host name where you will hosting the UI. This will be the same as the UI host you use in the browser. You can also do a local port forward on the machine accessing the UI by running: \nssh -N -L 8800:localhost:8800 -L 9999:localhost:9999 hostname-of-the-quickstart-components 2\n1\n.\n\n\n\n\nPlaying around with the instance:\n\n\nCheck out and follow along with the \nUI Usage\n page as it shows you some queries you can run using this UI.\n\n\nTeardown\n\n\nWhen you are done trying out Bullet, you can stop the processes and cleanup using the instructions below.\n\n\nIf you were using the \nInstall Script\n or if you don't want to manually bring down everything, you can run:\n\n\ncurl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-spark.sh | bash -s cleanup\n\n\n\n\nIf you were performing the steps yourself, you can also manually cleanup \nall the components and all the downloads\n using:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUI\n\n\npkill -f [e]xpress-server.js\n\n\n\n\n\n\nWeb Service\n\n\npkill -f [e]xample_kafka_pubsub_config.yaml\n\n\n\n\n\n\nSpark\n\n\npkill -f [b]ullet-spark\n\n\n\n\n\n\nKafka\n\n\n${KAFKA_DIR}/bin/kafka-server-stop.sh\n\n\n\n\n\n\nZookeeper\n\n\n${KAFKA_DIR}/bin/zookeeper-server-stop.sh\n\n\n\n\n\n\nFile System\n\n\nrm -rf $BULLET_HOME /tmp/zookeeper /tmp/kafka-logs/ /tmp/spark-checkpoint\n\n\n\n\n\n\n\n\nNote: This does \nnot\n delete \n$HOME/.nvm\n.\n\n\nWhat did we do?\n\n\nThis section will go over the various custom pieces this example plugged into Bullet, so you can better understand what we did.\n\n\nSpark Streaming Job\n\n\nThe Spark Streaming application we ran was Bullet plugged in with a custom Receiver in our implementation of the Bullet Spark DataProducer trait. This Receiver and DataProducer are implemented in this \nexample project\n and was already built for you when you \ndownloaded the examples\n. It does not read from any data source and just produces random, structured data. It also produces only up to a maximum number of records in a given period. Both this maximum and the length of a period are configured in the Receiver (at most 100 every 1 second).\n\n\n$BULLET_SPARK/spark-2.2.1-bin-hadoop2.7/bin/spark-submit \\\n    --master local[10]  \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --jars $BULLET_HOME/pubsub/bullet-kafka.jar,$BULLET_SPARK/bullet-spark-example.jar \\\n    $BULLET_SPARK/bullet-spark.jar \\\n    --bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml \n log.txt \n\n\n\n\n\n\nWe launched the bullet-spark jar (an uber or \"fat\" jar) containing Bullet Spark and all its dependencies. We added our Pubsub (see below) implementation and our jar containing our custom Receiver to the Spark job's additional jars.\n\n\nThe settings defined by \n--bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml\n and the arguments here run all components in the Spark Streaming job.\n\n\n\n\nI thought you said hundreds of thousands of records...\n\n\n100 records per second is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components with CPU and memory configurations to accommodate for your data volume and querying needs.\n\n\n\n\nLet's look at the \ncustom Receiver code\n that generates the data.\n\n\n  private def receive(): Unit = {\n    nextIntervalStart = System.currentTimeMillis()\n    while (!isStopped) {\n      val timeNow = System.currentTimeMillis()\n      // Only emit if we are still in the interval and haven't gone over our per period max\n      if (timeNow \n= nextIntervalStart \n generatedThisPeriod \n maxPerPeriod) {\n        store(generateRecord())\n        generatedThisPeriod += 1\n      }\n      if (timeNow \n nextIntervalStart) {\n        logger.info(\nGenerated {} tuples out of {}\n, generatedThisPeriod, maxPerPeriod)\n        nextIntervalStart = timeNow + period\n        generatedThisPeriod = 0\n        periodCount += 1\n      }\n      // It is courteous to sleep for a short time.\n      try {\n        Thread.sleep(1)\n      } catch {\n        case e: InterruptedException =\n logger.error(\nError: \n, e)\n      }\n    }\n  }\n\n\n\n\nThis method above emits the data. This method is wrapped in a thread that is called by the Spark framework. This function only emits at most the given maximum tuples per period.\n\n\n  private def makeRandomMap: Map[java.lang.String, java.lang.String] = {\n    val randomMap = new HashMap[java.lang.String, java.lang.String](2)\n    randomMap.put(RandomReceiver.RANDOM_MAP_KEY_A, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n    randomMap.put(RandomReceiver.RANDOM_MAP_KEY_B, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n    randomMap\n  }\n\n  private def generateRecord(): BulletRecord = {\n    val record = new SimpleBulletRecord()\n    val uuid = UUID.randomUUID().toString\n    record.setString(RandomReceiver.STRING, uuid)\n    record.setLong(RandomReceiver.LONG, generatedThisPeriod)\n    record.setDouble(RandomReceiver.DOUBLE, Random.nextDouble())\n    record.setDouble(RandomReceiver.GAUSSIAN, Random.nextGaussian())\n    record.setString(RandomReceiver.TYPE, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n    record.setLong(RandomReceiver.DURATION, System.nanoTime() % RandomReceiver.INTEGER_POOL(Random.nextInt(RandomReceiver.INTEGER_POOL.length)))\n\n    // Don't use Scala Map and convert it by asJava when calling setxxxMap method in BulletRecord.\n    // It converts Scala Map to scala.collection.convert.Wrappers$MapWrapper which is not serializable in scala 2.11.x (https://issues.scala-lang.org/browse/SI-8911).\n\n    record.setStringMap(RandomReceiver.SUBTYPES_MAP, makeRandomMap);\n\n    val booleanMap = new HashMap[java.lang.String, java.lang.Boolean](4)\n    booleanMap.put(uuid.substring(0, 8), Random.nextBoolean())\n    booleanMap.put(uuid.substring(9, 13), Random.nextBoolean())\n    booleanMap.put(uuid.substring(14, 18), Random.nextBoolean())\n    booleanMap.put(uuid.substring(19, 23), Random.nextBoolean())\n    record.setBooleanMap(RandomReceiver.BOOLEAN_MAP, booleanMap)\n\n\n    val statsMap = new HashMap[java.lang.String, java.lang.Long](4)\n    statsMap.put(RandomReceiver.PERIOD_COUNT, periodCount)\n    statsMap.put(RandomReceiver.RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod)\n    statsMap.put(RandomReceiver.NANO_TIME, System.nanoTime())\n    statsMap.put(RandomReceiver.TIMESTAMP, System.nanoTime())\n    record.setLongMap(RandomReceiver.STATS_MAP, statsMap)\n\n    record.setListOfStringMap(RandomReceiver.LIST, asList(makeRandomMap, makeRandomMap))\n    record\n  }\n\n\n\n\nThis \ngenerateRecord\n method generates some fields randomly and inserts them into a BulletRecord (simple). Note that the BulletRecord is typed and all data must be inserted with the proper types.\n\n\nThis whole receiver is plugged into an implementation of the Spark DataProducer trait that Bullet Spark requires to plug in your data (as a Spark DStream) into it. You can find this class implemented \nhere\n and reproduced below.\n\n\npackage com.yahoo.bullet.spark.examples\n\nimport com.yahoo.bullet.record.BulletRecord\nimport com.yahoo.bullet.spark.DataProducer\nimport com.yahoo.bullet.spark.examples.receiver.RandomReceiver\nimport com.yahoo.bullet.spark.utils.BulletSparkConfig\nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.dstream.DStream\n\nclass RandomProducer extends DataProducer {\n  override def getBulletRecordStream(ssc: StreamingContext, config: BulletSparkConfig): DStream[BulletRecord] = {\n    // Bullet record input stream.\n    val bulletReceiver = new RandomReceiver(config)\n    ssc.receiverStream(bulletReceiver).asInstanceOf[DStream[BulletRecord]]\n  }\n}\n\n\n\n\nIf you put Bullet on your data, you will need to write a DataProducer (or a full on Spark DAG if your reading is complex), that reads from your data source and emits a DStream of BulletRecords with the fields you wish to be query-able similar to this example.\n\n\nPubSub\n\n\nWe used the \nKafka PubSub\n. We configured the Backend to use this PubSub by adding these settings to the YAML file that we passed to our Spark Streaming job. Notice that we set the context to \nQUERY_PROCESSING\n since this is the Backend.\n\n\nbullet.pubsub.context.name: \nQUERY_PROCESSING\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.kafka.KafkaPubSub\n\nbullet.pubsub.kafka.bootstrap.servers: \nlocalhost:9092\n\nbullet.pubsub.kafka.request.topic.name: \nbullet.requests\n\nbullet.pubsub.kafka.response.topic.name: \nbullet.responses\n\n\n\n\n\nFor the Web Service, we passed in a YAML file that pointed to the same Kafka topics. Notice that we set the context to \nQUERY_SUBMISSION\n since this is the Web Service.\n\n\nbullet.pubsub.context.name: \nQUERY_SUBMISSION\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.kafka.KafkaPubSub\n\nbullet.pubsub.kafka.bootstrap.servers: \nlocalhost:9092\n\nbullet.pubsub.kafka.request.topic.name: \nbullet.requests\n\nbullet.pubsub.kafka.response.topic.name: \nbullet.responses\n\n\n\n\n\nWeb Service\n\n\nWe launched the Web Service using two custom files - a PubSub configuration YAML file and JSON schema file.\n\n\nThe JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.\n\n\nThe following is a snippet from the \nJSON file\n. Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using \nenumerations\n.\n\n\n[\n    {\n        \nname\n: \nprobability\n,\n        \ntype\n: \nDOUBLE\n,\n        \ndescription\n: \nGenerated from Random#nextDouble\n\n    },\n    ...\n    {\n        \nname\n: \nstats\n,\n        \ntype\n: \nMAP\n,\n        \nsubtype\n: \nLONG\n,\n        \ndescription\n: \nThis map contains some numeric information such as the current number of periods etc.\n,\n        \nenumerations\n: [\n            ...\n            {\nname\n: \nnano_time\n, \ndescription\n: \nThe ns time when this record was generated\n}\n        ]\n    },\n    {\n        \nname\n: \nclassifiers\n,\n        \ntype\n: \nLIST\n,\n        \nsubtype\n: \nMAP\n,\n        \ndescription\n: \nThis contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf\n\n    }\n]\n\n\n\n\nThe contents of the \nPubSub configuration file\n was discussed in the \nPubSub section above\n.\n\n\nUI\n\n\nFinally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.\n\n\n{\n  \ndefault\n: {\n    \nqueryHost\n: \nhttp://localhost:9999\n,\n    \nqueryNamespace\n: \napi/bullet\n,\n    \nqueryPath\n: \nws-query\n,\n    \nqueryStompRequestChannel\n: \n/server/request\n,\n    \nqueryStompResponseChannel\n: \n/client/response\n,\n    \nschemaHost\n: \nhttp://localhost:9999\n,\n    \nschemaNamespace\n: \napi/bullet\n,\n    \nhelpLinks\n: [\n      {\n        \nname\n: \nTutorials\n,\n        \nlink\n: \nhttps://bullet-db.github.io/ui/usage\n\n      }\n    ],\n    \nbugLink\n: \nhttps://github.com/bullet-db/bullet-ui/issues\n,\n    \nmodelVersion\n: 3,\n    \nmigrations\n: {\n      \ndeletions\n: \nquery\n\n    },\n    \ndefaultValues\n: {\n      \naggregationMaxSize\n: 1024,\n      \nrawMaxSize\n: 500,\n      \ndurationMaxSecs\n: 86400,\n      \ndistributionNumberOfPoints\n: 11,\n      \ndistributionQuantilePoints\n: \n0, 0.25, 0.5, 0.75, 0.9, 1\n,\n      \ndistributionQuantileStart\n: 0,\n      \ndistributionQuantileEnd\n: 1,\n      \ndistributionQuantileIncrement\n: 0.1,\n      \nwindowEmitFrequencyMinSecs\n: 1,\n      \neveryForRecordBasedWindow\n: 1,\n      \neveryForTimeBasedWindow\n: 2,\n      \nsketches\n: {\n        \ncountDistinctMaxEntries\n: 16384,\n        \ngroupByMaxEntries\n: 512,\n        \ndistributionMaxEntries\n: 1024,\n        \ndistributionMaxNumberOfPoints\n: 200,\n        \ntopKMaxEntries\n: 1024,\n        \ntopKErrorType\n: \nNo False Negatives\n\n      },\n      \nmetadataKeyMapping\n: {\n        \nquerySection\n: \nQuery\n,\n        \nwindowSection\n: \nWindow\n,\n        \nsketchSection\n: \nSketch\n,\n        \ntheta\n: \nTheta\n,\n        \nuniquesEstimate\n: \nUniques Estimate\n,\n        \nqueryCreationTime\n: \nReceive Time\n,\n        \nqueryTerminationTime\n: \nFinish Time\n,\n        \nestimatedResult\n: \nWas Estimated\n,\n        \nstandardDeviations\n: \nStandard Deviations\n,\n        \nnormalizedRankError\n: \nNormalized Rank Error\n,\n        \nmaximumCountError\n: \nMaximum Count Error\n,\n        \nitemsSeen\n: \nItems Seen\n,\n        \nminimumValue\n: \nMinimum Value\n,\n        \nmaximumValue\n: \nMaximum Value\n,\n        \nwindowNumber\n: \nNumber\n,\n        \nwindowSize\n: \nSize\n,\n        \nwindowEmitTime\n: \nEmit Time\n,\n        \nexpectedEmitTime\n: \nExpected Emit Time\n\n      }\n    }\n  }\n}\n\n\n\n\nSince we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no \nschemaPath\n because it must be the constant string \ncolumns\n. If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to \nschemaHost/schemaNamespace/columns\n.", 
            "title": "Spark"
        }, 
        {
            "location": "/quick-start/spark/#quick-start-on-spark", 
            "text": "In this section we will setup a mock instance of Bullet to play around with. We will use  Bullet Spark  to run the backend of Bullet on the  Spark  framework. And we will use the  Bullet Kafka PubSub .  At the end of this section, you will have:   Launched the Bullet backend on Spark  Setup the  Web Service  Setup the  UI  to talk to the Web Service   Prerequisites   You will need to be on an Unix-based system (Mac OS X, Ubuntu ...) with  curl  installed  You will need  JDK 8  installed", 
            "title": "Quick Start on Spark"
        }, 
        {
            "location": "/quick-start/spark/#install-script", 
            "text": "Simply run:  curl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-spark.sh | bash  This will setup a local Spark and Kafka cluster, a Bullet running on it, the Bullet Web Service and a Bullet UI for you. Once everything has launched, you should be able to go to the Bullet UI running locally at  http://localhost:8800 . You can then  continue this guide from here .   Want to DIY?  If you want to manually run all the commands or if the script died while doing something above (might want to perform the  teardown  first), you can continue below.", 
            "title": "Install Script"
        }, 
        {
            "location": "/quick-start/spark/#manual-installation", 
            "text": "", 
            "title": "Manual Installation"
        }, 
        {
            "location": "/quick-start/spark/#step-1-setup-directories-and-examples", 
            "text": "export BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/spark\nmkdir -p $BULLET_HOME/pubsub\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME\ncurl -LO https://github.com/bullet-db/bullet-db.github.io/releases/download/v0.5.2/examples_artifacts.tar.gz\ntar -xzf examples_artifacts.tar.gz\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-examples", 
            "title": "Step 1: Setup directories and examples"
        }, 
        {
            "location": "/quick-start/spark/#setup-kafka", 
            "text": "For this instance of Bullet we will use the Kafka PubSub implementation found in  bullet-spark . So we will first download and run Kafka, and setup a couple Kafka topics.", 
            "title": "Setup Kafka"
        }, 
        {
            "location": "/quick-start/spark/#step-2-download-and-install-kafka", 
            "text": "cd $BULLET_HOME/pubsub\ncurl -Lo bullet-kafka.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-kafka/0.3.0/bullet-kafka-0.3.0-fat.jar\ncurl -LO https://archive.apache.org/dist/kafka/0.11.0.1/kafka_2.12-0.11.0.1.tgz\ntar -xzf kafka_2.12-0.11.0.1.tgz\nexport KAFKA_DIR=$BULLET_HOME/pubsub/kafka_2.12-0.11.0.1", 
            "title": "Step 2: Download and Install Kafka"
        }, 
        {
            "location": "/quick-start/spark/#step-3-start-zookeeper", 
            "text": "$KAFKA_DIR/bin/zookeeper-server-start.sh $KAFKA_DIR/config/zookeeper.properties", 
            "title": "Step 3: Start Zookeeper"
        }, 
        {
            "location": "/quick-start/spark/#step-4-start-kafka", 
            "text": "Give Zookeeper ~5-10 seconds to start up, then start Kafka:  $KAFKA_DIR/bin/kafka-server-start.sh $KAFKA_DIR/config/server.properties", 
            "title": "Step 4: Start Kafka"
        }, 
        {
            "location": "/quick-start/spark/#step-5-create-kafka-topics", 
            "text": "The Bullet Kafka PubSub uses two topics. One to send messages from the Web Service to the Backend, and one to send messages from the Backend to the Web Service. So we will create a Kafka topic called \"bullet.requests\" and another called \"bullet.responses\".  $KAFKA_DIR/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic bullet.requests\n$KAFKA_DIR/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic bullet.responses", 
            "title": "Step 5: Create Kafka Topics"
        }, 
        {
            "location": "/quick-start/spark/#setup-bullet-backend-on-spark", 
            "text": "We will run the bullet-spark backend using  Spark 2.2.1 .", 
            "title": "Setup Bullet Backend on Spark"
        }, 
        {
            "location": "/quick-start/spark/#step-6-install-spark-221", 
            "text": "export BULLET_SPARK=$BULLET_HOME/backend/spark\ncd $BULLET_SPARK\ncurl -O http://www-eu.apache.org/dist/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz\ntar -xzf spark-2.2.1-bin-hadoop2.7.tgz", 
            "title": "Step 6: Install Spark 2.2.1"
        }, 
        {
            "location": "/quick-start/spark/#step-7-setup-bullet-spark-and-example-data-producer", 
            "text": "cp $BULLET_HOME/bullet-examples/backend/spark/* $BULLET_SPARK\ncurl -Lo bullet-spark.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-spark/0.1.2/bullet-spark-0.1.2-standalone.jar", 
            "title": "Step 7: Setup Bullet-Spark and Example Data Producer"
        }, 
        {
            "location": "/quick-start/spark/#step-8-launch-the-bullet-spark-backend", 
            "text": "Run this multi-line command (new lines are escaped):  $BULLET_SPARK/spark-2.2.1-bin-hadoop2.7/bin/spark-submit \\\n    --master local[10]  \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --jars $BULLET_HOME/pubsub/bullet-kafka.jar,$BULLET_SPARK/bullet-spark-example.jar \\\n    $BULLET_SPARK/bullet-spark.jar \\\n    --bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml   log.txt    The Backend will usually be up and running usually within 5-10 seconds. Once it is running you can get information about the Spark job in the Spark UI, which can be seen in your browser at  http://localhost:4040  by default. The Web Service will now be hooked up through the Kafka PubSub to the Spark backend. To test it you can now run a Bullet query by hitting the Web Service directly:", 
            "title": "Step 8: Launch the Bullet Spark Backend"
        }, 
        {
            "location": "/quick-start/spark/#setup-web-service", 
            "text": "", 
            "title": "Setup Web Service"
        }, 
        {
            "location": "/quick-start/spark/#step-9-install-the-bullet-web-service", 
            "text": "cd $BULLET_HOME/service\ncurl -Lo bullet-service.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/0.2.1/bullet-service-0.2.1-embedded.jar\ncp $BULLET_EXAMPLES/web-service/example_kafka_pubsub_config.yaml $BULLET_HOME/service/\ncp $BULLET_EXAMPLES/web-service/example_columns.json $BULLET_HOME/service/", 
            "title": "Step 9: Install the Bullet Web Service"
        }, 
        {
            "location": "/quick-start/spark/#step-10-launch-the-web-service", 
            "text": "Run this multi-line command (new lines are escaped):  java -Dloader.path=$BULLET_HOME/pubsub/bullet-kafka.jar -jar bullet-service.jar \\\n    --bullet.pubsub.config=$BULLET_HOME/service/example_kafka_pubsub_config.yaml \\\n    --bullet.schema.file=$BULLET_HOME/service/example_columns.json \\\n    --server.port=9999  \\\n    --logging.path=. \\\n    --logging.file=log.txt   log.txt", 
            "title": "Step 10: Launch the Web Service"
        }, 
        {
            "location": "/quick-start/spark/#step-11-test-the-web-service-optional", 
            "text": "We can check that the Web Service is up and running by getting the example columns through the API:  curl -s http://localhost:9999/api/bullet/columns  curl -s -H 'Content-Type: text/plain' -X POST -d '{ aggregation : { size : 1}}' http://localhost:9999/api/bullet/sse-query  This query will return a result JSON containing a \"records\" field containing a single record, and a \"meta\" field with some meta information.   What is this data?  This data is randomly generated by the  custom Spark Streaming Receiver  that generates toy data to demo Bullet. In practice, your producer would read from an actual data source such as Kafka etc.   You can also check the status of the Web Service by looking at the Web Service log: $BULLET_HOME/service/log.txt", 
            "title": "Step 11: Test the Web Service (optional)"
        }, 
        {
            "location": "/quick-start/spark/#setting-up-the-bullet-ui", 
            "text": "", 
            "title": "Setting up the Bullet UI"
        }, 
        {
            "location": "/quick-start/spark/#step-12-install-node", 
            "text": "cd $BULLET_HOME/ui\ncurl -s https://raw.githubusercontent.com/creationix/nvm/v0.33.1/install.sh | bash\nsource ~/.bashrc\nnvm install v6.9.4\nnvm use v6.9.4", 
            "title": "Step 12: Install Node"
        }, 
        {
            "location": "/quick-start/spark/#step-13-install-the-bullet-ui", 
            "text": "curl -LO https://github.com/bullet-db/bullet-ui/releases/download/v0.5.0/bullet-ui-v0.5.0.tar.gz\ntar -xzf bullet-ui-v0.5.0.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/", 
            "title": "Step 13: Install the Bullet UI"
        }, 
        {
            "location": "/quick-start/spark/#step-14-launch-the-ui", 
            "text": "PORT=8800 node express-server.js    Visit  http://localhost:8800  to query your topology with the UI. See  UI usage  for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.   Running it remotely?  If you access the UI from another machine than where your UI is actually running, you will need to edit  config/env-settings.json . Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change  localhost  in your env-settings.json to point to the host name where you will hosting the UI. This will be the same as the UI host you use in the browser. You can also do a local port forward on the machine accessing the UI by running:  ssh -N -L 8800:localhost:8800 -L 9999:localhost:9999 hostname-of-the-quickstart-components 2 1 .", 
            "title": "Step 14: Launch the UI"
        }, 
        {
            "location": "/quick-start/spark/#playing-around-with-the-instance", 
            "text": "Check out and follow along with the  UI Usage  page as it shows you some queries you can run using this UI.", 
            "title": "Playing around with the instance:"
        }, 
        {
            "location": "/quick-start/spark/#teardown", 
            "text": "When you are done trying out Bullet, you can stop the processes and cleanup using the instructions below.  If you were using the  Install Script  or if you don't want to manually bring down everything, you can run:  curl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-spark.sh | bash -s cleanup  If you were performing the steps yourself, you can also manually cleanup  all the components and all the downloads  using:           UI  pkill -f [e]xpress-server.js    Web Service  pkill -f [e]xample_kafka_pubsub_config.yaml    Spark  pkill -f [b]ullet-spark    Kafka  ${KAFKA_DIR}/bin/kafka-server-stop.sh    Zookeeper  ${KAFKA_DIR}/bin/zookeeper-server-stop.sh    File System  rm -rf $BULLET_HOME /tmp/zookeeper /tmp/kafka-logs/ /tmp/spark-checkpoint     Note: This does  not  delete  $HOME/.nvm .", 
            "title": "Teardown"
        }, 
        {
            "location": "/quick-start/spark/#what-did-we-do", 
            "text": "This section will go over the various custom pieces this example plugged into Bullet, so you can better understand what we did.", 
            "title": "What did we do?"
        }, 
        {
            "location": "/quick-start/spark/#spark-streaming-job", 
            "text": "The Spark Streaming application we ran was Bullet plugged in with a custom Receiver in our implementation of the Bullet Spark DataProducer trait. This Receiver and DataProducer are implemented in this  example project  and was already built for you when you  downloaded the examples . It does not read from any data source and just produces random, structured data. It also produces only up to a maximum number of records in a given period. Both this maximum and the length of a period are configured in the Receiver (at most 100 every 1 second).  $BULLET_SPARK/spark-2.2.1-bin-hadoop2.7/bin/spark-submit \\\n    --master local[10]  \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --jars $BULLET_HOME/pubsub/bullet-kafka.jar,$BULLET_SPARK/bullet-spark-example.jar \\\n    $BULLET_SPARK/bullet-spark.jar \\\n    --bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml   log.txt    We launched the bullet-spark jar (an uber or \"fat\" jar) containing Bullet Spark and all its dependencies. We added our Pubsub (see below) implementation and our jar containing our custom Receiver to the Spark job's additional jars.  The settings defined by  --bullet-spark-conf=$BULLET_SPARK/bullet_spark_kafka_settings.yaml  and the arguments here run all components in the Spark Streaming job.   I thought you said hundreds of thousands of records...  100 records per second is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components with CPU and memory configurations to accommodate for your data volume and querying needs.   Let's look at the  custom Receiver code  that generates the data.    private def receive(): Unit = {\n    nextIntervalStart = System.currentTimeMillis()\n    while (!isStopped) {\n      val timeNow = System.currentTimeMillis()\n      // Only emit if we are still in the interval and haven't gone over our per period max\n      if (timeNow  = nextIntervalStart   generatedThisPeriod   maxPerPeriod) {\n        store(generateRecord())\n        generatedThisPeriod += 1\n      }\n      if (timeNow   nextIntervalStart) {\n        logger.info( Generated {} tuples out of {} , generatedThisPeriod, maxPerPeriod)\n        nextIntervalStart = timeNow + period\n        generatedThisPeriod = 0\n        periodCount += 1\n      }\n      // It is courteous to sleep for a short time.\n      try {\n        Thread.sleep(1)\n      } catch {\n        case e: InterruptedException =  logger.error( Error:  , e)\n      }\n    }\n  }  This method above emits the data. This method is wrapped in a thread that is called by the Spark framework. This function only emits at most the given maximum tuples per period.    private def makeRandomMap: Map[java.lang.String, java.lang.String] = {\n    val randomMap = new HashMap[java.lang.String, java.lang.String](2)\n    randomMap.put(RandomReceiver.RANDOM_MAP_KEY_A, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n    randomMap.put(RandomReceiver.RANDOM_MAP_KEY_B, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n    randomMap\n  }\n\n  private def generateRecord(): BulletRecord = {\n    val record = new SimpleBulletRecord()\n    val uuid = UUID.randomUUID().toString\n    record.setString(RandomReceiver.STRING, uuid)\n    record.setLong(RandomReceiver.LONG, generatedThisPeriod)\n    record.setDouble(RandomReceiver.DOUBLE, Random.nextDouble())\n    record.setDouble(RandomReceiver.GAUSSIAN, Random.nextGaussian())\n    record.setString(RandomReceiver.TYPE, RandomReceiver.STRING_POOL(Random.nextInt(RandomReceiver.STRING_POOL.length)))\n    record.setLong(RandomReceiver.DURATION, System.nanoTime() % RandomReceiver.INTEGER_POOL(Random.nextInt(RandomReceiver.INTEGER_POOL.length)))\n\n    // Don't use Scala Map and convert it by asJava when calling setxxxMap method in BulletRecord.\n    // It converts Scala Map to scala.collection.convert.Wrappers$MapWrapper which is not serializable in scala 2.11.x (https://issues.scala-lang.org/browse/SI-8911).\n\n    record.setStringMap(RandomReceiver.SUBTYPES_MAP, makeRandomMap);\n\n    val booleanMap = new HashMap[java.lang.String, java.lang.Boolean](4)\n    booleanMap.put(uuid.substring(0, 8), Random.nextBoolean())\n    booleanMap.put(uuid.substring(9, 13), Random.nextBoolean())\n    booleanMap.put(uuid.substring(14, 18), Random.nextBoolean())\n    booleanMap.put(uuid.substring(19, 23), Random.nextBoolean())\n    record.setBooleanMap(RandomReceiver.BOOLEAN_MAP, booleanMap)\n\n\n    val statsMap = new HashMap[java.lang.String, java.lang.Long](4)\n    statsMap.put(RandomReceiver.PERIOD_COUNT, periodCount)\n    statsMap.put(RandomReceiver.RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod)\n    statsMap.put(RandomReceiver.NANO_TIME, System.nanoTime())\n    statsMap.put(RandomReceiver.TIMESTAMP, System.nanoTime())\n    record.setLongMap(RandomReceiver.STATS_MAP, statsMap)\n\n    record.setListOfStringMap(RandomReceiver.LIST, asList(makeRandomMap, makeRandomMap))\n    record\n  }  This  generateRecord  method generates some fields randomly and inserts them into a BulletRecord (simple). Note that the BulletRecord is typed and all data must be inserted with the proper types.  This whole receiver is plugged into an implementation of the Spark DataProducer trait that Bullet Spark requires to plug in your data (as a Spark DStream) into it. You can find this class implemented  here  and reproduced below.  package com.yahoo.bullet.spark.examples\n\nimport com.yahoo.bullet.record.BulletRecord\nimport com.yahoo.bullet.spark.DataProducer\nimport com.yahoo.bullet.spark.examples.receiver.RandomReceiver\nimport com.yahoo.bullet.spark.utils.BulletSparkConfig\nimport org.apache.spark.streaming.StreamingContext\nimport org.apache.spark.streaming.dstream.DStream\n\nclass RandomProducer extends DataProducer {\n  override def getBulletRecordStream(ssc: StreamingContext, config: BulletSparkConfig): DStream[BulletRecord] = {\n    // Bullet record input stream.\n    val bulletReceiver = new RandomReceiver(config)\n    ssc.receiverStream(bulletReceiver).asInstanceOf[DStream[BulletRecord]]\n  }\n}  If you put Bullet on your data, you will need to write a DataProducer (or a full on Spark DAG if your reading is complex), that reads from your data source and emits a DStream of BulletRecords with the fields you wish to be query-able similar to this example.", 
            "title": "Spark Streaming Job"
        }, 
        {
            "location": "/quick-start/spark/#pubsub", 
            "text": "We used the  Kafka PubSub . We configured the Backend to use this PubSub by adding these settings to the YAML file that we passed to our Spark Streaming job. Notice that we set the context to  QUERY_PROCESSING  since this is the Backend.  bullet.pubsub.context.name:  QUERY_PROCESSING \nbullet.pubsub.class.name:  com.yahoo.bullet.kafka.KafkaPubSub \nbullet.pubsub.kafka.bootstrap.servers:  localhost:9092 \nbullet.pubsub.kafka.request.topic.name:  bullet.requests \nbullet.pubsub.kafka.response.topic.name:  bullet.responses   For the Web Service, we passed in a YAML file that pointed to the same Kafka topics. Notice that we set the context to  QUERY_SUBMISSION  since this is the Web Service.  bullet.pubsub.context.name:  QUERY_SUBMISSION \nbullet.pubsub.class.name:  com.yahoo.bullet.kafka.KafkaPubSub \nbullet.pubsub.kafka.bootstrap.servers:  localhost:9092 \nbullet.pubsub.kafka.request.topic.name:  bullet.requests \nbullet.pubsub.kafka.response.topic.name:  bullet.responses", 
            "title": "PubSub"
        }, 
        {
            "location": "/quick-start/spark/#web-service", 
            "text": "We launched the Web Service using two custom files - a PubSub configuration YAML file and JSON schema file.  The JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.  The following is a snippet from the  JSON file . Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using  enumerations .  [\n    {\n         name :  probability ,\n         type :  DOUBLE ,\n         description :  Generated from Random#nextDouble \n    },\n    ...\n    {\n         name :  stats ,\n         type :  MAP ,\n         subtype :  LONG ,\n         description :  This map contains some numeric information such as the current number of periods etc. ,\n         enumerations : [\n            ...\n            { name :  nano_time ,  description :  The ns time when this record was generated }\n        ]\n    },\n    {\n         name :  classifiers ,\n         type :  LIST ,\n         subtype :  MAP ,\n         description :  This contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf \n    }\n]  The contents of the  PubSub configuration file  was discussed in the  PubSub section above .", 
            "title": "Web Service"
        }, 
        {
            "location": "/quick-start/spark/#ui", 
            "text": "Finally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.  {\n   default : {\n     queryHost :  http://localhost:9999 ,\n     queryNamespace :  api/bullet ,\n     queryPath :  ws-query ,\n     queryStompRequestChannel :  /server/request ,\n     queryStompResponseChannel :  /client/response ,\n     schemaHost :  http://localhost:9999 ,\n     schemaNamespace :  api/bullet ,\n     helpLinks : [\n      {\n         name :  Tutorials ,\n         link :  https://bullet-db.github.io/ui/usage \n      }\n    ],\n     bugLink :  https://github.com/bullet-db/bullet-ui/issues ,\n     modelVersion : 3,\n     migrations : {\n       deletions :  query \n    },\n     defaultValues : {\n       aggregationMaxSize : 1024,\n       rawMaxSize : 500,\n       durationMaxSecs : 86400,\n       distributionNumberOfPoints : 11,\n       distributionQuantilePoints :  0, 0.25, 0.5, 0.75, 0.9, 1 ,\n       distributionQuantileStart : 0,\n       distributionQuantileEnd : 1,\n       distributionQuantileIncrement : 0.1,\n       windowEmitFrequencyMinSecs : 1,\n       everyForRecordBasedWindow : 1,\n       everyForTimeBasedWindow : 2,\n       sketches : {\n         countDistinctMaxEntries : 16384,\n         groupByMaxEntries : 512,\n         distributionMaxEntries : 1024,\n         distributionMaxNumberOfPoints : 200,\n         topKMaxEntries : 1024,\n         topKErrorType :  No False Negatives \n      },\n       metadataKeyMapping : {\n         querySection :  Query ,\n         windowSection :  Window ,\n         sketchSection :  Sketch ,\n         theta :  Theta ,\n         uniquesEstimate :  Uniques Estimate ,\n         queryCreationTime :  Receive Time ,\n         queryTerminationTime :  Finish Time ,\n         estimatedResult :  Was Estimated ,\n         standardDeviations :  Standard Deviations ,\n         normalizedRankError :  Normalized Rank Error ,\n         maximumCountError :  Maximum Count Error ,\n         itemsSeen :  Items Seen ,\n         minimumValue :  Minimum Value ,\n         maximumValue :  Maximum Value ,\n         windowNumber :  Number ,\n         windowSize :  Size ,\n         windowEmitTime :  Emit Time ,\n         expectedEmitTime :  Expected Emit Time \n      }\n    }\n  }\n}  Since we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no  schemaPath  because it must be the constant string  columns . If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to  schemaHost/schemaNamespace/columns .", 
            "title": "UI"
        }, 
        {
            "location": "/quick-start/storm/", 
            "text": "Quick Start on Storm\n\n\nThis section gets you running a mock instance of Bullet to play around with. The instance will run using \nBullet on Storm\n and use the \nREST Pubsub\n. Since we do not have an actual data source, we will produce some fake data and convert it into \nBullet Records\n in a \ncustom Storm spout\n. If you want to use Bullet for your data, you will need to do read and convert your data to Bullet Records in a similar manner.\n\n\nAt the end of this section, you will have:\n\n\n\n\nSetup the Bullet topology using a custom spout on \nbullet-storm-0.8.3\n\n\nSetup the \nWeb Service\n talking to the topology and serving a schema for your UI using \nbullet-service-0.3.0\n\n\nSetup the \nREST PubSub\n talking to the topology and Web Service using \nbullet-core-0.4.2\n.\n\n\nSetup the \nUI\n talking to the Web Service using \nbullet-ui-0.5.0\n\n\n\n\nPrerequisites\n\n\n\n\nYou will need to be on an Unix-based system (Mac OS X, Ubuntu ...) with \ncurl\n installed\n\n\nYou will need \nJDK 8\n installed\n\n\nYou will need enough CPU and RAM on your machine to run about 8-10 JVMs in \nserver\n mode. You should have at least 2 GB free space on your disk. We will be setting up a Storm cluster with multiple components, a couple of Jetty instances and a Node server.\n\n\n\n\nInstall Script\n\n\nSimply run:\n\n\ncurl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-storm.sh | bash\n\n\n\n\nThis will setup a local Storm cluster, a Bullet running on it, the Bullet Web Service and a Bullet UI for you. Once everything has launched, you should be able to go to the Bullet UI running locally at \nhttp://localhost:8800\n. You can then \ncontinue this guide from here\n.\n\n\n\n\nWant to DIY?\n\n\nIf you want to manually run all the commands or if the script died while doing something above (might want to perform the \nteardown\n first), you can continue below.\n\n\n\n\nManual Installation\n\n\nSetting up Storm\n\n\nTo set up a clean working environment, let's start with creating some directories.\n\n\nStep 1: Setup directories and examples\n\n\nexport BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/storm\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME\ncurl -LO https://github.com/bullet-db/bullet-db.github.io/releases/download/v0.5.2/examples_artifacts.tar.gz\ntar -xzf examples_artifacts.tar.gz\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-examples\n\n\n\n\nStep 2: Install Storm 1.2\n\n\ncd $BULLET_HOME/backend\ncurl -O http://apache.org/dist/storm/apache-storm-1.2.2/apache-storm-1.2.2.zip\nunzip apache-storm-1.2.2.zip\nexport PATH=$(pwd)/apache-storm-1.2.2/bin/:$PATH\n\n\n\n\nStep 3: Launch Storm components\n\n\nLaunch each of the following components, in order and wait for the commands to go through. You may have to do these one at a time. You will see a JVM being launched for each one and connection messages as the components communicate through Zookeeper.\n\n\nstorm dev-zookeeper \n\nstorm nimbus \n\nstorm ui \n\nstorm logviewer \n\nstorm supervisor \n\n\n\n\n\nIt may take 30-60 seconds for all the components to launch.\n\n\nOnce everything is up without errors, visit \nhttp://localhost:8080\n and see if the Storm UI loads.\n\n\n\n\nLocal mode cleanup\n\n\nIf you notice any problems while setting up storm or while relaunching a topology, it may be because some state is corrupted. When running Storm in this fashion, states and serializations are stored in \nstorm-local\n and \n/tmp/\n. You may want to \nrm -rf storm-local/* /tmp/dev-storm-zookeeper\n to clean up this state before relaunching Storm components. See the \ntear down section\n on how to kill any running instances.\n\n\n\n\nSetting up the example Bullet topology\n\n\nNow that Storm is up and running, we can put Bullet on it. We will use an example Spout that runs on Bullet 0.8.3 on our Storm cluster. The source is available \nhere\n. This was part of the artifact that you installed in Step 1.\n\n\nStep 4: Setup the Storm example\n\n\ncp $BULLET_EXAMPLES/backend/storm/* $BULLET_HOME/backend/storm\n\n\n\n\n\n\nSettings\n\n\nTake a look at bullet_settings.yaml for the settings that are being overridden for this example. You can add or change settings as you like by referring to \ncore Bullet settings in bullet_defaults.yaml\n and \nStorm settings in bullet_storm_defaults.yaml\n. In particular, we have \ncustomized these settings\n that affect the Bullet queries you can run:\n\n\nbullet.query.aggregation.raw.max.size: 500\n The max \nRAW\n records you can fetch is 500.\n\n\nbullet.query.aggregation.max.size: 1024\n The max records you can fetch for any query is 1024.\n\n\nbullet.query.aggregation.count.distinct.sketch.entries: 16384\n We can count 16384 unique values exactly. Approximates after.\n\n\nbullet.query.aggregation.group.sketch.entries: 1024\n The max unique groups can be 1024. Uniform sample after.\n\n\nbullet.query.aggregation.distribution.sketch.entries: 1024\n Determines the normalized rank error for distributions.\n\n\nbullet.query.aggregation.top.k.sketch.entries: 1024\n 0.75 times this number is the number of unique items for which counts can be done exactly. Approximates after.\n\n\nbullet.query.aggregation.distribution.max.points: 200\n The maximum number of points you can generate, use or provide for a Distribution aggregation.\n\n\n\n\n\n\nWant to tweak the example topology code?\n\n\nYou will need to clone the \nexamples repository\n and customize it. To build the examples, you'll need to install \nMaven 3\n.\n\n\ncd $BULLET_HOME \n git clone git@github.com:bullet-db/bullet-db.github.io.git\n\n\ncd bullet-db.github.io/examples/storm \n mvn package\n\n\nYou will find the \nbullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar\n in \n$BULLET_HOME/bullet-db.github.io/examples/storm/target/\n\n\nYou can also make the \nexamples_artifacts.tar.gz\n file with all the settings that is placed in \n$BULLET_EXAMPLES\n by just running \nmake\n in the \nbullet-db.github.io/examples/\n folder.\n\n\n\n\nStep 5: Launch the topology\n\n\ncd $BULLET_HOME/backend/storm \n ./launch.sh\n\n\n\n\nVisit the UI and see if the topology is up. You should see the \nDataSource\n spout begin emitting records.\n\n\n\n\nWhere is this data coming from?\n\n\nThis data is randomly generated by the \ncustom Storm spout\n that is in the example topology you just launched. In practice, your spout would read from an actual data source such as Kafka etc. See \nbelow\n for more details about this random data spout.\n\n\n\n\nSetting up the Bullet Web Service\n\n\nStep 6: Install the Bullet Web Service\n\n\ncd $BULLET_HOME/service\ncurl -Lo bullet-service.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/0.3.0/bullet-service-0.3.0-embedded.jar\ncp $BULLET_EXAMPLES/web-service/example* $BULLET_HOME/service/\n\n\n\n\nStep 7: Launch the Web Service\n\n\ncd $BULLET_HOME/service\njava -jar bullet-service.jar --bullet.pubsub.config=example_rest_pubsub_config.yaml --bullet.schema.file=example_columns.json --bullet.pubsub.builtin.rest.enabled=true --server.port=9999  --logging.path=. --logging.file=log.txt \n log.txt \n\n\n\n\n\nNote that we turned on the built-in REST pubsub in the Web Service when launching it. The REST PubSub is bundled into the Bullet API by default, so no additional jars are needed.\n\n\nYou can verify that it is up by running a Bullet query or getting the example columns through the API:\n\n\ncurl -s -H 'Content-Type: text/plain' -X POST -d '{\naggregation\n: {\nsize\n: 1}}' http://localhost:9999/api/bullet/sse-query\ncurl -s http://localhost:9999/api/bullet/columns\n\n\n\n\nSetting up the Bullet UI\n\n\nStep 8: Install Node\n\n\ncurl -s https://raw.githubusercontent.com/creationix/nvm/v0.33.1/install.sh | bash\nsource ~/.bashrc\nnvm install v6.9.4\nnvm use v6.9.4\n\n\n\n\nStep 9: Install the Bullet UI\n\n\ncd $BULLET_HOME/ui\ncurl -LO https://github.com/bullet-db/bullet-ui/releases/download/src/bullet-ui-v0.5.0.tar.gz\ntar -xzf bullet-ui-v0.5.0.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/\n\n\n\n\nStep 10: Launch the UI\n\n\nPORT=8800 node express-server.js \n\n\n\n\n\nVisit \nhttp://localhost:8800\n to query your topology with the UI. See \nUI usage\n for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.\n\n\n\n\nRunning it remotely?\n\n\nIf you access the UI from another machine than where your UI is actually running, you will need to edit \nconfig/env-settings.json\n. Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change \nlocalhost\n in your env-settings.json to point to the host name where you will hosting the UI. This will be the same as the UI host you use in the browser. You can also do a local port forward on the machine accessing the UI by running: \nssh -N -L 8800:localhost:8800 -L 9999:localhost:9999 hostname-of-the-quickstart-components 2\n1\n\n\n\n\nPlaying around with the instance\n\n\nCheck out and follow along with the \nUI Usage\n page as it shows you some queries you can run using this UI.\n\n\nTeardown\n\n\nIf you were using the \nInstall Script\n or if you don't want to manually bring down everything, you can run:\n\n\ncurl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-storm.sh | bash -s cleanup\n\n\n\n\nIf you were performing the steps yourself, you can also manually cleanup \nall the components and all the downloads\n using:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUI\n\n\npkill -f [e]xpress-server.js\n\n\n\n\n\n\nWeb Service\n\n\npkill -f [e]xample_rest_pubsub_config.yaml\n\n\n\n\n\n\nStorm\n\n\npkill -f [a]pache-storm-1.2.2\n\n\n\n\n\n\nFile System\n\n\nrm -rf $BULLET_HOME /tmp/dev-storm-zookeeper /tmp/jetty-*\n\n\n\n\n\n\n\n\nThis does \nnot\n delete \n$HOME/.nvm\n and some extra lines nvm may have added to your \n$HOME/{.profile, .bash_profile, .zshrc, .bashrc}\n.\n\n\nWhat did we do?\n\n\nThis section will go over the various custom pieces this example plugged into Bullet, so you can better understand what we did.\n\n\nStorm topology\n\n\nThe topology was the Bullet topology plugged in with a custom spout. This spout is implemented in this \nexample project\n and was already built for you when you \ndownloaded the examples\n. It does not read from any data source and just produces random, structured data. It also produces only up to a maximum number of records in a given period. Both this maximum and the length of a period are configurable. If you examine $BULLET_HOME/backend/storm/launch.sh, you'll see the following:\n\n\nstorm jar bullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf ./bullet_settings.yaml \\\n          --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout \\\n          --bullet-spout-parallelism 1 \\\n          ...\n          --bullet-spout-arg 20 \\\n          --bullet-spout-arg 101 \\\n          ...\n\n\n\n\nThis command launches the jar (an uber or \"fat\" jar) containing the custom spout code and all dependencies you copied in Step 5. We pass the name of your spout class with \n--bullet-spout com.yahoo.bullet.storm.examples.RandomSpout\n to the Bullet main class \ncom.yahoo.bullet.Topology\n with two arguments \n--bullet-spout-arg 20\n and \n--bullet-spout-arg 101\n. The first argument tells the Spout to generate at most 20 tuples (records) in a period and the second argument says a period is 101 ms long.\n\n\nThe settings defined by \n--bullet-conf ./bullet_settings.yaml\n and the arguments here run all components in the topology with a parallelism of 1. So there will be one spout that is producing ~200 rps.\n\n\n\n\nI thought you said hundreds of thousands of records...\n\n\n200 records is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components with CPU and memory configurations to accommodate for your data volume and querying needs.\n\n\n\n\nLet's look at the \ncustom spout code\n that generates the data.\n\n\n    @Override\n    public void nextTuple() {\n        long timeNow = System.nanoTime();\n        // Only emit if we are still in the interval and haven't gone over our per period max\n        if (timeNow \n= nextIntervalStart \n generatedThisPeriod \n maxPerPeriod) {\n            outputCollector.emit(new Values(generateRecord()), DUMMY_ID);\n            generatedThisPeriod++;\n            return;\n        }\n        if (timeNow \n nextIntervalStart) {\n            log.info(\nGenerated {} tuples out of {}\n, generatedThisPeriod, maxPerPeriod);\n            nextIntervalStart = timeNow + period;\n            generatedThisPeriod = 0;\n            periodCount++;\n        }\n        // It is courteous to sleep for a short time if you're not emitting anything...\n        try {\n            Thread.sleep(1);\n        } catch (InterruptedException e) {\n            log.error(\nError: \n, e);\n        }\n    }\n\n\n\n\nThis method above emits the tuples. The Storm framework calls this method. This function only emits at most the given maximum tuples per period.\n\n\n\n\nWhy a DUMMY_ID?\n\n\nWhen the spout emits the randomly generated tuple, it attaches a \nDUMMY_ID\n to it. In Storm terms, this is a message ID. By adding a message ID, this tuple can be made to flow reliably. The Bullet component that receives this tuple (Filter bolt) acknowledges or \"acks\" this tuple. If the tuple did not make it to Filter bolt within a configured timeout window, Storm will call a \nfail(Object messageId)\n method on the spout. This particular spout does not define one and hence the usage of a \nDUMMY_ID\n. If your source of data can identify records uniquely and you can re-emit them on a fail, you should attach that actual ID in place of the \nDUMMY_ID\n.\n\n\n\n\nprivate Map\nString, String\n makeRandomMap() {\n    Map\nString, String\n randomMap = new HashMap\n(2);\n    randomMap.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    randomMap.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    return randomMap;\n}\n\nprivate BulletRecord generateRecord() {\n    BulletRecord record = new AvroBulletRecord();\n    String uuid = UUID.randomUUID().toString();\n\n    record.setString(STRING, uuid);\n    record.setLong(LONG, (long) generatedThisPeriod);\n    record.setDouble(DOUBLE, random.nextDouble());\n    record.setDouble(GAUSSIAN, random.nextGaussian());\n    record.setString(TYPE, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    record.setLong(DURATION, System.currentTimeMillis() % INTEGER_POOL[random.nextInt(INTEGER_POOL.length)]);\n\n    record.setStringMap(SUBTYPES_MAP, makeRandomMap());\n\n    Map\nString, Boolean\n booleanMap = new HashMap\n(4);\n    booleanMap.put(uuid.substring(0, 8), random.nextBoolean());\n    booleanMap.put(uuid.substring(9, 13), random.nextBoolean());\n    booleanMap.put(uuid.substring(14, 18), random.nextBoolean());\n    booleanMap.put(uuid.substring(19, 23), random.nextBoolean());\n    record.setBooleanMap(BOOLEAN_MAP, booleanMap);\n\n    Map\nString, Long\n statsMap = new HashMap\n(4);\n    statsMap.put(PERIOD_COUNT, periodCount);\n    statsMap.put(RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod);\n    statsMap.put(NANO_TIME, System.nanoTime());\n    statsMap.put(TIMESTAMP, System.currentTimeMillis());\n    record.setLongMap(STATS_MAP, statsMap);\n\n    record.setListOfStringMap(LIST, asList(makeRandomMap(), makeRandomMap()));\n\n    return record;\n}\n\n\n\n\nThis \ngenerateRecord\n method generates some fields randomly and inserts them into a BulletRecord. Note that the BulletRecord is typed and all data must be inserted with the proper types.\n\n\nIf you put Bullet on your data, you will need to write a Spout (or a topology if your reading is complex), that reads from your data source and emits BulletRecords with the fields you wish to be query-able placed into a BulletRecord similar to this example.\n\n\nPubSub\n\n\nWe used the \nREST PubSub\n. Note that even though we support a DRPC PubSub, it doesn't actually support windowing so we have not used it for this example. We configured the Backend to use this PubSub by adding these settings to the YAML file that we passed to our Storm topology. Notice that we set the context to \nQUERY_PROCESSING\n since this is the Backend. We do not set \nbullet.pubsub.rest.result.url\n because each query sent to the topology has this information so that the results could be returned back to it.\n\n\nbullet.pubsub.context.name: \nQUERY_PROCESSING\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.pubsub.rest.RESTPubSub\n\nbullet.pubsub.rest.query.urls:\n    - \nhttp://localhost:9999/api/bullet/pubsub/query\n\n\n\n\n\nFor the Web Service, we passed in a YAML file that pointed to itself for the REST endpoints that serve as the PubSub interface. Notice that we set the context to \nQUERY_SUBMISSION\n since this is the Web Service.\n\n\nbullet.pubsub.context.name: \nQUERY_SUBMISSION\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.pubsub.rest.RESTPubSub\n\nbullet.pubsub.rest.query.urls:\n    - \nhttp://localhost:9999/api/bullet/pubsub/query\n\nbullet.pubsub.rest.result.url: \nhttp://localhost:9999/api/bullet/pubsub/result\n\nbullet.pubsub.rest.subscriber.connect.timeout.ms: 5000\nbullet.pubsub.rest.publisher.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.result.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10\n\n\n\n\nWeb Service\n\n\nWe launched the Web Service using two custom files - a PubSub configuration YAML file and JSON schema file.\n\n\nThe JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.\n\n\nThe following is a snippet from the \nJSON file\n. Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using \nenumerations\n.\n\n\n[\n    {\n        \nname\n: \nprobability\n,\n        \ntype\n: \nDOUBLE\n,\n        \ndescription\n: \nGenerated from Random#nextDouble\n\n    },\n    ...\n    {\n        \nname\n: \nstats_map\n,\n        \ntype\n: \nMAP\n,\n        \nsubtype\n: \nLONG\n,\n        \ndescription\n: \nThis map contains some numeric information such as the current number of periods etc.\n,\n        \nenumerations\n: [\n            ...\n            {\nname\n: \nnano_time\n, \ndescription\n: \nThe ns time when this record was generated\n}\n        ]\n    },\n    {\n        \nname\n: \nclassifiers\n,\n        \ntype\n: \nLIST\n,\n        \nsubtype\n: \nMAP\n,\n        \ndescription\n: \nThis contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf\n\n    }\n]\n\n\n\n\nThe contents of the \nPubSub configuration file\n was discussed in the \nPubSub section above\n.\n\n\nUI\n\n\nFinally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.\n\n\n{\n  \ndefault\n: {\n    \nqueryHost\n: \nhttp://localhost:9999\n,\n    \nqueryNamespace\n: \napi/bullet\n,\n    \nqueryPath\n: \nws-query\n,\n    \nqueryStompRequestChannel\n: \n/server/request\n,\n    \nqueryStompResponseChannel\n: \n/client/response\n,\n    \nschemaHost\n: \nhttp://localhost:9999\n,\n    \nschemaNamespace\n: \napi/bullet\n,\n    \nhelpLinks\n: [\n      {\n        \nname\n: \nTutorials\n,\n        \nlink\n: \nhttps://bullet-db.github.io/ui/usage\n\n      }\n    ],\n    \nbugLink\n: \nhttps://github.com/bullet-db/bullet-ui/issues\n,\n    \nmodelVersion\n: 3,\n    \nmigrations\n: {\n      \ndeletions\n: \nquery\n\n    },\n    \ndefaultValues\n: {\n      \naggregationMaxSize\n: 1024,\n      \nrawMaxSize\n: 500,\n      \ndurationMaxSecs\n: 86400,\n      \ndistributionNumberOfPoints\n: 11,\n      \ndistributionQuantilePoints\n: \n0, 0.25, 0.5, 0.75, 0.9, 1\n,\n      \ndistributionQuantileStart\n: 0,\n      \ndistributionQuantileEnd\n: 1,\n      \ndistributionQuantileIncrement\n: 0.1,\n      \nwindowEmitFrequencyMinSecs\n: 1,\n      \neveryForRecordBasedWindow\n: 1,\n      \neveryForTimeBasedWindow\n: 2,\n      \nsketches\n: {\n        \ncountDistinctMaxEntries\n: 16384,\n        \ngroupByMaxEntries\n: 512,\n        \ndistributionMaxEntries\n: 1024,\n        \ndistributionMaxNumberOfPoints\n: 200,\n        \ntopKMaxEntries\n: 1024,\n        \ntopKErrorType\n: \nNo False Negatives\n\n      },\n      \nmetadataKeyMapping\n: {\n        \nquerySection\n: \nQuery\n,\n        \nwindowSection\n: \nWindow\n,\n        \nsketchSection\n: \nSketch\n,\n        \ntheta\n: \nTheta\n,\n        \nuniquesEstimate\n: \nUniques Estimate\n,\n        \nqueryCreationTime\n: \nReceive Time\n,\n        \nqueryTerminationTime\n: \nFinish Time\n,\n        \nestimatedResult\n: \nWas Estimated\n,\n        \nstandardDeviations\n: \nStandard Deviations\n,\n        \nnormalizedRankError\n: \nNormalized Rank Error\n,\n        \nmaximumCountError\n: \nMaximum Count Error\n,\n        \nitemsSeen\n: \nItems Seen\n,\n        \nminimumValue\n: \nMinimum Value\n,\n        \nmaximumValue\n: \nMaximum Value\n,\n        \nwindowNumber\n: \nNumber\n,\n        \nwindowSize\n: \nSize\n,\n        \nwindowEmitTime\n: \nEmit Time\n,\n        \nexpectedEmitTime\n: \nExpected Emit Time\n\n      }\n    }\n  }\n}\n\n\n\n\nSince we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no \nschemaPath\n because it must be the constant string \ncolumns\n. If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to \nschemaHost/schemaNamespace/columns\n.", 
            "title": "Storm"
        }, 
        {
            "location": "/quick-start/storm/#quick-start-on-storm", 
            "text": "This section gets you running a mock instance of Bullet to play around with. The instance will run using  Bullet on Storm  and use the  REST Pubsub . Since we do not have an actual data source, we will produce some fake data and convert it into  Bullet Records  in a  custom Storm spout . If you want to use Bullet for your data, you will need to do read and convert your data to Bullet Records in a similar manner.  At the end of this section, you will have:   Setup the Bullet topology using a custom spout on  bullet-storm-0.8.3  Setup the  Web Service  talking to the topology and serving a schema for your UI using  bullet-service-0.3.0  Setup the  REST PubSub  talking to the topology and Web Service using  bullet-core-0.4.2 .  Setup the  UI  talking to the Web Service using  bullet-ui-0.5.0   Prerequisites   You will need to be on an Unix-based system (Mac OS X, Ubuntu ...) with  curl  installed  You will need  JDK 8  installed  You will need enough CPU and RAM on your machine to run about 8-10 JVMs in  server  mode. You should have at least 2 GB free space on your disk. We will be setting up a Storm cluster with multiple components, a couple of Jetty instances and a Node server.", 
            "title": "Quick Start on Storm"
        }, 
        {
            "location": "/quick-start/storm/#install-script", 
            "text": "Simply run:  curl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-storm.sh | bash  This will setup a local Storm cluster, a Bullet running on it, the Bullet Web Service and a Bullet UI for you. Once everything has launched, you should be able to go to the Bullet UI running locally at  http://localhost:8800 . You can then  continue this guide from here .   Want to DIY?  If you want to manually run all the commands or if the script died while doing something above (might want to perform the  teardown  first), you can continue below.", 
            "title": "Install Script"
        }, 
        {
            "location": "/quick-start/storm/#manual-installation", 
            "text": "", 
            "title": "Manual Installation"
        }, 
        {
            "location": "/quick-start/storm/#setting-up-storm", 
            "text": "To set up a clean working environment, let's start with creating some directories.", 
            "title": "Setting up Storm"
        }, 
        {
            "location": "/quick-start/storm/#step-1-setup-directories-and-examples", 
            "text": "export BULLET_HOME=$(pwd)/bullet-quickstart\nmkdir -p $BULLET_HOME/backend/storm\nmkdir -p $BULLET_HOME/service\nmkdir -p $BULLET_HOME/ui\ncd $BULLET_HOME\ncurl -LO https://github.com/bullet-db/bullet-db.github.io/releases/download/v0.5.2/examples_artifacts.tar.gz\ntar -xzf examples_artifacts.tar.gz\nexport BULLET_EXAMPLES=$BULLET_HOME/bullet-examples", 
            "title": "Step 1: Setup directories and examples"
        }, 
        {
            "location": "/quick-start/storm/#step-2-install-storm-12", 
            "text": "cd $BULLET_HOME/backend\ncurl -O http://apache.org/dist/storm/apache-storm-1.2.2/apache-storm-1.2.2.zip\nunzip apache-storm-1.2.2.zip\nexport PATH=$(pwd)/apache-storm-1.2.2/bin/:$PATH", 
            "title": "Step 2: Install Storm 1.2"
        }, 
        {
            "location": "/quick-start/storm/#step-3-launch-storm-components", 
            "text": "Launch each of the following components, in order and wait for the commands to go through. You may have to do these one at a time. You will see a JVM being launched for each one and connection messages as the components communicate through Zookeeper.  storm dev-zookeeper  \nstorm nimbus  \nstorm ui  \nstorm logviewer  \nstorm supervisor    It may take 30-60 seconds for all the components to launch.  Once everything is up without errors, visit  http://localhost:8080  and see if the Storm UI loads.   Local mode cleanup  If you notice any problems while setting up storm or while relaunching a topology, it may be because some state is corrupted. When running Storm in this fashion, states and serializations are stored in  storm-local  and  /tmp/ . You may want to  rm -rf storm-local/* /tmp/dev-storm-zookeeper  to clean up this state before relaunching Storm components. See the  tear down section  on how to kill any running instances.", 
            "title": "Step 3: Launch Storm components"
        }, 
        {
            "location": "/quick-start/storm/#setting-up-the-example-bullet-topology", 
            "text": "Now that Storm is up and running, we can put Bullet on it. We will use an example Spout that runs on Bullet 0.8.3 on our Storm cluster. The source is available  here . This was part of the artifact that you installed in Step 1.", 
            "title": "Setting up the example Bullet topology"
        }, 
        {
            "location": "/quick-start/storm/#step-4-setup-the-storm-example", 
            "text": "cp $BULLET_EXAMPLES/backend/storm/* $BULLET_HOME/backend/storm   Settings  Take a look at bullet_settings.yaml for the settings that are being overridden for this example. You can add or change settings as you like by referring to  core Bullet settings in bullet_defaults.yaml  and  Storm settings in bullet_storm_defaults.yaml . In particular, we have  customized these settings  that affect the Bullet queries you can run:  bullet.query.aggregation.raw.max.size: 500  The max  RAW  records you can fetch is 500.  bullet.query.aggregation.max.size: 1024  The max records you can fetch for any query is 1024.  bullet.query.aggregation.count.distinct.sketch.entries: 16384  We can count 16384 unique values exactly. Approximates after.  bullet.query.aggregation.group.sketch.entries: 1024  The max unique groups can be 1024. Uniform sample after.  bullet.query.aggregation.distribution.sketch.entries: 1024  Determines the normalized rank error for distributions.  bullet.query.aggregation.top.k.sketch.entries: 1024  0.75 times this number is the number of unique items for which counts can be done exactly. Approximates after.  bullet.query.aggregation.distribution.max.points: 200  The maximum number of points you can generate, use or provide for a Distribution aggregation.    Want to tweak the example topology code?  You will need to clone the  examples repository  and customize it. To build the examples, you'll need to install  Maven 3 .  cd $BULLET_HOME   git clone git@github.com:bullet-db/bullet-db.github.io.git  cd bullet-db.github.io/examples/storm   mvn package  You will find the  bullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar  in  $BULLET_HOME/bullet-db.github.io/examples/storm/target/  You can also make the  examples_artifacts.tar.gz  file with all the settings that is placed in  $BULLET_EXAMPLES  by just running  make  in the  bullet-db.github.io/examples/  folder.", 
            "title": "Step 4: Setup the Storm example"
        }, 
        {
            "location": "/quick-start/storm/#step-5-launch-the-topology", 
            "text": "cd $BULLET_HOME/backend/storm   ./launch.sh  Visit the UI and see if the topology is up. You should see the  DataSource  spout begin emitting records.   Where is this data coming from?  This data is randomly generated by the  custom Storm spout  that is in the example topology you just launched. In practice, your spout would read from an actual data source such as Kafka etc. See  below  for more details about this random data spout.", 
            "title": "Step 5: Launch the topology"
        }, 
        {
            "location": "/quick-start/storm/#setting-up-the-bullet-web-service", 
            "text": "", 
            "title": "Setting up the Bullet Web Service"
        }, 
        {
            "location": "/quick-start/storm/#step-6-install-the-bullet-web-service", 
            "text": "cd $BULLET_HOME/service\ncurl -Lo bullet-service.jar http://jcenter.bintray.com/com/yahoo/bullet/bullet-service/0.3.0/bullet-service-0.3.0-embedded.jar\ncp $BULLET_EXAMPLES/web-service/example* $BULLET_HOME/service/", 
            "title": "Step 6: Install the Bullet Web Service"
        }, 
        {
            "location": "/quick-start/storm/#step-7-launch-the-web-service", 
            "text": "cd $BULLET_HOME/service\njava -jar bullet-service.jar --bullet.pubsub.config=example_rest_pubsub_config.yaml --bullet.schema.file=example_columns.json --bullet.pubsub.builtin.rest.enabled=true --server.port=9999  --logging.path=. --logging.file=log.txt   log.txt    Note that we turned on the built-in REST pubsub in the Web Service when launching it. The REST PubSub is bundled into the Bullet API by default, so no additional jars are needed.  You can verify that it is up by running a Bullet query or getting the example columns through the API:  curl -s -H 'Content-Type: text/plain' -X POST -d '{ aggregation : { size : 1}}' http://localhost:9999/api/bullet/sse-query\ncurl -s http://localhost:9999/api/bullet/columns", 
            "title": "Step 7: Launch the Web Service"
        }, 
        {
            "location": "/quick-start/storm/#setting-up-the-bullet-ui", 
            "text": "", 
            "title": "Setting up the Bullet UI"
        }, 
        {
            "location": "/quick-start/storm/#step-8-install-node", 
            "text": "curl -s https://raw.githubusercontent.com/creationix/nvm/v0.33.1/install.sh | bash\nsource ~/.bashrc\nnvm install v6.9.4\nnvm use v6.9.4", 
            "title": "Step 8: Install Node"
        }, 
        {
            "location": "/quick-start/storm/#step-9-install-the-bullet-ui", 
            "text": "cd $BULLET_HOME/ui\ncurl -LO https://github.com/bullet-db/bullet-ui/releases/download/src/bullet-ui-v0.5.0.tar.gz\ntar -xzf bullet-ui-v0.5.0.tar.gz\ncp $BULLET_EXAMPLES/ui/env-settings.json config/", 
            "title": "Step 9: Install the Bullet UI"
        }, 
        {
            "location": "/quick-start/storm/#step-10-launch-the-ui", 
            "text": "PORT=8800 node express-server.js    Visit  http://localhost:8800  to query your topology with the UI. See  UI usage  for some example queries and interactions using this UI. You see what the Schema means by visiting the Schema section.   Running it remotely?  If you access the UI from another machine than where your UI is actually running, you will need to edit  config/env-settings.json . Since the UI is a client-side app, the machine that your browser is running on will fetch the UI and attempt to use these settings to talk to the Web Service. Since they point to localhost by default, your browser will attempt to connect there and fail. An easy fix is to change  localhost  in your env-settings.json to point to the host name where you will hosting the UI. This will be the same as the UI host you use in the browser. You can also do a local port forward on the machine accessing the UI by running:  ssh -N -L 8800:localhost:8800 -L 9999:localhost:9999 hostname-of-the-quickstart-components 2 1", 
            "title": "Step 10: Launch the UI"
        }, 
        {
            "location": "/quick-start/storm/#playing-around-with-the-instance", 
            "text": "Check out and follow along with the  UI Usage  page as it shows you some queries you can run using this UI.", 
            "title": "Playing around with the instance"
        }, 
        {
            "location": "/quick-start/storm/#teardown", 
            "text": "If you were using the  Install Script  or if you don't want to manually bring down everything, you can run:  curl -sLo- https://raw.githubusercontent.com/bullet-db/bullet-db.github.io/src/examples/install-all-storm.sh | bash -s cleanup  If you were performing the steps yourself, you can also manually cleanup  all the components and all the downloads  using:           UI  pkill -f [e]xpress-server.js    Web Service  pkill -f [e]xample_rest_pubsub_config.yaml    Storm  pkill -f [a]pache-storm-1.2.2    File System  rm -rf $BULLET_HOME /tmp/dev-storm-zookeeper /tmp/jetty-*     This does  not  delete  $HOME/.nvm  and some extra lines nvm may have added to your  $HOME/{.profile, .bash_profile, .zshrc, .bashrc} .", 
            "title": "Teardown"
        }, 
        {
            "location": "/quick-start/storm/#what-did-we-do", 
            "text": "This section will go over the various custom pieces this example plugged into Bullet, so you can better understand what we did.", 
            "title": "What did we do?"
        }, 
        {
            "location": "/quick-start/storm/#storm-topology", 
            "text": "The topology was the Bullet topology plugged in with a custom spout. This spout is implemented in this  example project  and was already built for you when you  downloaded the examples . It does not read from any data source and just produces random, structured data. It also produces only up to a maximum number of records in a given period. Both this maximum and the length of a period are configurable. If you examine $BULLET_HOME/backend/storm/launch.sh, you'll see the following:  storm jar bullet-storm-example-1.0-SNAPSHOT-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf ./bullet_settings.yaml \\\n          --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout \\\n          --bullet-spout-parallelism 1 \\\n          ...\n          --bullet-spout-arg 20 \\\n          --bullet-spout-arg 101 \\\n          ...  This command launches the jar (an uber or \"fat\" jar) containing the custom spout code and all dependencies you copied in Step 5. We pass the name of your spout class with  --bullet-spout com.yahoo.bullet.storm.examples.RandomSpout  to the Bullet main class  com.yahoo.bullet.Topology  with two arguments  --bullet-spout-arg 20  and  --bullet-spout-arg 101 . The first argument tells the Spout to generate at most 20 tuples (records) in a period and the second argument says a period is 101 ms long.  The settings defined by  --bullet-conf ./bullet_settings.yaml  and the arguments here run all components in the topology with a parallelism of 1. So there will be one spout that is producing ~200 rps.   I thought you said hundreds of thousands of records...  200 records is not Big Data by any stretch of the imagination but this Quick Start is running everything on one machine and is meant to introduce you to what Bullet does. In practice, you would scale and run your components with CPU and memory configurations to accommodate for your data volume and querying needs.   Let's look at the  custom spout code  that generates the data.      @Override\n    public void nextTuple() {\n        long timeNow = System.nanoTime();\n        // Only emit if we are still in the interval and haven't gone over our per period max\n        if (timeNow  = nextIntervalStart   generatedThisPeriod   maxPerPeriod) {\n            outputCollector.emit(new Values(generateRecord()), DUMMY_ID);\n            generatedThisPeriod++;\n            return;\n        }\n        if (timeNow   nextIntervalStart) {\n            log.info( Generated {} tuples out of {} , generatedThisPeriod, maxPerPeriod);\n            nextIntervalStart = timeNow + period;\n            generatedThisPeriod = 0;\n            periodCount++;\n        }\n        // It is courteous to sleep for a short time if you're not emitting anything...\n        try {\n            Thread.sleep(1);\n        } catch (InterruptedException e) {\n            log.error( Error:  , e);\n        }\n    }  This method above emits the tuples. The Storm framework calls this method. This function only emits at most the given maximum tuples per period.   Why a DUMMY_ID?  When the spout emits the randomly generated tuple, it attaches a  DUMMY_ID  to it. In Storm terms, this is a message ID. By adding a message ID, this tuple can be made to flow reliably. The Bullet component that receives this tuple (Filter bolt) acknowledges or \"acks\" this tuple. If the tuple did not make it to Filter bolt within a configured timeout window, Storm will call a  fail(Object messageId)  method on the spout. This particular spout does not define one and hence the usage of a  DUMMY_ID . If your source of data can identify records uniquely and you can re-emit them on a fail, you should attach that actual ID in place of the  DUMMY_ID .   private Map String, String  makeRandomMap() {\n    Map String, String  randomMap = new HashMap (2);\n    randomMap.put(RANDOM_MAP_KEY_A, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    randomMap.put(RANDOM_MAP_KEY_B, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    return randomMap;\n}\n\nprivate BulletRecord generateRecord() {\n    BulletRecord record = new AvroBulletRecord();\n    String uuid = UUID.randomUUID().toString();\n\n    record.setString(STRING, uuid);\n    record.setLong(LONG, (long) generatedThisPeriod);\n    record.setDouble(DOUBLE, random.nextDouble());\n    record.setDouble(GAUSSIAN, random.nextGaussian());\n    record.setString(TYPE, STRING_POOL[random.nextInt(STRING_POOL.length)]);\n    record.setLong(DURATION, System.currentTimeMillis() % INTEGER_POOL[random.nextInt(INTEGER_POOL.length)]);\n\n    record.setStringMap(SUBTYPES_MAP, makeRandomMap());\n\n    Map String, Boolean  booleanMap = new HashMap (4);\n    booleanMap.put(uuid.substring(0, 8), random.nextBoolean());\n    booleanMap.put(uuid.substring(9, 13), random.nextBoolean());\n    booleanMap.put(uuid.substring(14, 18), random.nextBoolean());\n    booleanMap.put(uuid.substring(19, 23), random.nextBoolean());\n    record.setBooleanMap(BOOLEAN_MAP, booleanMap);\n\n    Map String, Long  statsMap = new HashMap (4);\n    statsMap.put(PERIOD_COUNT, periodCount);\n    statsMap.put(RECORD_NUMBER, periodCount * maxPerPeriod + generatedThisPeriod);\n    statsMap.put(NANO_TIME, System.nanoTime());\n    statsMap.put(TIMESTAMP, System.currentTimeMillis());\n    record.setLongMap(STATS_MAP, statsMap);\n\n    record.setListOfStringMap(LIST, asList(makeRandomMap(), makeRandomMap()));\n\n    return record;\n}  This  generateRecord  method generates some fields randomly and inserts them into a BulletRecord. Note that the BulletRecord is typed and all data must be inserted with the proper types.  If you put Bullet on your data, you will need to write a Spout (or a topology if your reading is complex), that reads from your data source and emits BulletRecords with the fields you wish to be query-able placed into a BulletRecord similar to this example.", 
            "title": "Storm topology"
        }, 
        {
            "location": "/quick-start/storm/#pubsub", 
            "text": "We used the  REST PubSub . Note that even though we support a DRPC PubSub, it doesn't actually support windowing so we have not used it for this example. We configured the Backend to use this PubSub by adding these settings to the YAML file that we passed to our Storm topology. Notice that we set the context to  QUERY_PROCESSING  since this is the Backend. We do not set  bullet.pubsub.rest.result.url  because each query sent to the topology has this information so that the results could be returned back to it.  bullet.pubsub.context.name:  QUERY_PROCESSING \nbullet.pubsub.class.name:  com.yahoo.bullet.pubsub.rest.RESTPubSub \nbullet.pubsub.rest.query.urls:\n    -  http://localhost:9999/api/bullet/pubsub/query   For the Web Service, we passed in a YAML file that pointed to itself for the REST endpoints that serve as the PubSub interface. Notice that we set the context to  QUERY_SUBMISSION  since this is the Web Service.  bullet.pubsub.context.name:  QUERY_SUBMISSION \nbullet.pubsub.class.name:  com.yahoo.bullet.pubsub.rest.RESTPubSub \nbullet.pubsub.rest.query.urls:\n    -  http://localhost:9999/api/bullet/pubsub/query \nbullet.pubsub.rest.result.url:  http://localhost:9999/api/bullet/pubsub/result \nbullet.pubsub.rest.subscriber.connect.timeout.ms: 5000\nbullet.pubsub.rest.publisher.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.result.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10", 
            "title": "PubSub"
        }, 
        {
            "location": "/quick-start/storm/#web-service", 
            "text": "We launched the Web Service using two custom files - a PubSub configuration YAML file and JSON schema file.  The JSON columns file contains the schema for our data specified in JSON. Since our schema is not going to change, we use the Web Service to serve it from a file. If your schema changes dynamically, you will need to provide your own endpoint to the UI.  The following is a snippet from the  JSON file . Notice how the types of the fields are specified. Also, if you have generated BulletRecord with Map fields whose keys are known, you can specify them here using  enumerations .  [\n    {\n         name :  probability ,\n         type :  DOUBLE ,\n         description :  Generated from Random#nextDouble \n    },\n    ...\n    {\n         name :  stats_map ,\n         type :  MAP ,\n         subtype :  LONG ,\n         description :  This map contains some numeric information such as the current number of periods etc. ,\n         enumerations : [\n            ...\n            { name :  nano_time ,  description :  The ns time when this record was generated }\n        ]\n    },\n    {\n         name :  classifiers ,\n         type :  LIST ,\n         subtype :  MAP ,\n         description :  This contains two maps, each with: field_A and field_B whose values are randomly chosen from: foo, bar, baz, qux, quux, norf \n    }\n]  The contents of the  PubSub configuration file  was discussed in the  PubSub section above .", 
            "title": "Web Service"
        }, 
        {
            "location": "/quick-start/storm/#ui", 
            "text": "Finally, we configured the UI with the custom environment specific settings file. We did not add any environments since we only had the one.  {\n   default : {\n     queryHost :  http://localhost:9999 ,\n     queryNamespace :  api/bullet ,\n     queryPath :  ws-query ,\n     queryStompRequestChannel :  /server/request ,\n     queryStompResponseChannel :  /client/response ,\n     schemaHost :  http://localhost:9999 ,\n     schemaNamespace :  api/bullet ,\n     helpLinks : [\n      {\n         name :  Tutorials ,\n         link :  https://bullet-db.github.io/ui/usage \n      }\n    ],\n     bugLink :  https://github.com/bullet-db/bullet-ui/issues ,\n     modelVersion : 3,\n     migrations : {\n       deletions :  query \n    },\n     defaultValues : {\n       aggregationMaxSize : 1024,\n       rawMaxSize : 500,\n       durationMaxSecs : 86400,\n       distributionNumberOfPoints : 11,\n       distributionQuantilePoints :  0, 0.25, 0.5, 0.75, 0.9, 1 ,\n       distributionQuantileStart : 0,\n       distributionQuantileEnd : 1,\n       distributionQuantileIncrement : 0.1,\n       windowEmitFrequencyMinSecs : 1,\n       everyForRecordBasedWindow : 1,\n       everyForTimeBasedWindow : 2,\n       sketches : {\n         countDistinctMaxEntries : 16384,\n         groupByMaxEntries : 512,\n         distributionMaxEntries : 1024,\n         distributionMaxNumberOfPoints : 200,\n         topKMaxEntries : 1024,\n         topKErrorType :  No False Negatives \n      },\n       metadataKeyMapping : {\n         querySection :  Query ,\n         windowSection :  Window ,\n         sketchSection :  Sketch ,\n         theta :  Theta ,\n         uniquesEstimate :  Uniques Estimate ,\n         queryCreationTime :  Receive Time ,\n         queryTerminationTime :  Finish Time ,\n         estimatedResult :  Was Estimated ,\n         standardDeviations :  Standard Deviations ,\n         normalizedRankError :  Normalized Rank Error ,\n         maximumCountError :  Maximum Count Error ,\n         itemsSeen :  Items Seen ,\n         minimumValue :  Minimum Value ,\n         maximumValue :  Maximum Value ,\n         windowNumber :  Number ,\n         windowSize :  Size ,\n         windowEmitTime :  Emit Time ,\n         expectedEmitTime :  Expected Emit Time \n      }\n    }\n  }\n}  Since we served our schema through the same Web Service as our queries, both these point to our Web Service. Note that there is no  schemaPath  because it must be the constant string  columns . If you define a custom endpoint for your schema, you must ensure that it can be obtained by making a GET request to  schemaHost/schemaNamespace/columns .", 
            "title": "UI"
        }, 
        {
            "location": "/backend/ingestion/", 
            "text": "Data Ingestion\n\n\nBullet operates on a generic data container that it understands. In order to get Bullet to operate on your data, you need to convert your data records into this format. This conversion is usually done when you plug in your data source into Bullet. Bullet does not make any assumptions on where you get this data from. It could be \nKafka\n, \nRabbitMQ\n, or something else.\n\n\n\n\nIf you are trying to set up Bullet...\n\n\nThe rest of this page gives more information about the Record container and how to depend on it in code directly. If you are setting up Bullet, the Record is already included by default with the Bullet artifact. You can head on over to \nsetting up the Storm topology\n to build the piece that gets your data into the Record container.\n\n\n\n\nBullet Record\n\n\nThe Bullet backend processes data that must be stored in a \nBullet Record\n which is an abstract Java class that can\nbe implemented as to be optimized for different backends or use-cases.\n\n\nThere are currently two concrete implementations of BulletRecord:\n\n\n\n\nSimpleBulletRecord\n which is based on a simple Java HashMap\n\n\nAvroBulletRecord\n which uses \nAvro\n for serialization\n\n\n\n\nTypes\n\n\nData placed into a Bullet Record is strongly typed. We support these types currently:\n\n\nPrimitives\n\n\n\n\nBoolean\n\n\nInteger\n\n\nLong\n\n\nFloat\n\n\nDouble\n\n\nString\n\n\n\n\nComplex\n\n\n\n\nMap of Strings to any of the \nPrimitives\n\n\nMap of Strings to any Map in 1\n\n\nList of any Map in 1\n\n\n\n\nWith these types, it is unlikely you would have data that cannot be represented as Bullet Record but if you do, please let us know and we are more than willing to accommodate.\n\n\nInstalling the Record directly\n\n\nGenerally, you depend on the Bullet Core artifact for your Stream Processor when you plug in the piece that gets your data into the Stream processor. The Bullet Core artifact already brings in the Bullet Record containers as well. See the usage for the \nStorm\n for an example.\n\n\nHowever, if you need it, the artifacts are available through JCenter to depend on them in code directly. You will need to add the repository. Below is a Maven example:\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-record\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nIf you just need the jar artifact, you can download it directly from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the sources or the javadoc.", 
            "title": "Getting your data into Bullet"
        }, 
        {
            "location": "/backend/ingestion/#data-ingestion", 
            "text": "Bullet operates on a generic data container that it understands. In order to get Bullet to operate on your data, you need to convert your data records into this format. This conversion is usually done when you plug in your data source into Bullet. Bullet does not make any assumptions on where you get this data from. It could be  Kafka ,  RabbitMQ , or something else.   If you are trying to set up Bullet...  The rest of this page gives more information about the Record container and how to depend on it in code directly. If you are setting up Bullet, the Record is already included by default with the Bullet artifact. You can head on over to  setting up the Storm topology  to build the piece that gets your data into the Record container.", 
            "title": "Data Ingestion"
        }, 
        {
            "location": "/backend/ingestion/#bullet-record", 
            "text": "The Bullet backend processes data that must be stored in a  Bullet Record  which is an abstract Java class that can\nbe implemented as to be optimized for different backends or use-cases.  There are currently two concrete implementations of BulletRecord:   SimpleBulletRecord  which is based on a simple Java HashMap  AvroBulletRecord  which uses  Avro  for serialization", 
            "title": "Bullet Record"
        }, 
        {
            "location": "/backend/ingestion/#types", 
            "text": "Data placed into a Bullet Record is strongly typed. We support these types currently:", 
            "title": "Types"
        }, 
        {
            "location": "/backend/ingestion/#primitives", 
            "text": "Boolean  Integer  Long  Float  Double  String", 
            "title": "Primitives"
        }, 
        {
            "location": "/backend/ingestion/#complex", 
            "text": "Map of Strings to any of the  Primitives  Map of Strings to any Map in 1  List of any Map in 1   With these types, it is unlikely you would have data that cannot be represented as Bullet Record but if you do, please let us know and we are more than willing to accommodate.", 
            "title": "Complex"
        }, 
        {
            "location": "/backend/ingestion/#installing-the-record-directly", 
            "text": "Generally, you depend on the Bullet Core artifact for your Stream Processor when you plug in the piece that gets your data into the Stream processor. The Bullet Core artifact already brings in the Bullet Record containers as well. See the usage for the  Storm  for an example.  However, if you need it, the artifacts are available through JCenter to depend on them in code directly. You will need to add the repository. Below is a Maven example:  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-record /artifactId \n   version ${bullet.version} /version  /dependency   If you just need the jar artifact, you can download it directly from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the sources or the javadoc.", 
            "title": "Installing the Record directly"
        }, 
        {
            "location": "/backend/storm-architecture/", 
            "text": "Storm architecture\n\n\nThis section describes how the \nBackend architecture\n is implemented in Storm.\n\n\nTopology\n\n\nFor Bullet on Storm, the Storm topology implements the backend piece from the full \nArchitecture\n. The topology is implemented with the standard Storm spout and bolt components:\n\n\n\n\nThe components in \nArchitecture\n have direct counterparts here. The Query spouts reading from the PubSub layer using plugged-in PubSub consumers make up the Request Processor. The Filter bolts and your plugin for your source of data (generally a spout but could be a topology) make up the Data Processor. The Join bolt and the Result bolt make up the Combiner.\n\n\nThe red colored lines are the path for the queries that come in through the PubSub and the blue is for the data from your data source. The pattern on the lines denote how the data (Storm tuples) is moved to the next component. Dashed indicates a broadcast (sent to all instances of the component), dotted indicates a key grouping (sent to a particular instance based on hashing on a particular field), and solid indicates a shuffle (randomly sent to an instance).\n\n\n\n\nWhat's a Ticker?\n\n\nThe Ticker component is attached to the Filter and Join Bolts produce Storm tuples at predefined intervals. This is a Storm feature (and is configurable when you launch the Bullet topology). These tuples, called tick tuples, behave like a CPU clock cycles for Bullet. Bullet performs all its system related activities on a tick. This includes purging stale queries, emitting left over data for queries, etc. We could have gone the route of having asynchronous threads that do the same thing but this was a far more simpler solution. The downside is that Bullet is as fast or as slow as its tick period, which can only go as low at 1 s in Storm. In practice, this means that your window is longer by a tick and you can accommodate that in your query if you wish.\n\n\nAs a practical example of how Bullet uses ticks: when the final data is emitted from the Filter bolts when the query has expired, the Join bolt receiving it waits for 3 (this is configurable) ticks after \nits query\n expires to collect all the last intermediate results from the Filter bolts. If the tick period is set as high as 5 s, this means that a query will take 3 * 15 or 15 s to get back after its expiry! Setting it to 1 s, makes it 1 * 3 s. By changing the number of ticks that the Join bolt waits for and the tick period, you can get to any integral delay \n= 1 s.\n\n\n\n\nData processing\n\n\nBullet can accept arbitrary sources of data as long as they can be read from Storm. You can either:\n\n\n\n\nWrite a Storm spout that reads your data from where ever it is (Kafka etc) and \nconverts it to Bullet Records\n. See \nQuick Start\n for an example.\n\n\nHook up an existing topology that is doing something else directly to Bullet. You will still write and hook up a component that converts your data into Bullet Records in your existing topology.\n\n\n\n\n\n\n\n\n\n\n\n\nPros\n\n\nCons\n\n\n\n\n\n\n\n\n\n\nOption 1\n\n\nVery simple to get started. Just implement a spout\n\n\nNeed a storage layer that your spout pulls or some system has to push to your spouts\n\n\n\n\n\n\nOption 2\n\n\nSaves a persistence layer\n\n\nTies your topology to Bullet directly, making it affected by Storm Backpressure etc\n\n\n\n\n\n\nOption 2\n\n\nYou can add bolts to do more processing on your data before sending it to Bullet\n\n\nIncreases the complexity of the topology\n\n\n\n\n\n\n\n\nYour data is then emitted to the Filter bolt.  If you have no queries in your system, the Filter bolt will promptly drop all Bullet Records and do absolutely nothing. If there are queries in the Filter bolt, the record is checked against the \nfilters\n in each query and if it matches, it is processed by the query. Each query type can choose to emit matched records in micro-batches. By default, \nRAW\n or \nLIMIT\n queries do not micro-batch. Each matched record up to the maximum for the query is emitted at once at the Filter bolt. Queries that aggregate, on the other hand, keep the query around till its duration is up and emit the local result. This is because these queries \ncannot\n return till they see all the data in your time window anyway because some late arriving data may update an existing aggregate. When the upcoming incremental results lands, queries will periodically (configurable) emit their intermediate results for combining in the Join bolt.\n\n\n\n\nWhy support micro-batching?\n\n\nRAW\n queries do not micro-batch by default, which makes Bullet really snappy when running those queries. As soon as your maximum record limit is reached, the query immediately returns. You can use a setting in \nbullet_defaults.yaml\n to turn on batching if you like. In the near future, micro-batching will let Bullet provide incremental results - partial results arrive over the duration of the query. Bullet can emit intermediate aggregations as they are all \nadditive\n.\n\n\n\n\nRequest processing\n\n\nThe Query spouts fetch Bullet queries through the PubSub layer using the Subscribers provided by the plugged in PubSub layer. The queries received through the PubSub also contain information about the query such as its unique identifier and potentially other metadata. The Query spouts broadcasts the query body to every Filter bolt. Since every Filter bolt has a copy of every query, the shuffled data from the source of data can be compared against the query no matter which particular Filter bolt it ends up at. Each Filter bolt has access to the unique query id and is able to key group by the id to the Join bolt with the intermediate results for the query.\n\n\nThe Query spout also key groups the query and additional query metadata to the Join bolts. This means that the query and the metadata will be end up at exactly one Join bolt.\n\n\nCombining\n\n\nSince the data from the Query spout (query and metadata) and the data from all Filter bolts (intermediate results) is key grouped by the unique query id, only one particular Join bolt receives both the query and the intermediate results for a particular query. The Join bolt can then combine the intermediate results and produce a final result. This result is joined (hence the name) along with the metadata for the query and is shuffled to the Result bolts. This bolt then uses the particular Publisher from the plugged in PubSub layer and uses the metadata if it needs to and sends the results back through the PubSub layer to the requestor.\n\n\n\n\nCombining and operations\n\n\nIn order to be able to combine intermediate results and process data in any order, all aggregations that Bullet does need to be associative and have an identity. In other words, they need to be \nMonoids\n. Luckily for us, the \nDataSketches\n that we use are monoids when exact (\nCOUNT DISTINCT\n and \nGROUP BY\n actually are commutative monoids). Sketches can be unioned and thus all the aggregations we support - \nSUM\n, \nCOUNT\n, \nMIN\n, \nMAX\n, \nAVG\n, \nCOUNT DISTINCT\n, \nDISTINCT\n etc - are monoidal. (\nAVG\n is monoidal if you store a \nSUM\n and a \nCOUNT\n instead). When \nDISTRIBUTION\n and \nTOP K\n Sketches are approximating, they may end up not being associative since they depend on the distribution of the data but you can think of them this way if you include their defined error functions bounding the result of the operation.\n\n\n\n\nScalability\n\n\nThe topology set up this way scales horizontally and has some nice properties:\n\n\n\n\nIf you want to scale for processing more data but the same amount of queries, you only need to scale the components that read your data (the spout reading the data or your custom topology) and the Filter bolts.\n\n\nIf you want to scale for more queries but the same amount of data, you generally need to scale up the Filter Bolts. If you need it, you can scale the Query spouts, Join bolts and Result bolts. You should ensure that your PubSub layer (if you're using the Storm DRPC PubSub layer, then this is the number of DRPC servers in your Storm cluster) can handle the volume of queries and results being sent through it. These components generally have low parallelisms compared to your data processing components since the data volume is generally much higher than your query volume, so this is generally not needed.\n\n\n\n\nSee \nScaling for more Queries\n and \nScaling for more Data\n for more details.\n\n\n\n\nMore queries and Filter bolts\n\n\nIf you send more queries to the Filter bolt, it will be limited by at most how many queries a Filter bolt can store and still process data efficiently. Factors like CPU, memory allocations etc for the Filter bolts come in to the picture in addition to the parallelism. Generally, if you have allocated enough Filter bolts to process your data with enough head room, this should let you run hundreds of queries simultaneously before you run into these issues. In practice, since most usage is expected to be on large data volumes and 100s of queries simultaneously, you will need to scale the the Filter bolts out so that they are not slowed down by the large number of queries in each.", 
            "title": "Architecture"
        }, 
        {
            "location": "/backend/storm-architecture/#storm-architecture", 
            "text": "This section describes how the  Backend architecture  is implemented in Storm.", 
            "title": "Storm architecture"
        }, 
        {
            "location": "/backend/storm-architecture/#topology", 
            "text": "For Bullet on Storm, the Storm topology implements the backend piece from the full  Architecture . The topology is implemented with the standard Storm spout and bolt components:   The components in  Architecture  have direct counterparts here. The Query spouts reading from the PubSub layer using plugged-in PubSub consumers make up the Request Processor. The Filter bolts and your plugin for your source of data (generally a spout but could be a topology) make up the Data Processor. The Join bolt and the Result bolt make up the Combiner.  The red colored lines are the path for the queries that come in through the PubSub and the blue is for the data from your data source. The pattern on the lines denote how the data (Storm tuples) is moved to the next component. Dashed indicates a broadcast (sent to all instances of the component), dotted indicates a key grouping (sent to a particular instance based on hashing on a particular field), and solid indicates a shuffle (randomly sent to an instance).   What's a Ticker?  The Ticker component is attached to the Filter and Join Bolts produce Storm tuples at predefined intervals. This is a Storm feature (and is configurable when you launch the Bullet topology). These tuples, called tick tuples, behave like a CPU clock cycles for Bullet. Bullet performs all its system related activities on a tick. This includes purging stale queries, emitting left over data for queries, etc. We could have gone the route of having asynchronous threads that do the same thing but this was a far more simpler solution. The downside is that Bullet is as fast or as slow as its tick period, which can only go as low at 1 s in Storm. In practice, this means that your window is longer by a tick and you can accommodate that in your query if you wish.  As a practical example of how Bullet uses ticks: when the final data is emitted from the Filter bolts when the query has expired, the Join bolt receiving it waits for 3 (this is configurable) ticks after  its query  expires to collect all the last intermediate results from the Filter bolts. If the tick period is set as high as 5 s, this means that a query will take 3 * 15 or 15 s to get back after its expiry! Setting it to 1 s, makes it 1 * 3 s. By changing the number of ticks that the Join bolt waits for and the tick period, you can get to any integral delay  = 1 s.", 
            "title": "Topology"
        }, 
        {
            "location": "/backend/storm-architecture/#data-processing", 
            "text": "Bullet can accept arbitrary sources of data as long as they can be read from Storm. You can either:   Write a Storm spout that reads your data from where ever it is (Kafka etc) and  converts it to Bullet Records . See  Quick Start  for an example.  Hook up an existing topology that is doing something else directly to Bullet. You will still write and hook up a component that converts your data into Bullet Records in your existing topology.       Pros  Cons      Option 1  Very simple to get started. Just implement a spout  Need a storage layer that your spout pulls or some system has to push to your spouts    Option 2  Saves a persistence layer  Ties your topology to Bullet directly, making it affected by Storm Backpressure etc    Option 2  You can add bolts to do more processing on your data before sending it to Bullet  Increases the complexity of the topology     Your data is then emitted to the Filter bolt.  If you have no queries in your system, the Filter bolt will promptly drop all Bullet Records and do absolutely nothing. If there are queries in the Filter bolt, the record is checked against the  filters  in each query and if it matches, it is processed by the query. Each query type can choose to emit matched records in micro-batches. By default,  RAW  or  LIMIT  queries do not micro-batch. Each matched record up to the maximum for the query is emitted at once at the Filter bolt. Queries that aggregate, on the other hand, keep the query around till its duration is up and emit the local result. This is because these queries  cannot  return till they see all the data in your time window anyway because some late arriving data may update an existing aggregate. When the upcoming incremental results lands, queries will periodically (configurable) emit their intermediate results for combining in the Join bolt.   Why support micro-batching?  RAW  queries do not micro-batch by default, which makes Bullet really snappy when running those queries. As soon as your maximum record limit is reached, the query immediately returns. You can use a setting in  bullet_defaults.yaml  to turn on batching if you like. In the near future, micro-batching will let Bullet provide incremental results - partial results arrive over the duration of the query. Bullet can emit intermediate aggregations as they are all  additive .", 
            "title": "Data processing"
        }, 
        {
            "location": "/backend/storm-architecture/#request-processing", 
            "text": "The Query spouts fetch Bullet queries through the PubSub layer using the Subscribers provided by the plugged in PubSub layer. The queries received through the PubSub also contain information about the query such as its unique identifier and potentially other metadata. The Query spouts broadcasts the query body to every Filter bolt. Since every Filter bolt has a copy of every query, the shuffled data from the source of data can be compared against the query no matter which particular Filter bolt it ends up at. Each Filter bolt has access to the unique query id and is able to key group by the id to the Join bolt with the intermediate results for the query.  The Query spout also key groups the query and additional query metadata to the Join bolts. This means that the query and the metadata will be end up at exactly one Join bolt.", 
            "title": "Request processing"
        }, 
        {
            "location": "/backend/storm-architecture/#combining", 
            "text": "Since the data from the Query spout (query and metadata) and the data from all Filter bolts (intermediate results) is key grouped by the unique query id, only one particular Join bolt receives both the query and the intermediate results for a particular query. The Join bolt can then combine the intermediate results and produce a final result. This result is joined (hence the name) along with the metadata for the query and is shuffled to the Result bolts. This bolt then uses the particular Publisher from the plugged in PubSub layer and uses the metadata if it needs to and sends the results back through the PubSub layer to the requestor.   Combining and operations  In order to be able to combine intermediate results and process data in any order, all aggregations that Bullet does need to be associative and have an identity. In other words, they need to be  Monoids . Luckily for us, the  DataSketches  that we use are monoids when exact ( COUNT DISTINCT  and  GROUP BY  actually are commutative monoids). Sketches can be unioned and thus all the aggregations we support -  SUM ,  COUNT ,  MIN ,  MAX ,  AVG ,  COUNT DISTINCT ,  DISTINCT  etc - are monoidal. ( AVG  is monoidal if you store a  SUM  and a  COUNT  instead). When  DISTRIBUTION  and  TOP K  Sketches are approximating, they may end up not being associative since they depend on the distribution of the data but you can think of them this way if you include their defined error functions bounding the result of the operation.", 
            "title": "Combining"
        }, 
        {
            "location": "/backend/storm-architecture/#scalability", 
            "text": "The topology set up this way scales horizontally and has some nice properties:   If you want to scale for processing more data but the same amount of queries, you only need to scale the components that read your data (the spout reading the data or your custom topology) and the Filter bolts.  If you want to scale for more queries but the same amount of data, you generally need to scale up the Filter Bolts. If you need it, you can scale the Query spouts, Join bolts and Result bolts. You should ensure that your PubSub layer (if you're using the Storm DRPC PubSub layer, then this is the number of DRPC servers in your Storm cluster) can handle the volume of queries and results being sent through it. These components generally have low parallelisms compared to your data processing components since the data volume is generally much higher than your query volume, so this is generally not needed.   See  Scaling for more Queries  and  Scaling for more Data  for more details.   More queries and Filter bolts  If you send more queries to the Filter bolt, it will be limited by at most how many queries a Filter bolt can store and still process data efficiently. Factors like CPU, memory allocations etc for the Filter bolts come in to the picture in addition to the parallelism. Generally, if you have allocated enough Filter bolts to process your data with enough head room, this should let you run hundreds of queries simultaneously before you run into these issues. In practice, since most usage is expected to be on large data volumes and 100s of queries simultaneously, you will need to scale the the Filter bolts out so that they are not slowed down by the large number of queries in each.", 
            "title": "Scalability"
        }, 
        {
            "location": "/backend/storm-setup/", 
            "text": "Bullet on Storm\n\n\nThis section explains how to set up and run Bullet on Storm. If you're using the Storm DRPC PubSub, refer to \nthis section\n for further details.\n\n\nConfiguration\n\n\nBullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in \nbullet_defaults.yaml\n. There are too many to list here. You can find out what these settings do in the comments listed in the defaults.\n\n\nInstallation\n\n\nTo use Bullet, you need to implement a way to read from your data source and convert your data into Bullet Records (bullet-record is a transitive dependency for Bullet and can be found \nin JCenter\n. You have two options in how to get your data into Bullet:\n\n\n\n\nYou can implement a Spout that reads from your data source and emits Bullet Record. This spout must have a constructor that takes a List of Strings.\n\n\nYou can pipe your existing Storm topology directly into Bullet. In other words, you convert the data you wish to be query-able through Bullet into Bullet Records from a bolt in your topology.\n\n\n\n\nOption 1 is the simplest to start with and should accommodate most scenarios. See \nPros and Cons\n.\n\n\nYou need a JVM based project that implements one of the two options above. You include the Bullet artifact and Storm dependencies in your pom.xml or other dependency management system. The artifacts are available through JCenter, so you will need to add the repository.\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\norg.apache.storm\n/groupId\n\n  \nartifactId\nstorm-core\n/artifactId\n\n  \nversion\n${storm.version}\n/version\n\n  \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-storm\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nIf you just need the jar artifact directly, you can download it from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the sources or javadoc. We also package up our test code where we have some helper classes to deal with \nStorm components\n. If you wish to use these to help with testing your topology, you can add another dependency on bullet-storm with \ntype\ntest-jar\n/type\n.\n\n\nIf you are going to use the second option (directly pipe data into Bullet from your Storm topology), then you will need a main class that directly calls the submit method with your wired up topology and the name of the component that is going to emit Bullet Records in that wired up topology. The submit method can be found in \nTopology.java\n. The submit method submits the topology so it should be the last thing you do in your main.\n\n\nIf you are just implementing a Spout, see the \nLaunch\n section below on how to use the main class in Bullet to create and submit your topology.\n\n\nStorm topologies are generally launched with \"fat\" jars (jar-with-dependencies), excluding storm itself:\n\n\nplugin\n\n    \ngroupId\norg.apache.maven.plugins\n/groupId\n\n    \nartifactId\nmaven-assembly-plugin\n/artifactId\n\n    \nversion\n2.4\n/version\n\n    \nexecutions\n\n        \nexecution\n\n            \nid\nassemble-all\n/id\n\n            \nphase\npackage\n/phase\n\n            \ngoals\n\n                \ngoal\nsingle\n/goal\n\n            \n/goals\n\n        \n/execution\n\n    \n/executions\n\n    \nconfiguration\n\n        \ndescriptorRefs\n\n            \ndescriptorRef\njar-with-dependencies\n/descriptorRef\n\n        \n/descriptorRefs\n\n    \n/configuration\n\n\n/plugin\n\n\n\n\n\nOlder Storm Versions\n\n\nSince package prefixes changed from \nbacktype.storm\n to \norg.apache.storm\n in Storm 1.0 and above, you will need to get the storm-0.10 version of Bullet if\nyour Storm cluster is still not at 1.0 or higher. You change your dependency to:\n\n\ndependency\n\n    \ngroupId\ncom.yahoo.bullet\n/groupId\n\n    \nartifactId\nbullet-storm-0.10\n/artifactId\n\n    \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nThe jar artifact can be downloaded directly from \nJCenter\n.\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the source or javadoc and \ntype\ntest-jar\n/type\n for the test classes as with bullet-storm.\n\n\nAlso, since storm-metrics and the Resource Aware Scheduler are not in Storm versions less than 1.0, there are changes in the Bullet settings. The settings that set the CPU and memory loads do not exist (so the config file does not specify them). The setting to enable the topology scheduler are no longer present (you can still override these settings if you run a custom version of Storm by passing it to the storm jar command. \nSee below\n.) You can take a look the settings file on the storm-0.10 branch in the Git repo.\n\n\nIf for some reason, you are running a version of Storm less than 1.0 that has the RAS back-ported to it and you wish to set the CPU and other settings, you will your own main class that mirrors the master branch of the main class but with backtype.storm packages instead.\n\n\nLaunch\n\n\nIf you have implemented your own main class (option 2 above), you just pass your main class to the storm executable as usual. If you are implementing a spout, here's an example of how you could launch the topology:\n\n\nstorm jar your-fat-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf path/to/the/bullet_settings.yaml \\\n          --bullet-spout full.package.prefix.to.your.spout.implementation \\\n          --bullet-spout-parallelism 64 \\\n          --bullet-spout-cpu-load 200.0 \\\n          --bullet-spout-on-heap-memory-load 512.0 \\\n          --bullet-spout-off-heap-memory-load 256.0 \\\n          --bullet-spout-arg arg-to-your-spout-class-for-ex-a-path-to-a-config-file \\\n          --bullet-spout-arg another-arg-to-your-spout-class \\\n          -c topology.acker.executors=64 \\\n          -c topology.max.spout.pending=10000\n\n\n\n\nYou can pass other arguments to Storm using the -c argument. The example above uses 64 ackers, for example and uses Storm's \nreliable message processing mechanisms\n. Certain components in the Bullet Storm topology cannot be reliable due to how Bullet operates currently. Hundreds of millions of Storm tuples could go into any query running in Bullet and it is intractable to \nanchor\n a single Bullet aggregation to those tuples, particularly when the results are approximate. However, you should enable acking to ensure at least once message deliveries for the hop from your topology (or spout) to the Filter bolts and for the Query spouts to the Filter and Join bolts. Ackers are lightweight so you need not have the same number of tasks as components that ack in your topology so you can tweak it accordingly. The example above also sets max spout pending to control how fast the spout emits. You could use the back-pressure mechanisms in Storm in addition or in lieu of as you choose. We have found that max spout pending gives a much more predictable way of throttling our spouts during catch up or data spikes.\n\n\n\n\nMain Class Arguments\n\n\nIf you run the main class without arguments or pass in the \n--help\n argument, you can see what these arguments mean and what others are supported.", 
            "title": "Setup"
        }, 
        {
            "location": "/backend/storm-setup/#bullet-on-storm", 
            "text": "This section explains how to set up and run Bullet on Storm. If you're using the Storm DRPC PubSub, refer to  this section  for further details.", 
            "title": "Bullet on Storm"
        }, 
        {
            "location": "/backend/storm-setup/#configuration", 
            "text": "Bullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in  bullet_defaults.yaml . There are too many to list here. You can find out what these settings do in the comments listed in the defaults.", 
            "title": "Configuration"
        }, 
        {
            "location": "/backend/storm-setup/#installation", 
            "text": "To use Bullet, you need to implement a way to read from your data source and convert your data into Bullet Records (bullet-record is a transitive dependency for Bullet and can be found  in JCenter . You have two options in how to get your data into Bullet:   You can implement a Spout that reads from your data source and emits Bullet Record. This spout must have a constructor that takes a List of Strings.  You can pipe your existing Storm topology directly into Bullet. In other words, you convert the data you wish to be query-able through Bullet into Bullet Records from a bolt in your topology.   Option 1 is the simplest to start with and should accommodate most scenarios. See  Pros and Cons .  You need a JVM based project that implements one of the two options above. You include the Bullet artifact and Storm dependencies in your pom.xml or other dependency management system. The artifacts are available through JCenter, so you will need to add the repository.  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId org.apache.storm /groupId \n   artifactId storm-core /artifactId \n   version ${storm.version} /version \n   scope provided /scope  /dependency  dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-storm /artifactId \n   version ${bullet.version} /version  /dependency   If you just need the jar artifact directly, you can download it from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the sources or javadoc. We also package up our test code where we have some helper classes to deal with  Storm components . If you wish to use these to help with testing your topology, you can add another dependency on bullet-storm with  type test-jar /type .  If you are going to use the second option (directly pipe data into Bullet from your Storm topology), then you will need a main class that directly calls the submit method with your wired up topology and the name of the component that is going to emit Bullet Records in that wired up topology. The submit method can be found in  Topology.java . The submit method submits the topology so it should be the last thing you do in your main.  If you are just implementing a Spout, see the  Launch  section below on how to use the main class in Bullet to create and submit your topology.  Storm topologies are generally launched with \"fat\" jars (jar-with-dependencies), excluding storm itself:  plugin \n     groupId org.apache.maven.plugins /groupId \n     artifactId maven-assembly-plugin /artifactId \n     version 2.4 /version \n     executions \n         execution \n             id assemble-all /id \n             phase package /phase \n             goals \n                 goal single /goal \n             /goals \n         /execution \n     /executions \n     configuration \n         descriptorRefs \n             descriptorRef jar-with-dependencies /descriptorRef \n         /descriptorRefs \n     /configuration  /plugin", 
            "title": "Installation"
        }, 
        {
            "location": "/backend/storm-setup/#older-storm-versions", 
            "text": "Since package prefixes changed from  backtype.storm  to  org.apache.storm  in Storm 1.0 and above, you will need to get the storm-0.10 version of Bullet if\nyour Storm cluster is still not at 1.0 or higher. You change your dependency to:  dependency \n     groupId com.yahoo.bullet /groupId \n     artifactId bullet-storm-0.10 /artifactId \n     version ${bullet.version} /version  /dependency   The jar artifact can be downloaded directly from  JCenter .  You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the source or javadoc and  type test-jar /type  for the test classes as with bullet-storm.  Also, since storm-metrics and the Resource Aware Scheduler are not in Storm versions less than 1.0, there are changes in the Bullet settings. The settings that set the CPU and memory loads do not exist (so the config file does not specify them). The setting to enable the topology scheduler are no longer present (you can still override these settings if you run a custom version of Storm by passing it to the storm jar command.  See below .) You can take a look the settings file on the storm-0.10 branch in the Git repo.  If for some reason, you are running a version of Storm less than 1.0 that has the RAS back-ported to it and you wish to set the CPU and other settings, you will your own main class that mirrors the master branch of the main class but with backtype.storm packages instead.", 
            "title": "Older Storm Versions"
        }, 
        {
            "location": "/backend/storm-setup/#launch", 
            "text": "If you have implemented your own main class (option 2 above), you just pass your main class to the storm executable as usual. If you are implementing a spout, here's an example of how you could launch the topology:  storm jar your-fat-jar-with-dependencies.jar \\\n          com.yahoo.bullet.Topology \\\n          --bullet-conf path/to/the/bullet_settings.yaml \\\n          --bullet-spout full.package.prefix.to.your.spout.implementation \\\n          --bullet-spout-parallelism 64 \\\n          --bullet-spout-cpu-load 200.0 \\\n          --bullet-spout-on-heap-memory-load 512.0 \\\n          --bullet-spout-off-heap-memory-load 256.0 \\\n          --bullet-spout-arg arg-to-your-spout-class-for-ex-a-path-to-a-config-file \\\n          --bullet-spout-arg another-arg-to-your-spout-class \\\n          -c topology.acker.executors=64 \\\n          -c topology.max.spout.pending=10000  You can pass other arguments to Storm using the -c argument. The example above uses 64 ackers, for example and uses Storm's  reliable message processing mechanisms . Certain components in the Bullet Storm topology cannot be reliable due to how Bullet operates currently. Hundreds of millions of Storm tuples could go into any query running in Bullet and it is intractable to  anchor  a single Bullet aggregation to those tuples, particularly when the results are approximate. However, you should enable acking to ensure at least once message deliveries for the hop from your topology (or spout) to the Filter bolts and for the Query spouts to the Filter and Join bolts. Ackers are lightweight so you need not have the same number of tasks as components that ack in your topology so you can tweak it accordingly. The example above also sets max spout pending to control how fast the spout emits. You could use the back-pressure mechanisms in Storm in addition or in lieu of as you choose. We have found that max spout pending gives a much more predictable way of throttling our spouts during catch up or data spikes.   Main Class Arguments  If you run the main class without arguments or pass in the  --help  argument, you can see what these arguments mean and what others are supported.", 
            "title": "Launch"
        }, 
        {
            "location": "/backend/storm-performance/", 
            "text": "Tuning and Performance of Bullet in Storm\n\n\nThe performance of a Bullet instance running on a multi-tenant Storm cluster has a lot of independent variables that we could vary and have an effect including:\n\n\n\n\nThe amount of data we consume\n\n\nThe number of simultaneous queries Bullet runs\n\n\nThe kinds of simultaneous queries - \nRAW\n, \nGROUP\n, \nCOUNT DISTINCT\n, etc.\n\n\nVarying parallelisms of the components - increase the parallelisms of Filter bolts disproportionately to others\n\n\nThe hardware configuration of machines\n\n\nThe various Storm versions\n\n\nHow free the cluster is and the kinds of topologies running on the cluster - CPU heavy, Disk/memory heavy, network heavy etc\n\n\nThe source of the data and tuning consumption from it\n\n\n\n\n...and many more.\n\n\nAs a streaming system, the two main features to measure is how much data it operates on and how many queries can you run simultaneously (1 and 2 above). For these results, see [performance][../performance.md]. This section will deal with determining what these are and how they vary. We will focus on 1 and 2, while keeping the others as fixed as possible. This section is to give you some insight as to what to tune to improve performance. This is not meant to be a rigorous benchmark.\n\n\nPrerequisites\n\n\nYou should be familiar with \nStorm\n, \nKafka\n and the \nBullet on Storm architecture\n.\n\n\nHow was this tested?\n\n\nAll tests run here were using \nBullet-Storm 0.4.2\n and \nBullet-Storm 0.4.3\n. We are working with just the Storm piece without going through the Web Service or the UI. The DRPC REST endpoint provided by Storm lets us do just that.\n\n\nThis particular version of Bullet on Storm was \nprior to the architecture shift\n to a PubSub layer but this would be the equivalent to using the Storm DRPC PubSub layer on a newer version of Bullet on Storm. You can replace DRPC spout and PrepareRequest bolt with Query spout and ReturnResults bolt with Result bolt conceptually. The actual implementation of the DRPC based PubSub layer just uses these spout and bolt implementations underneath anyway for the Publishers and Subscribers so the parallelisms and CPU utilizations should map 1-1.\n\n\nUsing the pluggable metrics interface in Bullet on Storm, we captured worker level metrics such as CPU time, Heap usage, GC times and types, sent them to a in-house monitoring service for time-slicing and graphing. The figures shown below use this service.\n\n\nSee \n0.3.0\n for how to plug in your own metrics collection.\n\n\nTools used\n\n\n\n\njq\n - a nice tool to parse Bullet JSON responses\n\n\ncurl, bash and python - for running and analyzing Bullet queries\n\n\n\n\nCluster\n\n\n\n\nRunning a custom build of Storm - Storm 0.10.2 with Storm 1.0+ features backported. For all intents and purposes, it's Storm 1.0.3\n\n\nThe spec for the machines we were running on:\n\n\n2 x Intel E5-2680v3 (12 core, 24 threads) - One reserved core gives each machine 47 cores from the Storm scheduler point of view\n\n\n256 GB RAM\n\n\n4 TB SATA Disk\n\n\n10 G Network Interface\n\n\n\n\n\n\nMulti-tenant cluster with other topologies running. Average cluster utilization ranged from \n70% - 90%\n\n\nThe DRPC servers in the cluster:\n\n\n2 x Intel E5620 (4 cores, 8 Threads) - 16 cores\n\n\n24 GB RAM\n\n\n1 TB SATA Disk\n\n\n10 G Network Interface\n\n\n\n\n\n\n\n\nData\n\n\n\n\nOur data was read from a Kafka cluster. We test with both Kafka 0.9.0.1 and 0.10.0.1\n\n\nThe Kafka cluster was located within the same datacenter as the Storm cluster - close network proximity gives us some measure of confidence that large data transmission delays aren't a factor.\n\n\nThe data volume can be measured by measuring how many individual records are being produced per second and what the size of the data throughput is per second.\n\n\nThere is variance in the volume of data over time as this is real data. For each of the tests below, the data volume at that time will be provided in this format: \nData: XX R/s and XX MiB/s\n, where each of the numbers are the average for each metric over the hour of when the test was done. \nR/s\n is \nRecords per second\n and \nMiB/s\n is \nMebiBytes per second\n. The data is compressed with a compression ratio of \n1.2\n.\n\n\nOur data schema contained \n92\n fields with \n62\n Strings, \n4\n Longs, \n23\n Maps and \n3\n Lists of Maps. Most of the data is generally present in the Maps and Lists of Maps.\n\n\n\n\nConfiguration\n\n\nHere is the default configuration we used to launch instances of Bullet.\n\n\nbullet.topology.metrics.enable: true\nbullet.topology.metrics.built.in.enable: true\nbullet.topology.metrics.built.in.emit.interval.mapping:\n   bullet_active_queries: 5\n   default: 60\nbullet.topology.metrics.classes:\n  - \npackage.containing.our.custom.class.pushing.metrics\n\nbullet.topology.tick.interval.secs: 1\nbullet.query.default.duration: 30000\nbullet.query.max.duration: 540000\nbullet.query.aggregation.max.size: 512\nbullet.query.aggregation.raw.max.size: 500\nbullet.query.aggregation.distribution.max.points: 200\n\n\n\n\nAny setting not listed here defaults to the defaults in \nbullet_defaults.yaml\n. In particular, \nmetadata collection\n and \ntimestamp injection\n is enabled. \nRAW\n type queries also micro-batch by size 1 (in other words, do not micro-batch).\n\n\nThe parallelisms, CPU and memory settings for the components are listed below.\n\n\nTesting on Kafka 0.9.0.1\n\n\nFor Tests 1 through 4, we read from a Kafka 0.9 cluster with the following configuration for the various Bullet components (unless specified). We use the single Spout model to read from the Kafka topic, partitioned into \n64\n partitions.\n\n\nResource utilization\n\n\n\n\n\n\n\n\n\n\n\nComponent\n\n\nParallelism\n\n\nCPU (cores)\n\n\nOn Heap Memory (MiB)\n\n\nOff Heap Memory (MiB)\n\n\nTotal Memory (MiB)\n\n\n\n\n\n\n\n\n\n\nDataSource Spout\n\n\n64\n\n\n64\n\n\n768.0\n\n\n192.0\n\n\n61440\n\n\n\n\n\n\nFilter Bolt\n\n\n128\n\n\n128\n\n\n384.0\n\n\n192.0\n\n\n73728\n\n\n\n\n\n\nJoin Bolt\n\n\n2\n\n\n1\n\n\n384.0\n\n\n192.0\n\n\n1152\n\n\n\n\n\n\nDRPC Spout\n\n\n2\n\n\n0.4\n\n\n128.0\n\n\n192.0\n\n\n640\n\n\n\n\n\n\nPrepareRequest Bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nReturnResults Bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nIMetricsConsumer\n\n\n1\n\n\n0.1\n\n\n128.0\n\n\n0\n\n\n128\n\n\n\n\n\n\nAckers\n\n\n256\n\n\n25.6\n\n\n128.0\n\n\n0\n\n\n32768\n\n\n\n\n\n\nTotal\n\n\n455\n\n\n219.5\n\n\n\n\n\n\n170496\n\n\n\n\n\n\n\n\nWith our ~47 virtual core machines, we would need \n5\n of these machines to run this instance of Bullet reading this data source and supporting a certain number of queries. What this certain number is, we will determine below.\n\n\nThe topology was also launched (command-line args to Storm) with the following Storm settings:\n\n\nstorm jar\n    ...\n    --bullet-spout-parallelism 64\n    --bullet-spout-cpu-load 100.0 \\\n    --bullet-spout-on-heap-memory-load 768.0 \\\n    --bullet-spout-off-heap-memory-load 192.0 \\\n    -c topology.acker.executors=256 \\\n    -c topology.max.spout.pending=20000 \\\n    -c topology.backpressure.enable=false \\\n    -c topology.worker.max.heap.size.mb=4096.0 \\\n    -c topology.worker.gc.childopts=\n-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:NewSize=128m -XX:CMSInitiatingOccupancyFraction=70 -XX:-CMSConcurrentMTEnabled -XX:NewRatio=1\n \\\n    ...\n\n\n\n\n\n\nThe spout parallelism is 64 because it is going to read from a Kafka topic with 64 partitions (any more is meaningless since it cannot be split further). It reads and converts the data into Bullet Records.\n\n\nWe've fanned out from the spouts to the Filter Bolts by a ratio of 2. We may or may not need this.\n\n\nWe use \ntopology.max.spout.pending=20000\n to limit the number of in-flight tuples there can be from a DataSource Spout instance and throttle it if too many queries are slowing down processing downstream. This is set pretty high to account for catch-up and skew in our Kafka partitions\n\n\nWe have set the max heap size for a worker to \n4 GiB\n since we do not want too large of a worker. If a component dies or a worker is killed by RAS, it will not affect too many other components. It also makes heap dumps etc manageable.\n\n\nWe set \ntopology.worker.gc.childopts\n to use \nParNewGC\n and \nCMS\n. These are our cluster defaults but we are listing them here since this may not be true for all Storm clusters. We have also added the \n-XX:NewRatio=1\n to the defaults since most of our objects are short-lived and having a larger Young Generation reduces our Young Generation GC (ParNew) frequency.\n\n\nWe are using 256 acker tasks. There is acking from the DataSource Spout to the Filter Bolt and from the DRPCSpout and the PrepareRequestBolt, so about ~130 components will be acking. We could get away with using much less ackers as they are very light-weight.\n\n\n\n\nTest 1: Measuring the minimum latency of Bullet\n\n\nWe are \nrunning this query\n in this test. This \nRAW\n query without any filters will serve to measure the intrinsic delay added by Bullet. The data record pulled out has a timestamp for when the record was emitted into Kafka, Bullet will inject the timestamp into the record when the Filter Bolt sends it on and the metadata collection logs timestamps for when the query was received and terminated. Using these, we can measure the end-to-end latency for getting one record through Bullet.\n\n\nResult\n\n\nThe following table shows the timestamps averaged by running \n100\n of these queries. The delays below are shown \nrelative\n to the Query Received timestamp (when the query was received by Bullet at the Join Bolt).\n\n\n\n\n\n\n\n\n\n\n\nTimestamp\n\n\nDelay (ms)\n\n\n\n\n\n\n\n\n\n\nKafka Received\n\n\n-710.75\n\n\n\n\n\n\nBullet Filtered\n\n\n-2.16\n\n\n\n\n\n\nQuery Received\n\n\n0\n\n\n\n\n\n\nQuery Finished\n\n\n1.66\n\n\n\n\n\n\n\n\nThe Bullet Filtered timestamp above is negative because the Filter Bolt received the query and emitted an arbitrary record \n2.16 ms\n before the Join Bolt received the query. The data was submitted into Kafka about \n710.75 ms\n before the query was received by Bullet and that difference is the processing time of Kafka and the time for our spouts to read the data into Bullet.\n\n\nConclusion\n\n\nBullet adds a delay of a few ms - \n1.66 ms\n in the test above - to just pull out a record. This result shows that this is the fastest Bullet can be. It cannot return data any faster than this for meaningful queries.\n\n\nTest 2: Measuring the time to find a record\n\n\nThe \nlast test\n attempted to measure how long Bullet takes to pick out a record. Here we will measure how long it takes to find a record \nthat we generate\n. This is the average of running \n100\n queries across a time interval of 30 minutes trying to filter for a record with a single unique value in a field \nsimilar to this query\n.\n\n\nWe added a timestamp into the record when the record was initially read by the DataSource Spout. Using this and the Bullet Filtered timestamp and Query Finished timestamps, we can easily track the record through Bullet.\n\n\nSince we are looking at values in the data, the average data volume across this test was: \nData: 76,000 R/s and 101 MiB/s\n\n\nResult\n\n\n\n\n\n\n\n\n\n\n\nTimestamp\n\n\nDelay (ms)\n\n\n\n\n\n\n\n\n\n\nKafka Received\n\n\n445.7\n\n\n\n\n\n\nBullet Received\n\n\n996.5\n\n\n\n\n\n\nBullet Filtered\n\n\n1003.3\n\n\n\n\n\n\nQuery Received\n\n\n0\n\n\n\n\n\n\nQuery Finished\n\n\n1006.8\n\n\n\n\n\n\n\n\nThe record was emitted into Kafka \n445.81 ms\n after the query was received. The delay is the time it takes for the generated record to flow through our network and into Kafka.\n\n\nBullet received the record \n996.5 ms\n after the query was received. The delay from when Kafka received the record to Bullet received is the delay for Kafka to make the record available for reading.\n\n\nConclusion\n\n\nWe see that Bullet took on average \n1006.8 ms - 996.5 ms\n or \n10.3 ms\n from the time it saw the record first in DataSource Spout to finishing up the query and returning it in the Join Bolt.\n\n\nTest 3: Measuring the maximum number of parallel \nRAW\n queries\n\n\nThis test runs a query similar to the \nsimple filtering query\n. The query looks for \n10\n records that match the filter or runs for \n30\n seconds, whichever comes first. The average, uncompressed record size was about \n1.8 KiB\n.\n\n\nWhat is meant by maximum?\n\n\nWe want to see how many of these queries we can have running simultaneously till the Filter Bolt is unable to process records from the spouts in time. If a Filter Bolt is unable to keep up with the rate of data produced by the spouts, our queries will not find all 10 records. Workers may start dying (killed by RAS for exceeding capacity) as well. We will be trying to find the number of queries in parallel that we can run without these happening.\n\n\nThe average data volume across this test was: \nData: 85,000 R/s and 126 MiB/s\n\n\nQuery\n\n\n{\n \nfilters\n : [\n              {\nfield\n : \nid\n, \noperation\n : \n==\n, \nvalues\n: [\nc39ba8cfe8\n]}\n             ],\n \naggregation\n : {\ntype\n : \nLIMIT\n, \nsize\n : 10},\n \nduration\n : 30000\n}\n\n\n\n\nWe will run a certain number of these queries then generate records matching this query. We will validate that we have all 10 records for all queries at the end.\n\n\nWe ran a number of queries in parallel (you may have to use \nulimit\n to change maximum user processes if you specify a large number), then generated data for the queries (the same data) and waited till the results came back. We then used jq to validate that all generated data was present. In a later test, we will actually measure the latency increase caused by increasing the queries.\n\n\n\n\nIt generates a provided number of the \nquery above\n and runs them in parallel against a randomly chosen DRPC server\n\n\nIt generates data for the query\n\n\nIt waits out the rest of the time and uses jq to validate that all the generated data was found\n\n\n\n\nResult\n\n\nWe were able to run 200 queries successfully but 300 and higher started causing our Filter Bolts to slow down. This slow down caused our spouts to be throttled and fall behind reading data. This caused the matching data to not show up in time during the queries. Some of our attempts would not return all the expected 10 records.\n\n\nUsing our metrics that were captured using our in-house metrics aggregation system (that our IMetricsConsumer publishes to), let's take a look at the CPU, Heap utilizations.\n\n\nBefore you look at the figures:\n\n\n\n\nAll the figures below are for the same time interval. The X-axis represents time in \n1 minute\n intervals\n\n\nFigure 1\n shows the number of queries running for a time interval\n\n\nThe other figures show a metric across \nall\n the workers (JVMs) in the Storm topology, each running a mix of a components (spouts reading from Kafka, Filter Bolts etc)\n\n\nThe majority of the components (excluding ackers) are spouts reading from Kafka or Filter Bolts, so the figures can be taken to be primarily describing those workers\n\n\n\n\nFigure 1. Queries running\n\n\n\n\nFigure 2. CPU user-time usage\n\n\n\n\nFigure 3. On Heap usage\n\n\n\n\nFigure 4. Garbage Collection times\n\n\n\n\nFigure 1\n shows that we first ran 100 queries, then 200, then 400 and finally 300. The numbers go over their target because we only added a 2 s buffer in our script. Network and tick delays caused some queries to not be entirely purged before the next set of N simultaneous queries came in.\n\n\nFigure 2\n shows the milliseconds of CPU time used per minute. For example, a value of \n300K ms\n ms for a line (worker) means that the worker used \n300K ms/min\n  or \n300s/60s\n or \n5\n CPU cores (virtual) in that minute.\n\n\nFigure 3\n shows raw numbers for Heap utilizations in bytes.\n\n\nFigure 4\n shows the time spent garbage collecting in ms.\n\n\n\n\nGarbage collection\n\n\nAs we increase the number of queries sent into Bullet, more objects are created in the Filter and Join Bolts. These quickly fill up our heap and cause GCs. The zig-zags represents heaps being cleared after GC and filling back up quickly. Also, note that the CPU usage is directly related to the GC times. In other words, performance is pretty much directly correlated with the amount of GC we do.\n\n\n\n\nThe following table summarizes these figures:\n\n\n\n\n\n\n\n\n\n\n\nSimultaneous Queries\n\n\nAverage CPU (ms)\n\n\nAverage Result size\n\n\n\n\n\n\n\n\n\n\n0\n\n\n90K\n\n\nN/A\n\n\n\n\n\n\n100\n\n\n130K\n\n\n10\n\n\n\n\n\n\n200\n\n\n170K\n\n\n10\n\n\n\n\n\n\n300\n\n\n230K\n\n\n8.9\n\n\n\n\n\n\n400\n\n\n270K\n\n\n7.3\n\n\n\n\n\n\n\n\nConclusion\n\n\nWe are able to run somewhere between 200 and 300 \nRAW\n queries simultaneously before losing data.\n\n\nTest 4: Improving the maximum number of simultaneous \nRAW\n queries\n\n\nThe \nlast test\n showed us that the \nstandard configuration\n lets us run somewhere from 200 and 300 \nRAW\n queries. Let's improve that. The \nGC times\n and the \nheap usage\n tell us that a larger heap may help. Also, since each of our machines has 256 GB of RAM and we have a lot of unused heap being wasted per machine (since we use up all the CPU cores), we can be use a lot more heap and not have to GC so much. Let's also try to make the worker even leaner by decreasing the max worker size so that slow components don't block others.\n\n\nChanges:\n\n\n-bullet.topology.filter.bolt.memory.on.heap.load: 384.0\n+bullet.topology.filter.bolt.memory.on.heap.load: 1024.0\n\n- --bullet-spout-on-heap-memory-load 768.0 \\\n+ --bullet-spout-on-heap-memory-load 1280.0 \\\n\n- -c topology.worker.max.heap.size.mb=4096.0 \\\n+ -c topology.worker.max.heap.size.mb=3072.0 \\\n\n\n\n\nOur resource utilization is now:\n\n\n\n\n\n\n\n\n\n\n\nComponent\n\n\nParallelism\n\n\nCPU (cores)\n\n\nOn Heap Memory (MiB)\n\n\nOff Heap Memory (MiB)\n\n\nTotal Memory (MiB)\n\n\n\n\n\n\n\n\n\n\nDataSource Spout\n\n\n64\n\n\n64\n\n\n1280.0\n\n\n192.0\n\n\n94208\n\n\n\n\n\n\nFilter Bolt\n\n\n128\n\n\n128\n\n\n1024.0\n\n\n192.0\n\n\n155648\n\n\n\n\n\n\nJoin Bolt\n\n\n2\n\n\n1\n\n\n384.0\n\n\n192.0\n\n\n1152\n\n\n\n\n\n\nDRPC Spout\n\n\n2\n\n\n0.4\n\n\n128.0\n\n\n192.0\n\n\n640\n\n\n\n\n\n\nPrepareRequest Bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nReturnResults Bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nIMetricsConsumer\n\n\n1\n\n\n0.1\n\n\n128.0\n\n\n0\n\n\n128\n\n\n\n\n\n\nAckers\n\n\n256\n\n\n25.6\n\n\n128.0\n\n\n0\n\n\n32768\n\n\n\n\n\n\nTotal\n\n\n455\n\n\n219.5\n\n\n\n\n\n\n285184\n\n\n\n\n\n\n\n\nOur data volume across this test was: \nData: 84,000 R/s and 124 MiB/s\n\n\nResult\n\n\nWith this configuration, we were able to run 700 queries simultaneously and failed at some where between 700 and 800. See below for why.\n\n\nWe notice that the GC times have improved a lot (down to ~12K ms from ~35K ms in \nFigure 4\n). While our overall CPU usage seems to have gone down since we GC a lot less, remember that our changes to the maximum worker size makes our workers run less components and as a result, use less CPU. This is why there are more lines overall (more workers).\n\n\nFigure 5. Queries running\n\n\n\n\nFigure 6. CPU user-time usage\n\n\n\n\nFigure 7. On Heap usage\n\n\n\n\nFigure 8. Garbage Collection times\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimultaneous Queries\n\n\nAverage CPU (ms)\n\n\nAverage Result size\n\n\n\n\n\n\n\n\n\n\n0\n\n\n50K\n\n\nN/A\n\n\n\n\n\n\n300\n\n\n80K\n\n\n10\n\n\n\n\n\n\n500\n\n\n95K\n\n\n10\n\n\n\n\n\n\n600\n\n\n100K\n\n\n10\n\n\n\n\n\n\n700\n\n\n100K\n\n\n10\n\n\n\n\n\n\n735\n\n\n105K\n\n\n10\n\n\n\n\n\n\n800\n\n\n105K\n\n\n9.19\n\n\n\n\n\n\n\n\nWe seem to cap out at 735 queries. This is actually due how the blocking implementation of Storm DRPC works. Storm DRPC currently dedicates a thread to each DRPC request and does not allow more requests till they are finished. For Bullet, when we ran 800 queries for the test, only the first 735 would even be sent to Bullet. The rest 65 would be sent after some of the first return but all of those 65 would return 0 records because the data that they are looking for had long since been processed. We had \n3\n DRPC servers in the cluster and you could \nscale by adding more of these\n.\n\n\nConclusion\n\n\nWith this change in heap usage, we could get to \n735\n of these queries simultaneously without any issues. We could do more but were limited by the number of DRPC servers in our cluster.\n\n\n\n\n735 is a hard limit then?\n\n\nThis is what our Storm cluster's configuration and our usage of the Storm DRPC limits us to. There is an async implementation for DRPC that we could eventually switch to. If we used another PubSub implementation like Kafka, we would be able to bypass this limit.\n\n\n\n\nTesting on Kafka 0.10.0.1\n\n\nFor this and subsequent tests, we upgraded our Kafka cluster to 0.10. We used the new Kafka consumer APIs to read \nbatches\n of messages instead of a message at a time. We changed our DataSource Spout to read batches of messages (raw bytes) instead and added a DataSource Bolt that converts each batch message into Bullet records. Switching to this model let us be a lot more efficient in our data reading.\n\n\nTo read more data, we will be trying to read a topic that is a superset of our data set so far and produces up to \n13\n times the number of records (maximum of 1.3 million records/sec) and \n20\n times the size of the data we were reading till now. This Kafka topic has \n256\n partitions.\n\n\nTest 5: Reading more Data\n\n\nOur average data volume across this test was: \nData: 756,000 R/s and 3080 MiB/s\n\n\n\n\n\n\n\n\n\n\n\nComponent\n\n\nParallelism\n\n\nCPU (cores)\n\n\nOn Heap Memory (MiB)\n\n\nOff Heap Memory (MiB)\n\n\nTotal Memory (MiB)\n\n\n\n\n\n\n\n\n\n\nDataSource Spout\n\n\n128\n\n\n128\n\n\n1024.0\n\n\n192.0\n\n\n155648\n\n\n\n\n\n\nDataSource Bolt\n\n\n256\n\n\n512\n\n\n2580.0\n\n\n192.0\n\n\n709632\n\n\n\n\n\n\nFilter Bolt\n\n\n512\n\n\n512\n\n\n1024.0\n\n\n192.0\n\n\n622592\n\n\n\n\n\n\nJoin Bolt\n\n\n2\n\n\n1\n\n\n512.0\n\n\n192.0\n\n\n1408\n\n\n\n\n\n\nDRPC Spout\n\n\n2\n\n\n0.4\n\n\n128.0\n\n\n192.0\n\n\n640\n\n\n\n\n\n\nPrepareRequest Bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nReturnResults Bolt\n\n\n1\n\n\n0.2\n\n\n128.0\n\n\n192.0\n\n\n320\n\n\n\n\n\n\nIMetricsConsumer\n\n\n4\n\n\n0.4\n\n\n128.0\n\n\n0\n\n\n512\n\n\n\n\n\n\nAckers\n\n\n256\n\n\n25.6\n\n\n128.0\n\n\n0\n\n\n32768\n\n\n\n\n\n\nTotal\n\n\n1162\n\n\n1179.8\n\n\n\n\n\n\n1523840\n\n\n\n\n\n\n\n\nIn terms of \nour machines\n, the CPU is clearly the limiting factor. We end up using about \n25\n machines to run this topology.\n\n\nWe also tweaked our GC and Storm settings. We ran the topology with the following command:\n\n\nstorm jar\n          ...\n          --bullet-conf bullet_settings.yaml \\\n          -c topology.max.spout.pending=30 \\\n          -c topology.backpressure.enable=false \\\n          -c topology.acker.executors=256 \\\n          -c topology.component.resources.onheap.memory.mb=128.0 \\\n          -c topology.worker.max.heap.size.mb=3072.0 \\\n          -c topology.worker.gc.childopts=\n-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:NewSize=128m -XX:NewRatio=2 -XX:SurvivorRatio=6 -XX:+UseCMSInitiatingOccupancyOnly -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=50 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=4\n \\\n          ...\n\n\n\n\nWe capped the GC threads to \n8\n and \n4\n, which helps performance on our 48 core machines when multiple JVMs run on a machine. We also bumped up the Young Generation size (\nNewRatio=2\n) and triggered the CMS collection to run at \n50%\n of Old Generation instead of the default of \n70%\n. Our objects are short-lived in our streaming scenario, this makes sense. We arrived at this after a few iterations, which we'll skip here for brevity.\n\n\n\n\nMax Spout Pending is now 30 ?!\n\n\nWe use \ntopology.max.spout.pending\n as a way to throttle how fast we read from Kafka. There is no acking past the Filter Bolt. The maximum number of batch messages we read is \n500\n from Kafka. This makes our true max spout pending: \n500 * 30 = 15,000\n. The tuple that is emitted from the spout is a large tuple that contains up to \n500\n records and we limit up to \n30\n of those to go unacked from any single spout before we throttle it. Since we have \n128\n spouts, we can have \n128 * 15,000\n messages unacked in the topology at any time at the most.\n\n\n\n\nResult\n\n\nWe were able to run over 600 queries with this configuration and failed in the high 600s due to hitting the DRPC limit again. With our improved reading model and GC configs, our base resource footprint has improved from \n50K\n CPU ms/min to around \n25K\n CPU ms/min.\n\n\nFigure 9. Queries running\n\n\n\n\nFigure 10. CPU user-time usage\n\n\n\n\nFigure 11. On Heap usage\n\n\n\n\nFigure 12. Garbage Collection times\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimultaneous Queries\n\n\nAverage CPU (ms)\n\n\nAverage Result size\n\n\n\n\n\n\n\n\n\n\n0\n\n\n25K\n\n\nN/A\n\n\n\n\n\n\n100\n\n\n35K\n\n\n10\n\n\n\n\n\n\n300\n\n\n60K\n\n\n10\n\n\n\n\n\n\n500\n\n\n85K\n\n\n10\n\n\n\n\n\n\n600\n\n\n90K\n\n\n10\n\n\n\n\n\n\n680\n\n\n95K\n\n\n10\n\n\n\n\n\n\n700\n\n\n95K\n\n\n9.79\n\n\n\n\n\n\n\n\nNote that the drops when we were running simultaneous queries in \nFigure 9\n are due to some\nhiccups in our collection mechanism (the collection granularity was not 5s as we configured but higher) and not actually drops in the simultaneous queries.\n\n\nConclusion\n\n\nWe are trying to read a data source that could have\n13\n times more records and \n20\n times more data volume. So we have roughly increased the parallelism of the components reading the data by 10x (\n128 + 512 = 768\n cores to read and convert the data whereas previously we were using \n64\n cores). Once this is fixed and we can read the data comfortably using our DataSource Spouts and Bolts, we can scale the Filter Bolts and other components to accommodate for queries. We set our Filter Bolt parallelism (dominates the rest of the components) to \n512\n. We need about \n25\n machines (5 times more than the previous of \n5\n).\n\n\nWith this configuration, we were able to run \n680\n queries simultaneously before we hit the DRPC limit. Since DRPC is a shared resource for the cluster, this limit is slightly lower than the previously observed number possibly due to our test environment being multi-tenant and other topologies using the shared resource.\n\n\n\n\nMeasuring latency in Bullet\n\n\nSo far, we have been using data being delayed long enough as a proxy for queries failing. \nBullet-Storm 0.4.3\n adds an average latency metric computed in the Filter Bolts. For the next tests, we add a timestamp in the Data Source spouts when the record is read and this latency metric tells us exactly how long it takes for the record to be matched against a query and acked. By setting a limit for this latency, we can much more accurately measure acceptable performance.\n\n\n\n\nTest 6: Scaling for More Data\n\n\nFor this test, we'll establish how much resources we need to read various data volumes. We want to be able to:\n\n\n\n\nRead the data\n\n\nBe able to catch up data backlogs at \n 5 : 1\n ratio (\n 5s of data in 1 s)\n\n\nSupport \n 400 RAW\n queries\n\n\nThe average latency for reading a record to filtering it be \n 200 ms\n\n\n\n\nFor reading the data, we have to first scale the DataSource spouts and bolts and then set the parallelism of the Filter bolts to support the minimum 400 queries we want at the data volume. We leave the rest of the components at their default values as seen in in \nTest 5\n.\n\n\nTo get various data volumes, we read a large Kafka topic with (256 partitions) with over 1 million R/s and sample various percentages to get less data. The sampling is done in our DataSource Spouts.\n\n\nResult\n\n\nThe following table summarizes the results:\n\n\n\n\n\n\n\n\n\n\n\nData (MiB/s, R/s)\n\n\nComponent\n\n\nParallelism\n\n\nCPU cores\n\n\nOn Heap (MiB)\n\n\nTotal CPU cores\n\n\nTotal Memory (MiB)\n\n\n\n\n\n\n\n\n\n\n307, 69700\n\n\n\n\n\n\n\n\n\n\n98.3\n\n\n123648\n\n\n\n\n\n\n\n\nDataSource Spout\n\n\n16\n\n\n0.5\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource Bolt\n\n\n32\n\n\n2\n\n\n2048\n\n\n\n\n\n\n\n\n\n\n\n\nFilter Bolt\n\n\n24\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n920, 216825\n\n\n\n\n\n\n\n\n\n\n242.7\n\n\n281856\n\n\n\n\n\n\n\n\nDataSource Spout\n\n\n32\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource Bolt\n\n\n72\n\n\n2\n\n\n2048\n\n\n\n\n\n\n\n\n\n\n\n\nFilter Bolt\n\n\n64\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n1535, 374370\n\n\n\n\n\n\n\n\n\n\n531.5\n\n\n616192\n\n\n\n\n\n\n\n\nDataSource Spout\n\n\n64\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource Bolt\n\n\n160\n\n\n2\n\n\n2048\n\n\n\n\n\n\n\n\n\n\n\n\nFilter Bolt\n\n\n144\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n2149, 524266\n\n\n\n\n\n\n\n\n\n\n812.3\n\n\n939264\n\n\n\n\n\n\n\n\nDataSource Spout\n\n\n72\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource Bolt\n\n\n256\n\n\n2\n\n\n2048\n\n\n\n\n\n\n\n\n\n\n\n\nFilter Bolt\n\n\n224\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n3070, 724390\n\n\n\n\n\n\n\n\n\n\n997.1\n\n\n1321984\n\n\n\n\n\n\n\n\nDataSource Spout\n\n\n96\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource Bolt\n\n\n320\n\n\n2\n\n\n2580\n\n\n\n\n\n\n\n\n\n\n\n\nFilter Bolt\n\n\n256\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n4024, 1004500\n\n\n\n\n\n\n\n\n\n\n1189.4\n\n\n1582208\n\n\n\n\n\n\n\n\nDataSource Spout\n\n\n96\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nDataSource Bolt\n\n\n384\n\n\n2\n\n\n2580\n\n\n\n\n\n\n\n\n\n\n\n\nFilter Bolt\n\n\n320\n\n\n1\n\n\n1024\n\n\n\n\n\n\n\n\n\n\n\n\nThe following figures graphs how the data volume relates to the total CPU and Memory needed.\n\n\nFigure 13. Data Volume vs CPU\n\n\n\n\nFigure 14. Data Volume vs Memory\n\n\n\n\nAs these figures show, Bullet scales pretty linearly for more data.  For each core of CPU core, we need about 1.2 GiB of Memory. That is the, CPU to Memory ratio is \n1 core : 1.2 GiB\n. The slopes of figures show that the CPU to data ratio is \n1 core : 850 R/s\n or \n1 core : 3.4 MiB/s\n.\n\n\nConclusion\n\n\nThis test demonstrates that Bullet scales pretty linearly for reading more data. We were able to support at least \n400\nqueries with plenty of headroom for each configuration. Each CPU core required about 1.2 GiB of Memory and gave us roughly 800 R/s or 3.4 MiB/s processing capability. Of course, this is particular to our data. As mentioned in the \ndata section\n, most of our data was concentrated in random maps. This means that in memory, the data is generally not contiguous. This creates memory fragmentation and more GC pressure. Depending on your data schema, your performance may be a lot better.\n\n\nTest 7: Scaling for More Queries\n\n\nFor this test, we'll establish how much resources we need to support more queries for a fixed data volume: \nData: 68,400 R/s and 105 MiB/s\n.\n\n\nAs our 3 server DRPC cluster currently does not let us do more than \n680 RAW\n queries, in this test, we will:\n\n\n\n\nVary the number of Filter Bolts as they are the primary bottleneck for supporting more queries.\n\n\nTo simplify things, we will only vary the \nparallelism\n and fix the CPU and memory of each Filter Bolt to \n1 Core\n and \n1 GiB Memory\n\n\nWe will use \nRAW\n queries as the queries to scale\n\n\nEach \nRAW\n query will run for \n30 s\n and search for \n10\n records that we generate. The query will actually look for \n11\n records to force it to run for the full \n30 s\n. This is because we want to stress the Filter Bolt as much as possible. As long as there is a query in the system, the Filter Bolt will deserialize and check every record that it processes\n\n\nWe will measure the same filtering latency: the time taken from the record read in DataSource Spout to its emission in the Filter Bolt. We want the maximum latency to be less than \n200 ms\n\n\n\n\nResults\n\n\nThe following table summarizes the results:\n\n\n\n\n\n\n\n\n\n\n\nFilter Bolt Parallelism\n\n\nQueries\n\n\nAverage Latency (ms)\n\n\nStatus\n\n\nTopology CPU (cores)\n\n\nTopology Memory (MiB)\n\n\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n78.3\n\n\n112256\n\n\n\n\n\n\n\n\n1\n\n\n7\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n10\n\n\n8\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n20\n\n\n8\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n30\n\n\n10\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n40\n\n\n12\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n50\n\n\n167\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n60\n\n\n1002\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n\n\n82.3\n\n\n117120\n\n\n\n\n\n\n\n\n40\n\n\n9\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n60\n\n\n9\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n80\n\n\n9\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n100\n\n\n10\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n120\n\n\n14\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n140\n\n\n18\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n160\n\n\n298\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n170\n\n\n2439\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n12\n\n\n\n\n\n\n\n\n86.3\n\n\n121984\n\n\n\n\n\n\n\n\n160\n\n\n13\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n200\n\n\n14\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n240\n\n\n78\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n280\n\n\n274\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n320\n\n\n680\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n360\n\n\n1700\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n400\n\n\n2685\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n16\n\n\n\n\n\n\n\n\n90.3\n\n\n126848\n\n\n\n\n\n\n\n\n300\n\n\n30\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n350\n\n\n44\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n400\n\n\n481\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n450\n\n\n935\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n500\n\n\n1968\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n550\n\n\n3267\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n20\n\n\n\n\n\n\n\n\n94.3\n\n\n131712\n\n\n\n\n\n\n\n\n350\n\n\n15\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n400\n\n\n18\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n450\n\n\n28\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n500\n\n\n40\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n550\n\n\n670\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n600\n\n\n1183\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n650\n\n\n1924\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n24\n\n\n\n\n\n\n\n\n98.3\n\n\n136576\n\n\n\n\n\n\n\n\n450\n\n\n17\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n500\n\n\n22\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n550\n\n\n30\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n600\n\n\n377\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n\n\n650\n\n\n490\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n28\n\n\n\n\n\n\n\n\n102.3\n\n\n141440\n\n\n\n\n\n\n\n\n550\n\n\n39\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n600\n\n\n53\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n650\n\n\n468\n\n\nFAIL\n\n\n\n\n\n\n\n\n\n\n32\n\n\n\n\n\n\n\n\n106.3\n\n\n146304\n\n\n\n\n\n\n\n\n600\n\n\n26\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\n650\n\n\n32\n\n\nOK\n\n\n\n\n\n\n\n\n\n\n\n\nThe following figure summarizes the minimum number of CPU cores (which are also the number of Filter Bolts) needed to support the the maximum number of \nRAW\n queries with latency \n 200 ms.\n\n\nFigure 15. Data Volume vs Memory\n\n\n\n\nThis shows that the queries supported also scale pretty linearly.\n\n\nYou may have noticed how when latency starts to increase, it increases pretty rapidly. This suggests that there is a \nknee\n or \nexponential\n curve for latency. The following figure shows this in the graph of the latency for queries with \n20\n Filter Bolts.\n\n\nFigure 16. Data Volume vs Memory\n\n\n\n\nConclusion\n\n\nSince Filter Bolts tend to be the most CPU intensive of the query processing components, this test measured how scaling Filter Bolts affected the number of queries that can be supported. For the fixed data volume, this relationship is linear.", 
            "title": "Performance"
        }, 
        {
            "location": "/backend/storm-performance/#tuning-and-performance-of-bullet-in-storm", 
            "text": "The performance of a Bullet instance running on a multi-tenant Storm cluster has a lot of independent variables that we could vary and have an effect including:   The amount of data we consume  The number of simultaneous queries Bullet runs  The kinds of simultaneous queries -  RAW ,  GROUP ,  COUNT DISTINCT , etc.  Varying parallelisms of the components - increase the parallelisms of Filter bolts disproportionately to others  The hardware configuration of machines  The various Storm versions  How free the cluster is and the kinds of topologies running on the cluster - CPU heavy, Disk/memory heavy, network heavy etc  The source of the data and tuning consumption from it   ...and many more.  As a streaming system, the two main features to measure is how much data it operates on and how many queries can you run simultaneously (1 and 2 above). For these results, see [performance][../performance.md]. This section will deal with determining what these are and how they vary. We will focus on 1 and 2, while keeping the others as fixed as possible. This section is to give you some insight as to what to tune to improve performance. This is not meant to be a rigorous benchmark.", 
            "title": "Tuning and Performance of Bullet in Storm"
        }, 
        {
            "location": "/backend/storm-performance/#prerequisites", 
            "text": "You should be familiar with  Storm ,  Kafka  and the  Bullet on Storm architecture .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/backend/storm-performance/#how-was-this-tested", 
            "text": "All tests run here were using  Bullet-Storm 0.4.2  and  Bullet-Storm 0.4.3 . We are working with just the Storm piece without going through the Web Service or the UI. The DRPC REST endpoint provided by Storm lets us do just that.  This particular version of Bullet on Storm was  prior to the architecture shift  to a PubSub layer but this would be the equivalent to using the Storm DRPC PubSub layer on a newer version of Bullet on Storm. You can replace DRPC spout and PrepareRequest bolt with Query spout and ReturnResults bolt with Result bolt conceptually. The actual implementation of the DRPC based PubSub layer just uses these spout and bolt implementations underneath anyway for the Publishers and Subscribers so the parallelisms and CPU utilizations should map 1-1.  Using the pluggable metrics interface in Bullet on Storm, we captured worker level metrics such as CPU time, Heap usage, GC times and types, sent them to a in-house monitoring service for time-slicing and graphing. The figures shown below use this service.  See  0.3.0  for how to plug in your own metrics collection.", 
            "title": "How was this tested?"
        }, 
        {
            "location": "/backend/storm-performance/#tools-used", 
            "text": "jq  - a nice tool to parse Bullet JSON responses  curl, bash and python - for running and analyzing Bullet queries", 
            "title": "Tools used"
        }, 
        {
            "location": "/backend/storm-performance/#cluster", 
            "text": "Running a custom build of Storm - Storm 0.10.2 with Storm 1.0+ features backported. For all intents and purposes, it's Storm 1.0.3  The spec for the machines we were running on:  2 x Intel E5-2680v3 (12 core, 24 threads) - One reserved core gives each machine 47 cores from the Storm scheduler point of view  256 GB RAM  4 TB SATA Disk  10 G Network Interface    Multi-tenant cluster with other topologies running. Average cluster utilization ranged from  70% - 90%  The DRPC servers in the cluster:  2 x Intel E5620 (4 cores, 8 Threads) - 16 cores  24 GB RAM  1 TB SATA Disk  10 G Network Interface", 
            "title": "Cluster"
        }, 
        {
            "location": "/backend/storm-performance/#data", 
            "text": "Our data was read from a Kafka cluster. We test with both Kafka 0.9.0.1 and 0.10.0.1  The Kafka cluster was located within the same datacenter as the Storm cluster - close network proximity gives us some measure of confidence that large data transmission delays aren't a factor.  The data volume can be measured by measuring how many individual records are being produced per second and what the size of the data throughput is per second.  There is variance in the volume of data over time as this is real data. For each of the tests below, the data volume at that time will be provided in this format:  Data: XX R/s and XX MiB/s , where each of the numbers are the average for each metric over the hour of when the test was done.  R/s  is  Records per second  and  MiB/s  is  MebiBytes per second . The data is compressed with a compression ratio of  1.2 .  Our data schema contained  92  fields with  62  Strings,  4  Longs,  23  Maps and  3  Lists of Maps. Most of the data is generally present in the Maps and Lists of Maps.", 
            "title": "Data"
        }, 
        {
            "location": "/backend/storm-performance/#configuration", 
            "text": "Here is the default configuration we used to launch instances of Bullet.  bullet.topology.metrics.enable: true\nbullet.topology.metrics.built.in.enable: true\nbullet.topology.metrics.built.in.emit.interval.mapping:\n   bullet_active_queries: 5\n   default: 60\nbullet.topology.metrics.classes:\n  -  package.containing.our.custom.class.pushing.metrics \nbullet.topology.tick.interval.secs: 1\nbullet.query.default.duration: 30000\nbullet.query.max.duration: 540000\nbullet.query.aggregation.max.size: 512\nbullet.query.aggregation.raw.max.size: 500\nbullet.query.aggregation.distribution.max.points: 200  Any setting not listed here defaults to the defaults in  bullet_defaults.yaml . In particular,  metadata collection  and  timestamp injection  is enabled.  RAW  type queries also micro-batch by size 1 (in other words, do not micro-batch).  The parallelisms, CPU and memory settings for the components are listed below.", 
            "title": "Configuration"
        }, 
        {
            "location": "/backend/storm-performance/#testing-on-kafka-0901", 
            "text": "For Tests 1 through 4, we read from a Kafka 0.9 cluster with the following configuration for the various Bullet components (unless specified). We use the single Spout model to read from the Kafka topic, partitioned into  64  partitions.", 
            "title": "Testing on Kafka 0.9.0.1"
        }, 
        {
            "location": "/backend/storm-performance/#resource-utilization", 
            "text": "Component  Parallelism  CPU (cores)  On Heap Memory (MiB)  Off Heap Memory (MiB)  Total Memory (MiB)      DataSource Spout  64  64  768.0  192.0  61440    Filter Bolt  128  128  384.0  192.0  73728    Join Bolt  2  1  384.0  192.0  1152    DRPC Spout  2  0.4  128.0  192.0  640    PrepareRequest Bolt  1  0.2  128.0  192.0  320    ReturnResults Bolt  1  0.2  128.0  192.0  320    IMetricsConsumer  1  0.1  128.0  0  128    Ackers  256  25.6  128.0  0  32768    Total  455  219.5    170496     With our ~47 virtual core machines, we would need  5  of these machines to run this instance of Bullet reading this data source and supporting a certain number of queries. What this certain number is, we will determine below.  The topology was also launched (command-line args to Storm) with the following Storm settings:  storm jar\n    ...\n    --bullet-spout-parallelism 64\n    --bullet-spout-cpu-load 100.0 \\\n    --bullet-spout-on-heap-memory-load 768.0 \\\n    --bullet-spout-off-heap-memory-load 192.0 \\\n    -c topology.acker.executors=256 \\\n    -c topology.max.spout.pending=20000 \\\n    -c topology.backpressure.enable=false \\\n    -c topology.worker.max.heap.size.mb=4096.0 \\\n    -c topology.worker.gc.childopts= -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:NewSize=128m -XX:CMSInitiatingOccupancyFraction=70 -XX:-CMSConcurrentMTEnabled -XX:NewRatio=1  \\\n    ...   The spout parallelism is 64 because it is going to read from a Kafka topic with 64 partitions (any more is meaningless since it cannot be split further). It reads and converts the data into Bullet Records.  We've fanned out from the spouts to the Filter Bolts by a ratio of 2. We may or may not need this.  We use  topology.max.spout.pending=20000  to limit the number of in-flight tuples there can be from a DataSource Spout instance and throttle it if too many queries are slowing down processing downstream. This is set pretty high to account for catch-up and skew in our Kafka partitions  We have set the max heap size for a worker to  4 GiB  since we do not want too large of a worker. If a component dies or a worker is killed by RAS, it will not affect too many other components. It also makes heap dumps etc manageable.  We set  topology.worker.gc.childopts  to use  ParNewGC  and  CMS . These are our cluster defaults but we are listing them here since this may not be true for all Storm clusters. We have also added the  -XX:NewRatio=1  to the defaults since most of our objects are short-lived and having a larger Young Generation reduces our Young Generation GC (ParNew) frequency.  We are using 256 acker tasks. There is acking from the DataSource Spout to the Filter Bolt and from the DRPCSpout and the PrepareRequestBolt, so about ~130 components will be acking. We could get away with using much less ackers as they are very light-weight.", 
            "title": "Resource utilization"
        }, 
        {
            "location": "/backend/storm-performance/#test-1-measuring-the-minimum-latency-of-bullet", 
            "text": "We are  running this query  in this test. This  RAW  query without any filters will serve to measure the intrinsic delay added by Bullet. The data record pulled out has a timestamp for when the record was emitted into Kafka, Bullet will inject the timestamp into the record when the Filter Bolt sends it on and the metadata collection logs timestamps for when the query was received and terminated. Using these, we can measure the end-to-end latency for getting one record through Bullet.", 
            "title": "Test 1: Measuring the minimum latency of Bullet"
        }, 
        {
            "location": "/backend/storm-performance/#result", 
            "text": "The following table shows the timestamps averaged by running  100  of these queries. The delays below are shown  relative  to the Query Received timestamp (when the query was received by Bullet at the Join Bolt).      Timestamp  Delay (ms)      Kafka Received  -710.75    Bullet Filtered  -2.16    Query Received  0    Query Finished  1.66     The Bullet Filtered timestamp above is negative because the Filter Bolt received the query and emitted an arbitrary record  2.16 ms  before the Join Bolt received the query. The data was submitted into Kafka about  710.75 ms  before the query was received by Bullet and that difference is the processing time of Kafka and the time for our spouts to read the data into Bullet.", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion", 
            "text": "Bullet adds a delay of a few ms -  1.66 ms  in the test above - to just pull out a record. This result shows that this is the fastest Bullet can be. It cannot return data any faster than this for meaningful queries.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#test-2-measuring-the-time-to-find-a-record", 
            "text": "The  last test  attempted to measure how long Bullet takes to pick out a record. Here we will measure how long it takes to find a record  that we generate . This is the average of running  100  queries across a time interval of 30 minutes trying to filter for a record with a single unique value in a field  similar to this query .  We added a timestamp into the record when the record was initially read by the DataSource Spout. Using this and the Bullet Filtered timestamp and Query Finished timestamps, we can easily track the record through Bullet.  Since we are looking at values in the data, the average data volume across this test was:  Data: 76,000 R/s and 101 MiB/s", 
            "title": "Test 2: Measuring the time to find a record"
        }, 
        {
            "location": "/backend/storm-performance/#result_1", 
            "text": "Timestamp  Delay (ms)      Kafka Received  445.7    Bullet Received  996.5    Bullet Filtered  1003.3    Query Received  0    Query Finished  1006.8     The record was emitted into Kafka  445.81 ms  after the query was received. The delay is the time it takes for the generated record to flow through our network and into Kafka.  Bullet received the record  996.5 ms  after the query was received. The delay from when Kafka received the record to Bullet received is the delay for Kafka to make the record available for reading.", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_1", 
            "text": "We see that Bullet took on average  1006.8 ms - 996.5 ms  or  10.3 ms  from the time it saw the record first in DataSource Spout to finishing up the query and returning it in the Join Bolt.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#test-3-measuring-the-maximum-number-of-parallel-raw-queries", 
            "text": "This test runs a query similar to the  simple filtering query . The query looks for  10  records that match the filter or runs for  30  seconds, whichever comes first. The average, uncompressed record size was about  1.8 KiB .", 
            "title": "Test 3: Measuring the maximum number of parallel RAW queries"
        }, 
        {
            "location": "/backend/storm-performance/#what-is-meant-by-maximum", 
            "text": "We want to see how many of these queries we can have running simultaneously till the Filter Bolt is unable to process records from the spouts in time. If a Filter Bolt is unable to keep up with the rate of data produced by the spouts, our queries will not find all 10 records. Workers may start dying (killed by RAS for exceeding capacity) as well. We will be trying to find the number of queries in parallel that we can run without these happening.  The average data volume across this test was:  Data: 85,000 R/s and 126 MiB/s", 
            "title": "What is meant by maximum?"
        }, 
        {
            "location": "/backend/storm-performance/#query", 
            "text": "{\n  filters  : [\n              { field  :  id ,  operation  :  == ,  values : [ c39ba8cfe8 ]}\n             ],\n  aggregation  : { type  :  LIMIT ,  size  : 10},\n  duration  : 30000\n}  We will run a certain number of these queries then generate records matching this query. We will validate that we have all 10 records for all queries at the end.  We ran a number of queries in parallel (you may have to use  ulimit  to change maximum user processes if you specify a large number), then generated data for the queries (the same data) and waited till the results came back. We then used jq to validate that all generated data was present. In a later test, we will actually measure the latency increase caused by increasing the queries.   It generates a provided number of the  query above  and runs them in parallel against a randomly chosen DRPC server  It generates data for the query  It waits out the rest of the time and uses jq to validate that all the generated data was found", 
            "title": "Query"
        }, 
        {
            "location": "/backend/storm-performance/#result_2", 
            "text": "We were able to run 200 queries successfully but 300 and higher started causing our Filter Bolts to slow down. This slow down caused our spouts to be throttled and fall behind reading data. This caused the matching data to not show up in time during the queries. Some of our attempts would not return all the expected 10 records.  Using our metrics that were captured using our in-house metrics aggregation system (that our IMetricsConsumer publishes to), let's take a look at the CPU, Heap utilizations.  Before you look at the figures:   All the figures below are for the same time interval. The X-axis represents time in  1 minute  intervals  Figure 1  shows the number of queries running for a time interval  The other figures show a metric across  all  the workers (JVMs) in the Storm topology, each running a mix of a components (spouts reading from Kafka, Filter Bolts etc)  The majority of the components (excluding ackers) are spouts reading from Kafka or Filter Bolts, so the figures can be taken to be primarily describing those workers", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#figure-1-queries-running", 
            "text": "", 
            "title": "Figure 1. Queries running"
        }, 
        {
            "location": "/backend/storm-performance/#figure-2-cpu-user-time-usage", 
            "text": "", 
            "title": "Figure 2. CPU user-time usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-3-on-heap-usage", 
            "text": "", 
            "title": "Figure 3. On Heap usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-4-garbage-collection-times", 
            "text": "Figure 1  shows that we first ran 100 queries, then 200, then 400 and finally 300. The numbers go over their target because we only added a 2 s buffer in our script. Network and tick delays caused some queries to not be entirely purged before the next set of N simultaneous queries came in.  Figure 2  shows the milliseconds of CPU time used per minute. For example, a value of  300K ms  ms for a line (worker) means that the worker used  300K ms/min   or  300s/60s  or  5  CPU cores (virtual) in that minute.  Figure 3  shows raw numbers for Heap utilizations in bytes.  Figure 4  shows the time spent garbage collecting in ms.   Garbage collection  As we increase the number of queries sent into Bullet, more objects are created in the Filter and Join Bolts. These quickly fill up our heap and cause GCs. The zig-zags represents heaps being cleared after GC and filling back up quickly. Also, note that the CPU usage is directly related to the GC times. In other words, performance is pretty much directly correlated with the amount of GC we do.   The following table summarizes these figures:      Simultaneous Queries  Average CPU (ms)  Average Result size      0  90K  N/A    100  130K  10    200  170K  10    300  230K  8.9    400  270K  7.3", 
            "title": "Figure 4. Garbage Collection times"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_2", 
            "text": "We are able to run somewhere between 200 and 300  RAW  queries simultaneously before losing data.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#test-4-improving-the-maximum-number-of-simultaneous-raw-queries", 
            "text": "The  last test  showed us that the  standard configuration  lets us run somewhere from 200 and 300  RAW  queries. Let's improve that. The  GC times  and the  heap usage  tell us that a larger heap may help. Also, since each of our machines has 256 GB of RAM and we have a lot of unused heap being wasted per machine (since we use up all the CPU cores), we can be use a lot more heap and not have to GC so much. Let's also try to make the worker even leaner by decreasing the max worker size so that slow components don't block others.  Changes:  -bullet.topology.filter.bolt.memory.on.heap.load: 384.0\n+bullet.topology.filter.bolt.memory.on.heap.load: 1024.0\n\n- --bullet-spout-on-heap-memory-load 768.0 \\\n+ --bullet-spout-on-heap-memory-load 1280.0 \\\n\n- -c topology.worker.max.heap.size.mb=4096.0 \\\n+ -c topology.worker.max.heap.size.mb=3072.0 \\  Our resource utilization is now:      Component  Parallelism  CPU (cores)  On Heap Memory (MiB)  Off Heap Memory (MiB)  Total Memory (MiB)      DataSource Spout  64  64  1280.0  192.0  94208    Filter Bolt  128  128  1024.0  192.0  155648    Join Bolt  2  1  384.0  192.0  1152    DRPC Spout  2  0.4  128.0  192.0  640    PrepareRequest Bolt  1  0.2  128.0  192.0  320    ReturnResults Bolt  1  0.2  128.0  192.0  320    IMetricsConsumer  1  0.1  128.0  0  128    Ackers  256  25.6  128.0  0  32768    Total  455  219.5    285184     Our data volume across this test was:  Data: 84,000 R/s and 124 MiB/s", 
            "title": "Test 4: Improving the maximum number of simultaneous RAW queries"
        }, 
        {
            "location": "/backend/storm-performance/#result_3", 
            "text": "With this configuration, we were able to run 700 queries simultaneously and failed at some where between 700 and 800. See below for why.  We notice that the GC times have improved a lot (down to ~12K ms from ~35K ms in  Figure 4 ). While our overall CPU usage seems to have gone down since we GC a lot less, remember that our changes to the maximum worker size makes our workers run less components and as a result, use less CPU. This is why there are more lines overall (more workers).", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#figure-5-queries-running", 
            "text": "", 
            "title": "Figure 5. Queries running"
        }, 
        {
            "location": "/backend/storm-performance/#figure-6-cpu-user-time-usage", 
            "text": "", 
            "title": "Figure 6. CPU user-time usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-7-on-heap-usage", 
            "text": "", 
            "title": "Figure 7. On Heap usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-8-garbage-collection-times", 
            "text": "Simultaneous Queries  Average CPU (ms)  Average Result size      0  50K  N/A    300  80K  10    500  95K  10    600  100K  10    700  100K  10    735  105K  10    800  105K  9.19     We seem to cap out at 735 queries. This is actually due how the blocking implementation of Storm DRPC works. Storm DRPC currently dedicates a thread to each DRPC request and does not allow more requests till they are finished. For Bullet, when we ran 800 queries for the test, only the first 735 would even be sent to Bullet. The rest 65 would be sent after some of the first return but all of those 65 would return 0 records because the data that they are looking for had long since been processed. We had  3  DRPC servers in the cluster and you could  scale by adding more of these .", 
            "title": "Figure 8. Garbage Collection times"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_3", 
            "text": "With this change in heap usage, we could get to  735  of these queries simultaneously without any issues. We could do more but were limited by the number of DRPC servers in our cluster.   735 is a hard limit then?  This is what our Storm cluster's configuration and our usage of the Storm DRPC limits us to. There is an async implementation for DRPC that we could eventually switch to. If we used another PubSub implementation like Kafka, we would be able to bypass this limit.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#testing-on-kafka-01001", 
            "text": "For this and subsequent tests, we upgraded our Kafka cluster to 0.10. We used the new Kafka consumer APIs to read  batches  of messages instead of a message at a time. We changed our DataSource Spout to read batches of messages (raw bytes) instead and added a DataSource Bolt that converts each batch message into Bullet records. Switching to this model let us be a lot more efficient in our data reading.  To read more data, we will be trying to read a topic that is a superset of our data set so far and produces up to  13  times the number of records (maximum of 1.3 million records/sec) and  20  times the size of the data we were reading till now. This Kafka topic has  256  partitions.", 
            "title": "Testing on Kafka 0.10.0.1"
        }, 
        {
            "location": "/backend/storm-performance/#test-5-reading-more-data", 
            "text": "Our average data volume across this test was:  Data: 756,000 R/s and 3080 MiB/s      Component  Parallelism  CPU (cores)  On Heap Memory (MiB)  Off Heap Memory (MiB)  Total Memory (MiB)      DataSource Spout  128  128  1024.0  192.0  155648    DataSource Bolt  256  512  2580.0  192.0  709632    Filter Bolt  512  512  1024.0  192.0  622592    Join Bolt  2  1  512.0  192.0  1408    DRPC Spout  2  0.4  128.0  192.0  640    PrepareRequest Bolt  1  0.2  128.0  192.0  320    ReturnResults Bolt  1  0.2  128.0  192.0  320    IMetricsConsumer  4  0.4  128.0  0  512    Ackers  256  25.6  128.0  0  32768    Total  1162  1179.8    1523840     In terms of  our machines , the CPU is clearly the limiting factor. We end up using about  25  machines to run this topology.  We also tweaked our GC and Storm settings. We ran the topology with the following command:  storm jar\n          ...\n          --bullet-conf bullet_settings.yaml \\\n          -c topology.max.spout.pending=30 \\\n          -c topology.backpressure.enable=false \\\n          -c topology.acker.executors=256 \\\n          -c topology.component.resources.onheap.memory.mb=128.0 \\\n          -c topology.worker.max.heap.size.mb=3072.0 \\\n          -c topology.worker.gc.childopts= -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:NewSize=128m -XX:NewRatio=2 -XX:SurvivorRatio=6 -XX:+UseCMSInitiatingOccupancyOnly -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=50 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=4  \\\n          ...  We capped the GC threads to  8  and  4 , which helps performance on our 48 core machines when multiple JVMs run on a machine. We also bumped up the Young Generation size ( NewRatio=2 ) and triggered the CMS collection to run at  50%  of Old Generation instead of the default of  70% . Our objects are short-lived in our streaming scenario, this makes sense. We arrived at this after a few iterations, which we'll skip here for brevity.   Max Spout Pending is now 30 ?!  We use  topology.max.spout.pending  as a way to throttle how fast we read from Kafka. There is no acking past the Filter Bolt. The maximum number of batch messages we read is  500  from Kafka. This makes our true max spout pending:  500 * 30 = 15,000 . The tuple that is emitted from the spout is a large tuple that contains up to  500  records and we limit up to  30  of those to go unacked from any single spout before we throttle it. Since we have  128  spouts, we can have  128 * 15,000  messages unacked in the topology at any time at the most.", 
            "title": "Test 5: Reading more Data"
        }, 
        {
            "location": "/backend/storm-performance/#result_4", 
            "text": "We were able to run over 600 queries with this configuration and failed in the high 600s due to hitting the DRPC limit again. With our improved reading model and GC configs, our base resource footprint has improved from  50K  CPU ms/min to around  25K  CPU ms/min.", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#figure-9-queries-running", 
            "text": "", 
            "title": "Figure 9. Queries running"
        }, 
        {
            "location": "/backend/storm-performance/#figure-10-cpu-user-time-usage", 
            "text": "", 
            "title": "Figure 10. CPU user-time usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-11-on-heap-usage", 
            "text": "", 
            "title": "Figure 11. On Heap usage"
        }, 
        {
            "location": "/backend/storm-performance/#figure-12-garbage-collection-times", 
            "text": "Simultaneous Queries  Average CPU (ms)  Average Result size      0  25K  N/A    100  35K  10    300  60K  10    500  85K  10    600  90K  10    680  95K  10    700  95K  9.79     Note that the drops when we were running simultaneous queries in  Figure 9  are due to some\nhiccups in our collection mechanism (the collection granularity was not 5s as we configured but higher) and not actually drops in the simultaneous queries.", 
            "title": "Figure 12. Garbage Collection times"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_4", 
            "text": "We are trying to read a data source that could have 13  times more records and  20  times more data volume. So we have roughly increased the parallelism of the components reading the data by 10x ( 128 + 512 = 768  cores to read and convert the data whereas previously we were using  64  cores). Once this is fixed and we can read the data comfortably using our DataSource Spouts and Bolts, we can scale the Filter Bolts and other components to accommodate for queries. We set our Filter Bolt parallelism (dominates the rest of the components) to  512 . We need about  25  machines (5 times more than the previous of  5 ).  With this configuration, we were able to run  680  queries simultaneously before we hit the DRPC limit. Since DRPC is a shared resource for the cluster, this limit is slightly lower than the previously observed number possibly due to our test environment being multi-tenant and other topologies using the shared resource.   Measuring latency in Bullet  So far, we have been using data being delayed long enough as a proxy for queries failing.  Bullet-Storm 0.4.3  adds an average latency metric computed in the Filter Bolts. For the next tests, we add a timestamp in the Data Source spouts when the record is read and this latency metric tells us exactly how long it takes for the record to be matched against a query and acked. By setting a limit for this latency, we can much more accurately measure acceptable performance.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#test-6-scaling-for-more-data", 
            "text": "For this test, we'll establish how much resources we need to read various data volumes. We want to be able to:   Read the data  Be able to catch up data backlogs at   5 : 1  ratio (  5s of data in 1 s)  Support   400 RAW  queries  The average latency for reading a record to filtering it be   200 ms   For reading the data, we have to first scale the DataSource spouts and bolts and then set the parallelism of the Filter bolts to support the minimum 400 queries we want at the data volume. We leave the rest of the components at their default values as seen in in  Test 5 .  To get various data volumes, we read a large Kafka topic with (256 partitions) with over 1 million R/s and sample various percentages to get less data. The sampling is done in our DataSource Spouts.", 
            "title": "Test 6: Scaling for More Data"
        }, 
        {
            "location": "/backend/storm-performance/#result_5", 
            "text": "The following table summarizes the results:      Data (MiB/s, R/s)  Component  Parallelism  CPU cores  On Heap (MiB)  Total CPU cores  Total Memory (MiB)      307, 69700      98.3  123648     DataSource Spout  16  0.5  1024       DataSource Bolt  32  2  2048       Filter Bolt  24  1  1024      920, 216825      242.7  281856     DataSource Spout  32  1  1024       DataSource Bolt  72  2  2048       Filter Bolt  64  1  1024      1535, 374370      531.5  616192     DataSource Spout  64  1  1024       DataSource Bolt  160  2  2048       Filter Bolt  144  1  1024      2149, 524266      812.3  939264     DataSource Spout  72  1  1024       DataSource Bolt  256  2  2048       Filter Bolt  224  1  1024      3070, 724390      997.1  1321984     DataSource Spout  96  1  1024       DataSource Bolt  320  2  2580       Filter Bolt  256  1  1024      4024, 1004500      1189.4  1582208     DataSource Spout  96  1  1024       DataSource Bolt  384  2  2580       Filter Bolt  320  1  1024       The following figures graphs how the data volume relates to the total CPU and Memory needed.", 
            "title": "Result"
        }, 
        {
            "location": "/backend/storm-performance/#figure-13-data-volume-vs-cpu", 
            "text": "", 
            "title": "Figure 13. Data Volume vs CPU"
        }, 
        {
            "location": "/backend/storm-performance/#figure-14-data-volume-vs-memory", 
            "text": "As these figures show, Bullet scales pretty linearly for more data.  For each core of CPU core, we need about 1.2 GiB of Memory. That is the, CPU to Memory ratio is  1 core : 1.2 GiB . The slopes of figures show that the CPU to data ratio is  1 core : 850 R/s  or  1 core : 3.4 MiB/s .", 
            "title": "Figure 14. Data Volume vs Memory"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_5", 
            "text": "This test demonstrates that Bullet scales pretty linearly for reading more data. We were able to support at least  400 queries with plenty of headroom for each configuration. Each CPU core required about 1.2 GiB of Memory and gave us roughly 800 R/s or 3.4 MiB/s processing capability. Of course, this is particular to our data. As mentioned in the  data section , most of our data was concentrated in random maps. This means that in memory, the data is generally not contiguous. This creates memory fragmentation and more GC pressure. Depending on your data schema, your performance may be a lot better.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/storm-performance/#test-7-scaling-for-more-queries", 
            "text": "For this test, we'll establish how much resources we need to support more queries for a fixed data volume:  Data: 68,400 R/s and 105 MiB/s .  As our 3 server DRPC cluster currently does not let us do more than  680 RAW  queries, in this test, we will:   Vary the number of Filter Bolts as they are the primary bottleneck for supporting more queries.  To simplify things, we will only vary the  parallelism  and fix the CPU and memory of each Filter Bolt to  1 Core  and  1 GiB Memory  We will use  RAW  queries as the queries to scale  Each  RAW  query will run for  30 s  and search for  10  records that we generate. The query will actually look for  11  records to force it to run for the full  30 s . This is because we want to stress the Filter Bolt as much as possible. As long as there is a query in the system, the Filter Bolt will deserialize and check every record that it processes  We will measure the same filtering latency: the time taken from the record read in DataSource Spout to its emission in the Filter Bolt. We want the maximum latency to be less than  200 ms", 
            "title": "Test 7: Scaling for More Queries"
        }, 
        {
            "location": "/backend/storm-performance/#results", 
            "text": "The following table summarizes the results:      Filter Bolt Parallelism  Queries  Average Latency (ms)  Status  Topology CPU (cores)  Topology Memory (MiB)      4     78.3  112256     1  7  OK       10  8  OK       20  8  OK       30  10  OK       40  12  OK       50  167  OK       60  1002  FAIL      8     82.3  117120     40  9  OK       60  9  OK       80  9  OK       100  10  OK       120  14  OK       140  18  OK       160  298  FAIL       170  2439  FAIL      12     86.3  121984     160  13  OK       200  14  OK       240  78  OK       280  274  FAIL       320  680  FAIL       360  1700  FAIL       400  2685  FAIL      16     90.3  126848     300  30  OK       350  44  OK       400  481  FAIL       450  935  FAIL       500  1968  FAIL       550  3267  FAIL      20     94.3  131712     350  15  OK       400  18  OK       450  28  OK       500  40  OK       550  670  FAIL       600  1183  FAIL       650  1924  FAIL      24     98.3  136576     450  17  OK       500  22  OK       550  30  OK       600  377  FAIL       650  490  FAIL      28     102.3  141440     550  39  OK       600  53  OK       650  468  FAIL      32     106.3  146304     600  26  OK       650  32  OK       The following figure summarizes the minimum number of CPU cores (which are also the number of Filter Bolts) needed to support the the maximum number of  RAW  queries with latency   200 ms.", 
            "title": "Results"
        }, 
        {
            "location": "/backend/storm-performance/#figure-15-data-volume-vs-memory", 
            "text": "This shows that the queries supported also scale pretty linearly.  You may have noticed how when latency starts to increase, it increases pretty rapidly. This suggests that there is a  knee  or  exponential  curve for latency. The following figure shows this in the graph of the latency for queries with  20  Filter Bolts.", 
            "title": "Figure 15. Data Volume vs Memory"
        }, 
        {
            "location": "/backend/storm-performance/#figure-16-data-volume-vs-memory", 
            "text": "", 
            "title": "Figure 16. Data Volume vs Memory"
        }, 
        {
            "location": "/backend/storm-performance/#conclusion_6", 
            "text": "Since Filter Bolts tend to be the most CPU intensive of the query processing components, this test measured how scaling Filter Bolts affected the number of queries that can be supported. For the fixed data volume, this relationship is linear.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/spark-architecture/", 
            "text": "Spark architecture\n\n\nThis section describes how the \nBackend architecture\n is implemented in Spark Streaming.\n\n\nData Flow Graph\n\n\nBullet Spark implements the backend piece from the full \nArchitecture\n. It is implemented with Spark Streaming:\n\n\n\n\nThe components in the \nArchitecture\n have direct counterparts here. The Query Receiver reading from the PubSub layer using plugged-in PubSub consumers and the Query Unioning make up the Request Processor. The Filter Streaming and your plugin for your source of data make up the Data Processor. The Join Streaming and the Result Emitter make up the Combiner.\n\n\nThe red lines are the path for the queries that come in through the PubSub, the orange lines are the path for the signals and the blue lines are for the data from your data source. The shapes of the boxes denote the type of transformation/action being executed in the boxes.\n\n\nData processing\n\n\nBullet can accept arbitrary sources of data as long as they can be ingested by Spark. They can be Kafka, Flume, Kinesis, and TCP sockets etc. In order to hook up your data to Bullet Spark, you just need to implement the \nData Producer Trait\n. In your implementation, you can either:\n\n\n\n\nUse \nSpark Streaming built-in sources\n to receive data. Below is a quick example for a direct Kafka source in Scala. You can also write it in Java:\n\n\n\n\nimport com.yahoo.bullet.spark.DataProducer\nimport org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}\n// import all other necessary packages\n\nclass DirectKafkaProducer extends DataProducer {\n  override def getBulletRecordStream(ssc: StreamingContext, config: BulletSparkConfig): DStream[BulletRecord] = {\n    val topics = Array(\ntest\n)\n    val kafkaParams = Map[String, AnyRef](\n      \nbootstrap.servers\n -\n \nserver1, server2\n,\n      \ngroup.id\n -\n \nmygroup\n,\n      \nkey.deserializer\n -\n classOf[StringDeserializer],\n      \nvalue.deserializer\n -\n classOf[ByteArrayDeserializer]\n      // Other kafka params\n      )\n\n    val directKafkaStream = KafkaUtils.createDirectStream[String, Array[Byte]](\n      ssc,\n      LocationStrategies.PreferConsistent,\n      ConsumerStrategies.Subscribe[String, Array[Byte]](topics, kafkaParams))\n\n    directKafkaStream.map(record =\n {\n      // Convert your record to BulletRecord\n    })\n  }\n}\n\n\n\n\n\n\nWrite a \ncustom receiver\n to receive data from any arbitrary data source beyond the ones for which it has built-in support (that is, beyond Flume, Kafka, Kinesis, files, sockets, etc.). See \nexample\n.\n\n\n\n\nAfter receiving your data, you can do any transformations like joins or type conversions in your implementation before emitting to the Filter Streaming stage.\n\n\nThe Filter Streaming stage checks every record from your data source against every query from Query Unioning stage to see if it matches and emits partial results to the Join Streaming stage.\n\n\nRequest processing\n\n\nThe Query Receiver fetches Bullet queries and signals through the PubSub layer using the Subscribers provided by the plugged in PubSub layer. The queries received through the PubSub also contain information about the query such as its unique identifier, potentially other metadata and signals. The Query Unioning collects all active queries by the stateful transformation \nupdateStateByKey\n and broadcasts all the collected queries to every executor for the Filter Streaming stage.\n\n\nThe Query Unioning also sends all active queries and signals to the Join Streaming stage.\n\n\nCombining\n\n\nThe Filter Streaming combines all the partial results from the Filter Streaming by the stateful transformation \nmapWithState\n and produces final results.\n\n\nThe Result Emitter uses the particular publisher from the plugged in PubSub layer to send back results/loop signals.", 
            "title": "Architecture"
        }, 
        {
            "location": "/backend/spark-architecture/#spark-architecture", 
            "text": "This section describes how the  Backend architecture  is implemented in Spark Streaming.", 
            "title": "Spark architecture"
        }, 
        {
            "location": "/backend/spark-architecture/#data-flow-graph", 
            "text": "Bullet Spark implements the backend piece from the full  Architecture . It is implemented with Spark Streaming:   The components in the  Architecture  have direct counterparts here. The Query Receiver reading from the PubSub layer using plugged-in PubSub consumers and the Query Unioning make up the Request Processor. The Filter Streaming and your plugin for your source of data make up the Data Processor. The Join Streaming and the Result Emitter make up the Combiner.  The red lines are the path for the queries that come in through the PubSub, the orange lines are the path for the signals and the blue lines are for the data from your data source. The shapes of the boxes denote the type of transformation/action being executed in the boxes.", 
            "title": "Data Flow Graph"
        }, 
        {
            "location": "/backend/spark-architecture/#data-processing", 
            "text": "Bullet can accept arbitrary sources of data as long as they can be ingested by Spark. They can be Kafka, Flume, Kinesis, and TCP sockets etc. In order to hook up your data to Bullet Spark, you just need to implement the  Data Producer Trait . In your implementation, you can either:   Use  Spark Streaming built-in sources  to receive data. Below is a quick example for a direct Kafka source in Scala. You can also write it in Java:   import com.yahoo.bullet.spark.DataProducer\nimport org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}\n// import all other necessary packages\n\nclass DirectKafkaProducer extends DataProducer {\n  override def getBulletRecordStream(ssc: StreamingContext, config: BulletSparkConfig): DStream[BulletRecord] = {\n    val topics = Array( test )\n    val kafkaParams = Map[String, AnyRef](\n       bootstrap.servers  -   server1, server2 ,\n       group.id  -   mygroup ,\n       key.deserializer  -  classOf[StringDeserializer],\n       value.deserializer  -  classOf[ByteArrayDeserializer]\n      // Other kafka params\n      )\n\n    val directKafkaStream = KafkaUtils.createDirectStream[String, Array[Byte]](\n      ssc,\n      LocationStrategies.PreferConsistent,\n      ConsumerStrategies.Subscribe[String, Array[Byte]](topics, kafkaParams))\n\n    directKafkaStream.map(record =  {\n      // Convert your record to BulletRecord\n    })\n  }\n}   Write a  custom receiver  to receive data from any arbitrary data source beyond the ones for which it has built-in support (that is, beyond Flume, Kafka, Kinesis, files, sockets, etc.). See  example .   After receiving your data, you can do any transformations like joins or type conversions in your implementation before emitting to the Filter Streaming stage.  The Filter Streaming stage checks every record from your data source against every query from Query Unioning stage to see if it matches and emits partial results to the Join Streaming stage.", 
            "title": "Data processing"
        }, 
        {
            "location": "/backend/spark-architecture/#request-processing", 
            "text": "The Query Receiver fetches Bullet queries and signals through the PubSub layer using the Subscribers provided by the plugged in PubSub layer. The queries received through the PubSub also contain information about the query such as its unique identifier, potentially other metadata and signals. The Query Unioning collects all active queries by the stateful transformation  updateStateByKey  and broadcasts all the collected queries to every executor for the Filter Streaming stage.  The Query Unioning also sends all active queries and signals to the Join Streaming stage.", 
            "title": "Request processing"
        }, 
        {
            "location": "/backend/spark-architecture/#combining", 
            "text": "The Filter Streaming combines all the partial results from the Filter Streaming by the stateful transformation  mapWithState  and produces final results.  The Result Emitter uses the particular publisher from the plugged in PubSub layer to send back results/loop signals.", 
            "title": "Combining"
        }, 
        {
            "location": "/backend/spark-setup/", 
            "text": "Bullet on Spark\n\n\nThis section explains how to set up and run Bullet on Spark.\n\n\nConfiguration\n\n\nBullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in \nbullet_spark_defaults.yaml\n. You can find out what these settings do in the comments listed in the defaults.\n\n\nInstallation\n\n\nDownload the Bullet Spark standalone jar from \nJCenter\n.\n\n\nIf you are using Bullet Kafka as pluggable PubSub, you can download the fat jar from \nJCenter\n. Otherwise, you need to plug in your own PubSub jar or use the RESTPubSub built-into bullet-core and turned on in the API.\n\n\nTo use Bullet Spark, you need to implement your own \nData Producer Trait\n with a JVM based project. You have two ways to implement it as described in the \nSpark Architecture\n section. You include the Bullet artifact and Spark dependencies in your pom.xml or other equivalent build tools. The artifacts are available through JCenter. Here is an example if you use Scala and Maven:\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\nproperties\n\n    \nscala.version\n2.11.7\n/scala.version\n\n    \nscala.dep.version\n2.11\n/scala.dep.version\n\n    \nspark.version\n2.3.0\n/spark.version\n\n    \nbullet.spark.version\n0.1.1\n/bullet.spark.version\n\n\n/properties\n\n\n\ndependency\n\n    \ngroupId\norg.scala-lang\n/groupId\n\n    \nartifactId\nscala-library\n/artifactId\n\n    \nversion\n${scala.version}\n/version\n\n    \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\ndependency\n\n    \ngroupId\norg.apache.spark\n/groupId\n\n    \nartifactId\nspark-streaming_${scala.dep.version}\n/artifactId\n\n    \nversion\n${spark.version}\n/version\n\n    \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\ndependency\n\n    \ngroupId\norg.apache.spark\n/groupId\n\n    \nartifactId\nspark-core_${scala.dep.version}\n/artifactId\n\n    \nversion\n${spark.version}\n/version\n\n    \nscope\nprovided\n/scope\n\n\n/dependency\n\n\n\ndependency\n\n     \ngroupId\ncom.yahoo.bullet\n/groupId\n\n     \nartifactId\nbullet-spark\n/artifactId\n\n     \nversion\n${bullet.spark.version}\n/version\n\n\n/dependency\n\n\n\n\n\nYou can also add \nclassifier\nsources\n/classifier\n or \nclassifier\njavadoc\n/classifier\n if you want the sources or javadoc.\n\n\nLaunch\n\n\nAfter you have implemented your own data producer and built a jar, you could launch your Bullet Spark application. Here is an example command for a \nYARN cluster\n.\n\n\n./bin/spark-submit \\\n    --master yarn \\\n    --deploy-mode cluster \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --queue \nyour queue\n \\\n    --executor-memory 12g \\\n    --executor-cores 2 \\\n    --num-executors 200 \\\n    --driver-cores 2 \\\n    --driver-memory 12g \\\n    --conf spark.streaming.backpressure.enabled=true \\\n    --conf spark.default.parallelism=20 \\\n    ... # other Spark settings\n    --jars /path/to/your-data-producer.jar,/path/to/your-pubsub.jar \\\n    /path/to/downloaded-bullet-spark-standalone.jar \\\n    --bullet-spark-conf /path/to/your-settings.yaml\n\n\n\n\nYou can pass other Spark settings by adding \n--conf key=value\n to the command. For more settings, you can refer to the \nSpark Configuration\n.\n\n\nFor other platforms, you can find the commands from the \nSpark Documentation\n.", 
            "title": "Setup"
        }, 
        {
            "location": "/backend/spark-setup/#bullet-on-spark", 
            "text": "This section explains how to set up and run Bullet on Spark.", 
            "title": "Bullet on Spark"
        }, 
        {
            "location": "/backend/spark-setup/#configuration", 
            "text": "Bullet is configured at run-time using settings defined in a file. Settings not overridden will default to the values in  bullet_spark_defaults.yaml . You can find out what these settings do in the comments listed in the defaults.", 
            "title": "Configuration"
        }, 
        {
            "location": "/backend/spark-setup/#installation", 
            "text": "Download the Bullet Spark standalone jar from  JCenter .  If you are using Bullet Kafka as pluggable PubSub, you can download the fat jar from  JCenter . Otherwise, you need to plug in your own PubSub jar or use the RESTPubSub built-into bullet-core and turned on in the API.  To use Bullet Spark, you need to implement your own  Data Producer Trait  with a JVM based project. You have two ways to implement it as described in the  Spark Architecture  section. You include the Bullet artifact and Spark dependencies in your pom.xml or other equivalent build tools. The artifacts are available through JCenter. Here is an example if you use Scala and Maven:  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   properties \n     scala.version 2.11.7 /scala.version \n     scala.dep.version 2.11 /scala.dep.version \n     spark.version 2.3.0 /spark.version \n     bullet.spark.version 0.1.1 /bullet.spark.version  /properties  dependency \n     groupId org.scala-lang /groupId \n     artifactId scala-library /artifactId \n     version ${scala.version} /version \n     scope provided /scope  /dependency  dependency \n     groupId org.apache.spark /groupId \n     artifactId spark-streaming_${scala.dep.version} /artifactId \n     version ${spark.version} /version \n     scope provided /scope  /dependency  dependency \n     groupId org.apache.spark /groupId \n     artifactId spark-core_${scala.dep.version} /artifactId \n     version ${spark.version} /version \n     scope provided /scope  /dependency  dependency \n      groupId com.yahoo.bullet /groupId \n      artifactId bullet-spark /artifactId \n      version ${bullet.spark.version} /version  /dependency   You can also add  classifier sources /classifier  or  classifier javadoc /classifier  if you want the sources or javadoc.", 
            "title": "Installation"
        }, 
        {
            "location": "/backend/spark-setup/#launch", 
            "text": "After you have implemented your own data producer and built a jar, you could launch your Bullet Spark application. Here is an example command for a  YARN cluster .  ./bin/spark-submit \\\n    --master yarn \\\n    --deploy-mode cluster \\\n    --class com.yahoo.bullet.spark.BulletSparkStreamingMain \\\n    --queue  your queue  \\\n    --executor-memory 12g \\\n    --executor-cores 2 \\\n    --num-executors 200 \\\n    --driver-cores 2 \\\n    --driver-memory 12g \\\n    --conf spark.streaming.backpressure.enabled=true \\\n    --conf spark.default.parallelism=20 \\\n    ... # other Spark settings\n    --jars /path/to/your-data-producer.jar,/path/to/your-pubsub.jar \\\n    /path/to/downloaded-bullet-spark-standalone.jar \\\n    --bullet-spark-conf /path/to/your-settings.yaml  You can pass other Spark settings by adding  --conf key=value  to the command. For more settings, you can refer to the  Spark Configuration .  For other platforms, you can find the commands from the  Spark Documentation .", 
            "title": "Launch"
        }, 
        {
            "location": "/backend/spark-performance/", 
            "text": "Performance of Bullet on Spark\n\n\nThis section describes how we tune the performance of Bullet on Spark. This is not meant to be a rigorous benchmark.\n\n\nPrerequisites\n\n\nYou should be familiar with \nSpark Streaming\n, \nKafka\n and the \nBullet on Spark architecture\n.\n\n\nHow was this tested?\n\n\nAll tests run here were using \nBullet-Spark 0.1.2\n.\n\n\nTools used\n\n\n\n\njq\n - a nice tool to parse Bullet JSON responses\n\n\ncurl, bash and python - for running and analyzing Bullet queries\n\n\nApache JMeter\n - a load testing tool to send queries to the server simultaneously\n\n\n\n\nCluster\n\n\n\n\nHadoop YARN cluster with Apache Spark 2.1.2.12 installed\n\n\nThe spec for the machines we were running on:\n\n\n2 x Intel E5530(4 cores, 8 Threads)\n\n\n24 GB RAM\n\n\n3 TB SATA Disk\n\n\n10 G Network Interface\n\n\n\n\n\n\n\n\nData\n\n\n\n\nOur data was read from a Kafka cluster. Kafka version is 0.10.2.1\n\n\nThe Kafka cluster was located within the same datacenter as the Hadoop YARN cluster - close network proximity gives us some measure of confidence that large data transmission delays aren't a factor.\n\n\nOur data schema contained \n92\n fields with \n62\n Strings, \n4\n Longs, \n23\n Maps and \n3\n Lists of Maps. Most of the data is generally present in the Maps and Lists of Maps.\n\n\nWe tested 2 set of data:\n\n\nThe smaller data was about 36,000 records/s and 43 MB/s\n\n\nThe larger data was about 124,700 records/s and 150 MB/s\n\n\n\n\n\n\n\n\nConfiguration\n\n\nHere are the configurations we used to launch instances of Bullet Spark.\n\n\n\n\nFor the smaller data:\n\n\n\n\nSettings:\n\n\nbullet.spark.batch.duration.ms: 2000\nbullet.spark.receiver.query.block.size: 1\nbullet.result.metadata.enable: true\nbullet.spark.metrics.enabled: true\nbullet.spark.filter.parallel.enabled: true\nbullet.spark.filter.parallelism: 16\nbullet.spark.filter.parallel.query.min.size: 10\nbullet.spark.query.union.checkpoint.duration.multiplier: 20\nbullet.spark.join.checkpoint.duration.multiplier: 20\n\n\n\n\nCommand line:\n\n\n./spark-submit \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --queue default \\\n  --executor-memory 12g \\\n  --executor-cores 2 \\\n  --num-executors 100 \\\n  --driver-cores 2 \\\n  --driver-memory 12g \\\n  --conf spark.streaming.backpressure.enabled=true \\\n  --conf spark.driver.extraJavaOptions=\n-XX:+UseG1GC\n \\\n  --conf spark.executor.extraJavaOptions=\n-XX:+UseG1GC\n \\\n  --conf spark.shuffle.consolidateFiles=true \\\n  --conf spark.dynamicAllocation.enabled=false \\\n  --conf spark.storage.memoryFraction=0.1 \\\n  --conf spark.shuffle.memoryFraction=0.8 \\\n  --conf spark.default.parallelism=20 \\\n  ...\n\n\n\n\n\n\nFor larger Data:\n\n\n\n\nSettings:\n\n\nbullet.spark.batch.duration.ms: 5000\nbullet.spark.receiver.query.block.size: 1\nbullet.result.metadata.enable: true\nbullet.spark.metrics.enabled: true\nbullet.spark.filter.parallel.enabled: true\nbullet.spark.filter.parallelism: 64\nbullet.spark.filter.parallel.query.min.size: 10\nbullet.spark.query.union.checkpoint.duration.multiplier: 20\nbullet.spark.join.checkpoint.duration.multiplier: 20\n\n\n\n\nCommand line:\n\n\n./spark-submit \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --queue default \\\n  --executor-memory 12g \\\n  --executor-cores 2 \\\n  --num-executors 400 \\\n  --driver-cores 2 \\\n  --driver-memory 12g \\\n  --conf spark.streaming.backpressure.enabled=true \\\n  --conf spark.driver.extraJavaOptions=\n-XX:+UseG1GC\n \\\n  --conf spark.executor.extraJavaOptions=\n-XX:+UseG1GC\n \\\n  --conf spark.shuffle.consolidateFiles=true \\\n  --conf spark.dynamicAllocation.enabled=false \\\n  --conf spark.storage.memoryFraction=0.1 \\\n  --conf spark.shuffle.memoryFraction=0.8 \\\n  --conf spark.default.parallelism=50 \\\n  ...\n\n\n\n\nTest 1: Latency of Bullet Spark\n\n\nThis test was done on the smaller data. We used a \nRAW query without any filtering\n to measure the latency added by Bullet Spark. This is not the end-to-end latency for a query. It is the latency from receiving the query to finishing the query, not includes the time spent in Kafka. We ran this query 100 times.\n\n\nResult\n\n\nThis graph shows the latency of each attempts:\n\n\n\n\nConclusion\n\n\nThe average latency was 1173 ms. This result shows that this is the fastest Bullet Spark can be. It cannot return data any faster than this for meaningful queries.\n\n\nTest 2: Scalability for smaller data\n\n\nThis test was done on the smaller data. We want to measure how many of queries we can have running simultaneously on Bullet Spark. We ran 400, 800, 1500 and 1100 queries each for 10 minutes.\n\n\nResult\n\n\nFigure 1. Spark Streaming UI\n\n\n\n\nFigure 2. Queries running\n\n\n\n\nFigure 3. CPU time\n\n\n\n\nFigure 4. Heap usage\n\n\n\n\nFigure 5. Garbage collection time\n\n\n\n\nFigure 6. Garbage collection count\n\n\n\n\nFigure 1\n shows the Spark Streaming UI when running the test.\n\n\nFigure 2\n shows the simultaneous queries we ran.\n\n\nFigure 3\n shows the milliseconds of CPU time used per minute. For example, a value of \n300K ms\n ms for a line (worker) means that the worker used \n300K ms/min\n  or \n300s/60s\n or \n5\n CPU cores (virtual) in that minute.\n\n\nFigure 4\n shows raw numbers for Heap utilizations in bytes.\n\n\nFigure 5\n shows the time in milliseconds spent for garbage collection per minute.\n\n\nFigure 6\n shows the count of garbage collection events per minute.\n\n\nConclusion\n\n\nThe average processing time for each batch was 1 second 143 ms which was below the batch duration 2 seconds. For average, 1 CPU core and 3GB memory were used in this experiment. CPU and memory usages go slowly up while queries number goes up but they are still within resource limits. We can easily run up to 1500 RAW queries simultaneously in this test.\n\n\nTest 3: Scalability for larger data\n\n\nThis test was done on the larger data. We ran 100, 400, 800 and 600 queries each for 10 minutes.\n\n\nResult\n\n\nFigure 7. Spark stream UI\n\n\n\n\nFigure 8. Queries running\n\n\n\n\nFigure 9. CPU time\n\n\n\n\nFigure 10. Heap usage\n\n\n\n\nFigure 11. Garbage collection time\n\n\n\n\nFigure 12. Garbage collection count\n\n\n\n\nConclusion\n\n\nThe average processing time for each batch was 3 second 97 ms which was below the batch duration 5 seconds. For average, 1.2 CPU core and average 5GB memory were used in this experiment. But with queries number goes up, some of the executors memory usage were up to 8-10GB which is close to our resource limits. With more queries running, OOM may happen. So in this experiment, we can only afford up to 800 queries simultaneously.", 
            "title": "Performance"
        }, 
        {
            "location": "/backend/spark-performance/#performance-of-bullet-on-spark", 
            "text": "This section describes how we tune the performance of Bullet on Spark. This is not meant to be a rigorous benchmark.", 
            "title": "Performance of Bullet on Spark"
        }, 
        {
            "location": "/backend/spark-performance/#prerequisites", 
            "text": "You should be familiar with  Spark Streaming ,  Kafka  and the  Bullet on Spark architecture .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/backend/spark-performance/#how-was-this-tested", 
            "text": "All tests run here were using  Bullet-Spark 0.1.2 .", 
            "title": "How was this tested?"
        }, 
        {
            "location": "/backend/spark-performance/#tools-used", 
            "text": "jq  - a nice tool to parse Bullet JSON responses  curl, bash and python - for running and analyzing Bullet queries  Apache JMeter  - a load testing tool to send queries to the server simultaneously", 
            "title": "Tools used"
        }, 
        {
            "location": "/backend/spark-performance/#cluster", 
            "text": "Hadoop YARN cluster with Apache Spark 2.1.2.12 installed  The spec for the machines we were running on:  2 x Intel E5530(4 cores, 8 Threads)  24 GB RAM  3 TB SATA Disk  10 G Network Interface", 
            "title": "Cluster"
        }, 
        {
            "location": "/backend/spark-performance/#data", 
            "text": "Our data was read from a Kafka cluster. Kafka version is 0.10.2.1  The Kafka cluster was located within the same datacenter as the Hadoop YARN cluster - close network proximity gives us some measure of confidence that large data transmission delays aren't a factor.  Our data schema contained  92  fields with  62  Strings,  4  Longs,  23  Maps and  3  Lists of Maps. Most of the data is generally present in the Maps and Lists of Maps.  We tested 2 set of data:  The smaller data was about 36,000 records/s and 43 MB/s  The larger data was about 124,700 records/s and 150 MB/s", 
            "title": "Data"
        }, 
        {
            "location": "/backend/spark-performance/#configuration", 
            "text": "Here are the configurations we used to launch instances of Bullet Spark.   For the smaller data:   Settings:  bullet.spark.batch.duration.ms: 2000\nbullet.spark.receiver.query.block.size: 1\nbullet.result.metadata.enable: true\nbullet.spark.metrics.enabled: true\nbullet.spark.filter.parallel.enabled: true\nbullet.spark.filter.parallelism: 16\nbullet.spark.filter.parallel.query.min.size: 10\nbullet.spark.query.union.checkpoint.duration.multiplier: 20\nbullet.spark.join.checkpoint.duration.multiplier: 20  Command line:  ./spark-submit \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --queue default \\\n  --executor-memory 12g \\\n  --executor-cores 2 \\\n  --num-executors 100 \\\n  --driver-cores 2 \\\n  --driver-memory 12g \\\n  --conf spark.streaming.backpressure.enabled=true \\\n  --conf spark.driver.extraJavaOptions= -XX:+UseG1GC  \\\n  --conf spark.executor.extraJavaOptions= -XX:+UseG1GC  \\\n  --conf spark.shuffle.consolidateFiles=true \\\n  --conf spark.dynamicAllocation.enabled=false \\\n  --conf spark.storage.memoryFraction=0.1 \\\n  --conf spark.shuffle.memoryFraction=0.8 \\\n  --conf spark.default.parallelism=20 \\\n  ...   For larger Data:   Settings:  bullet.spark.batch.duration.ms: 5000\nbullet.spark.receiver.query.block.size: 1\nbullet.result.metadata.enable: true\nbullet.spark.metrics.enabled: true\nbullet.spark.filter.parallel.enabled: true\nbullet.spark.filter.parallelism: 64\nbullet.spark.filter.parallel.query.min.size: 10\nbullet.spark.query.union.checkpoint.duration.multiplier: 20\nbullet.spark.join.checkpoint.duration.multiplier: 20  Command line:  ./spark-submit \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --queue default \\\n  --executor-memory 12g \\\n  --executor-cores 2 \\\n  --num-executors 400 \\\n  --driver-cores 2 \\\n  --driver-memory 12g \\\n  --conf spark.streaming.backpressure.enabled=true \\\n  --conf spark.driver.extraJavaOptions= -XX:+UseG1GC  \\\n  --conf spark.executor.extraJavaOptions= -XX:+UseG1GC  \\\n  --conf spark.shuffle.consolidateFiles=true \\\n  --conf spark.dynamicAllocation.enabled=false \\\n  --conf spark.storage.memoryFraction=0.1 \\\n  --conf spark.shuffle.memoryFraction=0.8 \\\n  --conf spark.default.parallelism=50 \\\n  ...", 
            "title": "Configuration"
        }, 
        {
            "location": "/backend/spark-performance/#test-1-latency-of-bullet-spark", 
            "text": "This test was done on the smaller data. We used a  RAW query without any filtering  to measure the latency added by Bullet Spark. This is not the end-to-end latency for a query. It is the latency from receiving the query to finishing the query, not includes the time spent in Kafka. We ran this query 100 times.", 
            "title": "Test 1: Latency of Bullet Spark"
        }, 
        {
            "location": "/backend/spark-performance/#result", 
            "text": "This graph shows the latency of each attempts:", 
            "title": "Result"
        }, 
        {
            "location": "/backend/spark-performance/#conclusion", 
            "text": "The average latency was 1173 ms. This result shows that this is the fastest Bullet Spark can be. It cannot return data any faster than this for meaningful queries.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/spark-performance/#test-2-scalability-for-smaller-data", 
            "text": "This test was done on the smaller data. We want to measure how many of queries we can have running simultaneously on Bullet Spark. We ran 400, 800, 1500 and 1100 queries each for 10 minutes.", 
            "title": "Test 2: Scalability for smaller data"
        }, 
        {
            "location": "/backend/spark-performance/#result_1", 
            "text": "", 
            "title": "Result"
        }, 
        {
            "location": "/backend/spark-performance/#figure-1-spark-streaming-ui", 
            "text": "", 
            "title": "Figure 1. Spark Streaming UI"
        }, 
        {
            "location": "/backend/spark-performance/#figure-2-queries-running", 
            "text": "", 
            "title": "Figure 2. Queries running"
        }, 
        {
            "location": "/backend/spark-performance/#figure-3-cpu-time", 
            "text": "", 
            "title": "Figure 3. CPU time"
        }, 
        {
            "location": "/backend/spark-performance/#figure-4-heap-usage", 
            "text": "", 
            "title": "Figure 4. Heap usage"
        }, 
        {
            "location": "/backend/spark-performance/#figure-5-garbage-collection-time", 
            "text": "", 
            "title": "Figure 5. Garbage collection time"
        }, 
        {
            "location": "/backend/spark-performance/#figure-6-garbage-collection-count", 
            "text": "Figure 1  shows the Spark Streaming UI when running the test.  Figure 2  shows the simultaneous queries we ran.  Figure 3  shows the milliseconds of CPU time used per minute. For example, a value of  300K ms  ms for a line (worker) means that the worker used  300K ms/min   or  300s/60s  or  5  CPU cores (virtual) in that minute.  Figure 4  shows raw numbers for Heap utilizations in bytes.  Figure 5  shows the time in milliseconds spent for garbage collection per minute.  Figure 6  shows the count of garbage collection events per minute.", 
            "title": "Figure 6. Garbage collection count"
        }, 
        {
            "location": "/backend/spark-performance/#conclusion_1", 
            "text": "The average processing time for each batch was 1 second 143 ms which was below the batch duration 2 seconds. For average, 1 CPU core and 3GB memory were used in this experiment. CPU and memory usages go slowly up while queries number goes up but they are still within resource limits. We can easily run up to 1500 RAW queries simultaneously in this test.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/backend/spark-performance/#test-3-scalability-for-larger-data", 
            "text": "This test was done on the larger data. We ran 100, 400, 800 and 600 queries each for 10 minutes.", 
            "title": "Test 3: Scalability for larger data"
        }, 
        {
            "location": "/backend/spark-performance/#result_2", 
            "text": "", 
            "title": "Result"
        }, 
        {
            "location": "/backend/spark-performance/#figure-7-spark-stream-ui", 
            "text": "", 
            "title": "Figure 7. Spark stream UI"
        }, 
        {
            "location": "/backend/spark-performance/#figure-8-queries-running", 
            "text": "", 
            "title": "Figure 8. Queries running"
        }, 
        {
            "location": "/backend/spark-performance/#figure-9-cpu-time", 
            "text": "", 
            "title": "Figure 9. CPU time"
        }, 
        {
            "location": "/backend/spark-performance/#figure-10-heap-usage", 
            "text": "", 
            "title": "Figure 10. Heap usage"
        }, 
        {
            "location": "/backend/spark-performance/#figure-11-garbage-collection-time", 
            "text": "", 
            "title": "Figure 11. Garbage collection time"
        }, 
        {
            "location": "/backend/spark-performance/#figure-12-garbage-collection-count", 
            "text": "", 
            "title": "Figure 12. Garbage collection count"
        }, 
        {
            "location": "/backend/spark-performance/#conclusion_2", 
            "text": "The average processing time for each batch was 3 second 97 ms which was below the batch duration 5 seconds. For average, 1.2 CPU core and average 5GB memory were used in this experiment. But with queries number goes up, some of the executors memory usage were up to 8-10GB which is close to our resource limits. With more queries running, OOM may happen. So in this experiment, we can only afford up to 800 queries simultaneously.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/pubsub/architecture/", 
            "text": "PubSub Architecture\n\n\nThis section describes how the Publish-Subscribe or \nPubSub layer\n works in Bullet.\n\n\nWhy a PubSub?\n\n\nWhen we initially created Bullet, it was built on \nApache Storm\n and leveraged a feature in it called Storm DRPC to deliver queries to and extract results from the Bullet Backend. Storm DRPC is supported by a set of clusters that are physically part of the Storm cluster and is a shared resource for the cluster. While many other stream processors support some form of RPC and we could support multiple versions of the Web Service for those, it quickly became clear that abstracting the transport layer from the Web Service to the Backend was needed. This was particularly highlighted when we wanted to switch Bullet queries from operating in a request-response model (one response at the end of the query) to a streaming model. Streaming responses back to the user for a query through DRPC would be cumbersome and require a lot of logic to handle. A PubSub system was a natural solution to this. Since DRPC was a shared resource per cluster, we also were \ntying the Backend's scalability\n to a resource that we didn't control.\n\n\nHowever, we didn't want to pick a particular PubSub like Kafka and restrict a user's choice. So, we added a PubSub layer that was generic and entirely pluggable into both the Backend and the Web Service. We would support a select few like \nKafka\n or \nStorm DRPC\n. See \nbelow\n for how to create your own.\n\n\nWith the transport mechanism abstracted out, it opens up a lot of possibilities like implementing Bullet on other stream processors, allowing for the development of \nBullet on Spark\n along with other possible implementations in the future.\n\n\nWhat does it do?\n\n\nA PubSub operates in two contexts:\n\n\n\n\nSubmitting queries and reading results. This is the \nQUERY_SUBMISSION\n context and this is PubSub mode for the Web Service\n\n\nReading queries and submitting results. This is the \nQUERY_PROCESSING\n context and this is PubSub mode for the Backend\n\n\n\n\nA PubSub provides Publisher and Subscriber instances that, depending on which context it is in, do the right thing. Publishers in \nQUERY_SUBMISSION\n write queries to your PubSub whereas in \nQUERY_PROCESSING\n, they write results. Similarly, the Subscribers in \nQUERY_SUBMISSION\n read results but read queries in \nQUERY_PROCESSING\n. A Publisher and Subscriber in a particular context make up read and write halves of the \npipes\n for stream of queries and stream of results.\n\n\nMessages\n\n\nThe PubSub layer does not deal with queries and results and just works on instances of messages of type \ncom.yahoo.bullet.pubsub.PubSubMessage\n. These \nPubSubMessages\n are keyed (\nid\n and \nsequence\n), store content and metadata. This is a light wrapper around the payload and is tailored to work with multiple results per query and support communicating additional information and signals to and from the PubSub in addition to just queries and results.\n\n\nChoosing a PubSub implementation\n\n\nIf you want to use an implementation already built, we currently support:\n\n\n\n\nKafka\n for any Backend\n\n\nREST\n for any Backend\n\n\nStorm DRPC\n if you're using Bullet on Storm as your Backend\n\n\n\n\nImplementing your own PubSub\n\n\nThe core of the PubSub interfaces are defined in the \ncore Bullet library\n that you can \ndepend on\n.\n\n\nTo create a PubSub, you should extend the abstract class \ncom.yahoo.bullet.pubsub.PubSub\n and implement the abstract methods for getting instances of Publishers (\ncom.yahoo.bullet.pubsub.Publisher\n) and Subscribers (\ncom.yahoo.bullet.pubsub.Subscriber\n). Depending on how you have configured the Web Service and the Backend, they will call the required methods to get the required number of Publishers or Subscribers to parallelize the reading or the writing. You should ensure that they are thread-safe. They will most likely be tied to your units of parallelisms for the underlying PubSub you are invoking.\n\n\nIf you are running sharded instances of your Web Service, you should ensure that your Publishers writing queries add Metadata to the messages to help the Publishers writing results to send the results back to the right Web Service instance that is waiting for them.\n\n\nReliability\n\n\nYou can choose to make your Publishers and Subscribers as reliable as you want. Both the Web Service and the Backend will call the appropriate reliability methods (\ncommit\n and \nfail\n) but your implementations can choose to be no-ops if you do not want to be reliability. Alternatively, if you want make your Subscribers reliable, you could use an in-memory reliable implementation of one by extending \ncom.yahoo.bullet.pubsub.BufferingSubscriber\n for a simple implementation. This keeps track of uncommitted messages in memory up to a configured threshold (does not read more messages if there are this many uncommitted messages left) and re-emits messages on failures using it.\n\n\nCanonical example\n\n\nFor an example of a PubSub implementation, see the \nBullet Kafka PubSub project\n. This is implemented in Java and is a simple implementation that wraps the Kafka client APIs. It supports reliability through the use of the \nBufferingSubscriber\n mentioned above. It allows you to specify one or two Kafka topics for queries and results. It can be sharded across multiple Web Service machines using Kafka topic partitions. See the \nconfiguration\n for details.", 
            "title": "Architecture"
        }, 
        {
            "location": "/pubsub/architecture/#pubsub-architecture", 
            "text": "This section describes how the Publish-Subscribe or  PubSub layer  works in Bullet.", 
            "title": "PubSub Architecture"
        }, 
        {
            "location": "/pubsub/architecture/#why-a-pubsub", 
            "text": "When we initially created Bullet, it was built on  Apache Storm  and leveraged a feature in it called Storm DRPC to deliver queries to and extract results from the Bullet Backend. Storm DRPC is supported by a set of clusters that are physically part of the Storm cluster and is a shared resource for the cluster. While many other stream processors support some form of RPC and we could support multiple versions of the Web Service for those, it quickly became clear that abstracting the transport layer from the Web Service to the Backend was needed. This was particularly highlighted when we wanted to switch Bullet queries from operating in a request-response model (one response at the end of the query) to a streaming model. Streaming responses back to the user for a query through DRPC would be cumbersome and require a lot of logic to handle. A PubSub system was a natural solution to this. Since DRPC was a shared resource per cluster, we also were  tying the Backend's scalability  to a resource that we didn't control.  However, we didn't want to pick a particular PubSub like Kafka and restrict a user's choice. So, we added a PubSub layer that was generic and entirely pluggable into both the Backend and the Web Service. We would support a select few like  Kafka  or  Storm DRPC . See  below  for how to create your own.  With the transport mechanism abstracted out, it opens up a lot of possibilities like implementing Bullet on other stream processors, allowing for the development of  Bullet on Spark  along with other possible implementations in the future.", 
            "title": "Why a PubSub?"
        }, 
        {
            "location": "/pubsub/architecture/#what-does-it-do", 
            "text": "A PubSub operates in two contexts:   Submitting queries and reading results. This is the  QUERY_SUBMISSION  context and this is PubSub mode for the Web Service  Reading queries and submitting results. This is the  QUERY_PROCESSING  context and this is PubSub mode for the Backend   A PubSub provides Publisher and Subscriber instances that, depending on which context it is in, do the right thing. Publishers in  QUERY_SUBMISSION  write queries to your PubSub whereas in  QUERY_PROCESSING , they write results. Similarly, the Subscribers in  QUERY_SUBMISSION  read results but read queries in  QUERY_PROCESSING . A Publisher and Subscriber in a particular context make up read and write halves of the  pipes  for stream of queries and stream of results.", 
            "title": "What does it do?"
        }, 
        {
            "location": "/pubsub/architecture/#messages", 
            "text": "The PubSub layer does not deal with queries and results and just works on instances of messages of type  com.yahoo.bullet.pubsub.PubSubMessage . These  PubSubMessages  are keyed ( id  and  sequence ), store content and metadata. This is a light wrapper around the payload and is tailored to work with multiple results per query and support communicating additional information and signals to and from the PubSub in addition to just queries and results.", 
            "title": "Messages"
        }, 
        {
            "location": "/pubsub/architecture/#choosing-a-pubsub-implementation", 
            "text": "If you want to use an implementation already built, we currently support:   Kafka  for any Backend  REST  for any Backend  Storm DRPC  if you're using Bullet on Storm as your Backend", 
            "title": "Choosing a PubSub implementation"
        }, 
        {
            "location": "/pubsub/architecture/#implementing-your-own-pubsub", 
            "text": "The core of the PubSub interfaces are defined in the  core Bullet library  that you can  depend on .  To create a PubSub, you should extend the abstract class  com.yahoo.bullet.pubsub.PubSub  and implement the abstract methods for getting instances of Publishers ( com.yahoo.bullet.pubsub.Publisher ) and Subscribers ( com.yahoo.bullet.pubsub.Subscriber ). Depending on how you have configured the Web Service and the Backend, they will call the required methods to get the required number of Publishers or Subscribers to parallelize the reading or the writing. You should ensure that they are thread-safe. They will most likely be tied to your units of parallelisms for the underlying PubSub you are invoking.  If you are running sharded instances of your Web Service, you should ensure that your Publishers writing queries add Metadata to the messages to help the Publishers writing results to send the results back to the right Web Service instance that is waiting for them.", 
            "title": "Implementing your own PubSub"
        }, 
        {
            "location": "/pubsub/architecture/#reliability", 
            "text": "You can choose to make your Publishers and Subscribers as reliable as you want. Both the Web Service and the Backend will call the appropriate reliability methods ( commit  and  fail ) but your implementations can choose to be no-ops if you do not want to be reliability. Alternatively, if you want make your Subscribers reliable, you could use an in-memory reliable implementation of one by extending  com.yahoo.bullet.pubsub.BufferingSubscriber  for a simple implementation. This keeps track of uncommitted messages in memory up to a configured threshold (does not read more messages if there are this many uncommitted messages left) and re-emits messages on failures using it.", 
            "title": "Reliability"
        }, 
        {
            "location": "/pubsub/architecture/#canonical-example", 
            "text": "For an example of a PubSub implementation, see the  Bullet Kafka PubSub project . This is implemented in Java and is a simple implementation that wraps the Kafka client APIs. It supports reliability through the use of the  BufferingSubscriber  mentioned above. It allows you to specify one or two Kafka topics for queries and results. It can be sharded across multiple Web Service machines using Kafka topic partitions. See the  configuration  for details.", 
            "title": "Canonical example"
        }, 
        {
            "location": "/pubsub/kafka/", 
            "text": "Kafka PubSub\n\n\nThe Kafka implementation of the Bullet PubSub can be used on any Backend and Web Service. It uses \nApache Kafka\n as the backing PubSub queue and works on all Backends.\n\n\nHow does it work?\n\n\nThe implementation by default asks you to create two topics in a Kafka cluster - one for queries and another for results. The Web Service publishes queries to the queries topic and reads results from the results topic. Similarly, the Backend reads queries from the queries topic and writes results to the results topic. All messages are sent as \nPubSubMessages\n.\n\n\nYou do not need to have two topics. You can have one but you should use multiple partitions and configure your Web Service and Backend to produce to and consume from the right partitions. See the \nsetup\n section for more details.\n\n\n\n\nKafka Client API\n\n\nThe Bullet Kafka implementation uses the Kafka 0.10.2 client APIs. Generally, your forward or backward compatibilities should work as expected.\n\n\n\n\nSetup\n\n\nBefore setting up, you will need a Kafka cluster setup with your topic(s) created. This cluster need only be a couple of machines if it's devoted for Bullet. However, this depends on your query and result volumes. Generally, these are at most a few hundred or thousands of messages per second and a small Kafka cluster will suffice.\n\n\nTo setup Kafka, follow the \ninstructions here\n.\n\n\nPlug into the Backend\n\n\nDepending on how your Backend is built, either add Bullet Kafka to your classpath or include it in your build tool. Head over to our \nreleases page\n for getting the artifacts. If you're adding Bullet Kafka to the classpath instead of building a fat jar, you will need to get the jar with the classifier: \nfat\n since you will need Bullet Kafka and all its dependencies.\n\n\nConfigure the backend to use the Kafka PubSub:\n\n\nbullet.pubsub.context.name: \nQUERY_PROCESSING\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.kafka.KafkaPubSub\n\nbullet.pubsub.kafka.bootstrap.servers: \nserver1:port1,server2:port2,...\n\nbullet.pubsub.kafka.request.topic.name: \nyour-query-topic\n\nbullet.pubsub.kafka.response.topic.name: \nyour-result-topic\n\n\n\n\n\nYou will then need to configure the Publishers and Subscribers. For details on what to configure and what the defaults are, see the \nconfiguration file\n.\n\n\nPlug into the Web Service\n\n\nYou will need the Head over to our \nreleases page\n and get the JAR artifact with the \nfat\n classifier. For example, you can download the artifact for the 0.2.0 release \ndirectly from JCenter\n).\n\n\nYou should then plug in this JAR to your Web Service following the instructions \nhere\n.\n\n\nFor configuration, you should \nfollow the steps here\n to create and provide a YAML file to the Web Service. Remember to change the context to \nQUERY_SUBMISSION\n.\n\n\nbullet.pubsub.context.name: \nQUERY_SUBMISSION\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.kafka.KafkaPubSub\n\nbullet.pubsub.kafka.bootstrap.servers: \nserver1:port1,server2:port2,...\n\nbullet.pubsub.kafka.request.topic.name: \nyour-query-topic\n\nbullet.pubsub.kafka.response.topic.name: \nyour-result-topic\n\n\n\n\n\nAs with the Backend, you will then need to configure the Publishers and Subscribers. See the \nconfiguration file\n. Remember that your Subscribers in the Backend are reading what the Producers in your Web Service are producing and vice-versa, so make sure to match up the topics and settings accordingly if you have any custom changes.\n\n\nPassthrough Configuration\n\n\nYou can pass additional Kafka Producer or Consumer properties to the PubSub Publishers and Subscribers by prefixing them with either \nbullet.pubsub.kafka.producer.\n for Producers or \nbullet.pubsub.kafka.consumer.\n for Consumers. The PubSub configuration uses and provides a few defaults for settings it thinks is important to manage. You can tweak them and add others. For a list of properties that you can configure, see the \nProducer\n or \nConsumer\n configs in Kafka.\n\n\n\n\nTypes for the properties\n\n\nAll Kafka properties are better off specified as Strings since Kafka type casts them accordingly. If you provide types, you might run into issues where YAML types do not match what the Kafka client is expecting.\n\n\n\n\nPartitions\n\n\nYou may choose to partition your topics for a couple of reasons:\n\n\n\n\nYou may have one topic for both queries and responses and use partitions as a way to separate them.\n\n\nYou may use two topics and partition one or both for scalability when reading and writing\n\n\nYou may use two topics and partition one or both for sharding across multiple Web Service instances (and multiple instances in your Backend)\n\n\n\n\nYou can accomplish all this with partition maps. You can configure what partitions your Publishers (Web Service or Backend) will write to using \nbullet.pubsub.kafka.request.partitions\n and what partitions your Subscribers will read from using \nbullet.pubsub.kafka.response.partitions\n. Providing these to an instance of the Web Service or the Backend in the YAML file ensures that the Publishers in that instance only write to these request partitions and Subscribers only read from the response partitions. The Publishers will randomly adds one of the response partitions in the messages sent to ensure that the responses only arrive to one of those partitions this instance's Subscribers are waiting on. For more details, see the \nconfiguration file\n.\n\n\nSecurity\n\n\nIf you're using secure Kafka, you will need to do the necessary metadata setup to make sure your principals have access to your topic(s) for reading and writing. If you're using SSL for securing your Kafka cluster, you will need to add the necessary SSL certificates to the keystore for your JVM before launching the Web Service or the Backend.\n\n\nStorm\n\n\nWe have tested Kafka with \nBullet Storm\n using \nKerberos\n from the Storm cluster and SSL from the Web Service. For Kerberos, you may need to add a \nJAAS\n \nconfig file\n to the \nStorm BlobStore\n and add it to your worker JVMs. To do this, you will need a JAAS configuration entry. For example, if your Kerberos KDC is shared with your Storm cluster's KDC, you may be adding a jaas_file.conf with\n\n\nKafkaClient {\n   org.apache.storm.security.auth.kerberos.AutoTGTKrb5LoginModule required\n   serviceName=\nkafka\n;\n};\n\n\n\n\nPut this file into Storm's BlobStore using:\n\n\nstorm blobstore create --file jaas_file.conf --acl o::rwa,u:$USER:rwa --repl-fctr 3 jaas_file.conf\n\n\n\n\nThen while launching your topology, you should provide as arguments to the \nstorm jar\n command, the following arguments:\n\n\n-c topology.blobstore.map='{\njaas_file.conf\n: {} }' \\\n-c topology.worker.childopts=\n-Djava.security.auth.login.config=./jaas_file.conf\n \\\n\n\n\n\nThis will add this to all your worker JVMs. You can refresh Kerberos credentials periodically and push credentials to Storm as \nmentioned here\n.", 
            "title": "Kafka"
        }, 
        {
            "location": "/pubsub/kafka/#kafka-pubsub", 
            "text": "The Kafka implementation of the Bullet PubSub can be used on any Backend and Web Service. It uses  Apache Kafka  as the backing PubSub queue and works on all Backends.", 
            "title": "Kafka PubSub"
        }, 
        {
            "location": "/pubsub/kafka/#how-does-it-work", 
            "text": "The implementation by default asks you to create two topics in a Kafka cluster - one for queries and another for results. The Web Service publishes queries to the queries topic and reads results from the results topic. Similarly, the Backend reads queries from the queries topic and writes results to the results topic. All messages are sent as  PubSubMessages .  You do not need to have two topics. You can have one but you should use multiple partitions and configure your Web Service and Backend to produce to and consume from the right partitions. See the  setup  section for more details.   Kafka Client API  The Bullet Kafka implementation uses the Kafka 0.10.2 client APIs. Generally, your forward or backward compatibilities should work as expected.", 
            "title": "How does it work?"
        }, 
        {
            "location": "/pubsub/kafka/#setup", 
            "text": "Before setting up, you will need a Kafka cluster setup with your topic(s) created. This cluster need only be a couple of machines if it's devoted for Bullet. However, this depends on your query and result volumes. Generally, these are at most a few hundred or thousands of messages per second and a small Kafka cluster will suffice.  To setup Kafka, follow the  instructions here .", 
            "title": "Setup"
        }, 
        {
            "location": "/pubsub/kafka/#plug-into-the-backend", 
            "text": "Depending on how your Backend is built, either add Bullet Kafka to your classpath or include it in your build tool. Head over to our  releases page  for getting the artifacts. If you're adding Bullet Kafka to the classpath instead of building a fat jar, you will need to get the jar with the classifier:  fat  since you will need Bullet Kafka and all its dependencies.  Configure the backend to use the Kafka PubSub:  bullet.pubsub.context.name:  QUERY_PROCESSING \nbullet.pubsub.class.name:  com.yahoo.bullet.kafka.KafkaPubSub \nbullet.pubsub.kafka.bootstrap.servers:  server1:port1,server2:port2,... \nbullet.pubsub.kafka.request.topic.name:  your-query-topic \nbullet.pubsub.kafka.response.topic.name:  your-result-topic   You will then need to configure the Publishers and Subscribers. For details on what to configure and what the defaults are, see the  configuration file .", 
            "title": "Plug into the Backend"
        }, 
        {
            "location": "/pubsub/kafka/#plug-into-the-web-service", 
            "text": "You will need the Head over to our  releases page  and get the JAR artifact with the  fat  classifier. For example, you can download the artifact for the 0.2.0 release  directly from JCenter ).  You should then plug in this JAR to your Web Service following the instructions  here .  For configuration, you should  follow the steps here  to create and provide a YAML file to the Web Service. Remember to change the context to  QUERY_SUBMISSION .  bullet.pubsub.context.name:  QUERY_SUBMISSION \nbullet.pubsub.class.name:  com.yahoo.bullet.kafka.KafkaPubSub \nbullet.pubsub.kafka.bootstrap.servers:  server1:port1,server2:port2,... \nbullet.pubsub.kafka.request.topic.name:  your-query-topic \nbullet.pubsub.kafka.response.topic.name:  your-result-topic   As with the Backend, you will then need to configure the Publishers and Subscribers. See the  configuration file . Remember that your Subscribers in the Backend are reading what the Producers in your Web Service are producing and vice-versa, so make sure to match up the topics and settings accordingly if you have any custom changes.", 
            "title": "Plug into the Web Service"
        }, 
        {
            "location": "/pubsub/kafka/#passthrough-configuration", 
            "text": "You can pass additional Kafka Producer or Consumer properties to the PubSub Publishers and Subscribers by prefixing them with either  bullet.pubsub.kafka.producer.  for Producers or  bullet.pubsub.kafka.consumer.  for Consumers. The PubSub configuration uses and provides a few defaults for settings it thinks is important to manage. You can tweak them and add others. For a list of properties that you can configure, see the  Producer  or  Consumer  configs in Kafka.   Types for the properties  All Kafka properties are better off specified as Strings since Kafka type casts them accordingly. If you provide types, you might run into issues where YAML types do not match what the Kafka client is expecting.", 
            "title": "Passthrough Configuration"
        }, 
        {
            "location": "/pubsub/kafka/#partitions", 
            "text": "You may choose to partition your topics for a couple of reasons:   You may have one topic for both queries and responses and use partitions as a way to separate them.  You may use two topics and partition one or both for scalability when reading and writing  You may use two topics and partition one or both for sharding across multiple Web Service instances (and multiple instances in your Backend)   You can accomplish all this with partition maps. You can configure what partitions your Publishers (Web Service or Backend) will write to using  bullet.pubsub.kafka.request.partitions  and what partitions your Subscribers will read from using  bullet.pubsub.kafka.response.partitions . Providing these to an instance of the Web Service or the Backend in the YAML file ensures that the Publishers in that instance only write to these request partitions and Subscribers only read from the response partitions. The Publishers will randomly adds one of the response partitions in the messages sent to ensure that the responses only arrive to one of those partitions this instance's Subscribers are waiting on. For more details, see the  configuration file .", 
            "title": "Partitions"
        }, 
        {
            "location": "/pubsub/kafka/#security", 
            "text": "If you're using secure Kafka, you will need to do the necessary metadata setup to make sure your principals have access to your topic(s) for reading and writing. If you're using SSL for securing your Kafka cluster, you will need to add the necessary SSL certificates to the keystore for your JVM before launching the Web Service or the Backend.", 
            "title": "Security"
        }, 
        {
            "location": "/pubsub/kafka/#storm", 
            "text": "We have tested Kafka with  Bullet Storm  using  Kerberos  from the Storm cluster and SSL from the Web Service. For Kerberos, you may need to add a  JAAS   config file  to the  Storm BlobStore  and add it to your worker JVMs. To do this, you will need a JAAS configuration entry. For example, if your Kerberos KDC is shared with your Storm cluster's KDC, you may be adding a jaas_file.conf with  KafkaClient {\n   org.apache.storm.security.auth.kerberos.AutoTGTKrb5LoginModule required\n   serviceName= kafka ;\n};  Put this file into Storm's BlobStore using:  storm blobstore create --file jaas_file.conf --acl o::rwa,u:$USER:rwa --repl-fctr 3 jaas_file.conf  Then while launching your topology, you should provide as arguments to the  storm jar  command, the following arguments:  -c topology.blobstore.map='{ jaas_file.conf : {} }' \\\n-c topology.worker.childopts= -Djava.security.auth.login.config=./jaas_file.conf  \\  This will add this to all your worker JVMs. You can refresh Kerberos credentials periodically and push credentials to Storm as  mentioned here .", 
            "title": "Storm"
        }, 
        {
            "location": "/pubsub/rest/", 
            "text": "REST PubSub\n\n\nThe REST PubSub implementation is included in bullet-core, and can be launched along with the Web Service. If it is enabled the Web Service will expose two additional REST endpoints, one for reading/writing Bullet queries, and one\nfor reading/writing results.\n\n\nHow does it work?\n\n\nWhen the Web Service receives a query from a user, it will create a PubSubMessage and write the message to the \"query\" RESTPubSub endpoint. This PubSubMessage will contain not only the query, but also some metadata, including the\nappropriate host/port to which the response should be sent (this is done to allow for multiple Web Services running simultaneously). The query is then stored in memory until the backend does a GET from this endpoint, at which\ntime the query will be served to the backend, and dropped from the queue in memory.\n\n\nOnce the backed has generated the results of the query, it will wrap those results in PubSubMessage. The backend extracts the URL to send the results to from the metadata and writes the results PubSubMessage to the\n\"results\" REST endpoint with a POST. This result will then be stored in memory until the Web Service does a GET to that endpoint, at which time the Web Service will have the results of the query to send back to the user.\n\n\nSetup\n\n\nTo enable the RESTPubSub and expose the two additional necessary REST endpoints, you must enable the setting:\n\n\nbullet.pubsub.builtin.rest.enabled: true\n\n\n\n\n...in the Web Service Application.yaml file. This can also be done from the command line when launching the Web Service jar file by adding the command-line option:\n\n\n--bullet.pubsub.builtin.rest.enabled=true\n\n\n\n\nThis will enable the two necessary REST endpoints, the paths for which can be configured in the Application.yaml file with the settings:\n\n\nbullet.pubsub.builtin.rest.query.path: /pubsub/query\nbullet.pubsub.builtin.rest.result.path: /pubsub/result\n\n\n\n\nPlug into the Backend\n\n\nConfigure the backend to use the REST PubSub:\n\n\nbullet.pubsub.context.name: \nQUERY_PROCESSING\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.kafka.KafkaPubSub\n\n\nbullet.pubsub.rest.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.result.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.urls:\n    - \nhttp://webServiceHostNameA:9901/api/bullet/pubsub/query\n\n    - \nhttp://webServiceHostNameB:9902/api/bullet/pubsub/query\n\n\n\n\n\n\n\nbullet.pubsub.context.name: \"QUERY_PROCESSING\"\n - tells the PubSub that it is running in the backend\n\n\nbullet.pubsub.class.name: \"com.yahoo.bullet.kafka.KafkaPubSub\"\n - tells Bullet to use this class for it's PubSub\n\n\nbullet.pubsub.rest.connect.timeout.ms: 5000\n - sets the HTTP connect timeout to a half second\n\n\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\n - this is the maxiumum number of uncommitted messages allowed before blocking\n\n\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10\n - this setting is used to avoid making an http request too rapidly and overloading the http endpoint. It will force the backend to poll the query endpoint at most once every 10ms.\n\n\nbullet.pubsub.rest.query.urls\n - this should be a list of all the query rest enpoint URLs. If you are only running one Web Service this will only contain one url (the url of your Web Service followed by the full path of the query endpoint).\n\n\n\n\nPlug into the Web Service\n\n\nConfigure the Web Service to use the REST PubSub:\n\n\nbullet.pubsub.context.name: \nQUERY_SUBMISSION\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.kafka.KafkaPubSub\n\n\nbullet.pubsub.rest.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.result.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.result.url: \nhttp://localhost:9901/api/bullet/pubsub/result\n\nbullet.pubsub.rest.query.urls:\n    - \nhttp://localhost:9901/api/bullet/pubsub/query\n\n\n\n\n\n\n\nbullet.pubsub.context.name: \"QUERY_SUBMISSION\"\n - tells the PubSub that it is running in the Web Service\n\n\nbullet.pubsub.class.name: \"com.yahoo.bullet.kafka.KafkaPubSub\"\n - tells Bullet to use this class for it's PubSub\n\n\nbullet.pubsub.rest.connect.timeout.ms: 5000\n - sets the HTTP connect timeout to a half second\n\n\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\n - this is the maxiumum number of uncommitted messages allowed before blocking\n\n\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10\n - this setting is used to avoid making an http request too rapidly and overloading the http endpoint. It will force the backend to poll the query endpoint at most once every 10ms.\n\n\nbullet.pubsub.rest.result.url: \"http://localhost:9901/api/bullet/pubsub/result\"\n - this is the endpoint from which the WebService should read results - it should generally be the hostname of that machine the Web Service is running on (or \"localhost\").\n\n\nbullet.pubsub.rest.query.urls\n - in the Web Service this setting should contain \nexactly one\n url - the url to which queries should be written  - it should generally be the hostname of that machine the Web Service is running on (or \"localhost\").", 
            "title": "REST"
        }, 
        {
            "location": "/pubsub/rest/#rest-pubsub", 
            "text": "The REST PubSub implementation is included in bullet-core, and can be launched along with the Web Service. If it is enabled the Web Service will expose two additional REST endpoints, one for reading/writing Bullet queries, and one\nfor reading/writing results.", 
            "title": "REST PubSub"
        }, 
        {
            "location": "/pubsub/rest/#how-does-it-work", 
            "text": "When the Web Service receives a query from a user, it will create a PubSubMessage and write the message to the \"query\" RESTPubSub endpoint. This PubSubMessage will contain not only the query, but also some metadata, including the\nappropriate host/port to which the response should be sent (this is done to allow for multiple Web Services running simultaneously). The query is then stored in memory until the backend does a GET from this endpoint, at which\ntime the query will be served to the backend, and dropped from the queue in memory.  Once the backed has generated the results of the query, it will wrap those results in PubSubMessage. The backend extracts the URL to send the results to from the metadata and writes the results PubSubMessage to the\n\"results\" REST endpoint with a POST. This result will then be stored in memory until the Web Service does a GET to that endpoint, at which time the Web Service will have the results of the query to send back to the user.", 
            "title": "How does it work?"
        }, 
        {
            "location": "/pubsub/rest/#setup", 
            "text": "To enable the RESTPubSub and expose the two additional necessary REST endpoints, you must enable the setting:  bullet.pubsub.builtin.rest.enabled: true  ...in the Web Service Application.yaml file. This can also be done from the command line when launching the Web Service jar file by adding the command-line option:  --bullet.pubsub.builtin.rest.enabled=true  This will enable the two necessary REST endpoints, the paths for which can be configured in the Application.yaml file with the settings:  bullet.pubsub.builtin.rest.query.path: /pubsub/query\nbullet.pubsub.builtin.rest.result.path: /pubsub/result", 
            "title": "Setup"
        }, 
        {
            "location": "/pubsub/rest/#plug-into-the-backend", 
            "text": "Configure the backend to use the REST PubSub:  bullet.pubsub.context.name:  QUERY_PROCESSING \nbullet.pubsub.class.name:  com.yahoo.bullet.kafka.KafkaPubSub \n\nbullet.pubsub.rest.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.result.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.urls:\n    -  http://webServiceHostNameA:9901/api/bullet/pubsub/query \n    -  http://webServiceHostNameB:9902/api/bullet/pubsub/query    bullet.pubsub.context.name: \"QUERY_PROCESSING\"  - tells the PubSub that it is running in the backend  bullet.pubsub.class.name: \"com.yahoo.bullet.kafka.KafkaPubSub\"  - tells Bullet to use this class for it's PubSub  bullet.pubsub.rest.connect.timeout.ms: 5000  - sets the HTTP connect timeout to a half second  bullet.pubsub.rest.subscriber.max.uncommitted.messages: 100  - this is the maxiumum number of uncommitted messages allowed before blocking  bullet.pubsub.rest.query.subscriber.min.wait.ms: 10  - this setting is used to avoid making an http request too rapidly and overloading the http endpoint. It will force the backend to poll the query endpoint at most once every 10ms.  bullet.pubsub.rest.query.urls  - this should be a list of all the query rest enpoint URLs. If you are only running one Web Service this will only contain one url (the url of your Web Service followed by the full path of the query endpoint).", 
            "title": "Plug into the Backend"
        }, 
        {
            "location": "/pubsub/rest/#plug-into-the-web-service", 
            "text": "Configure the Web Service to use the REST PubSub:  bullet.pubsub.context.name:  QUERY_SUBMISSION \nbullet.pubsub.class.name:  com.yahoo.bullet.kafka.KafkaPubSub \n\nbullet.pubsub.rest.connect.timeout.ms: 5000\nbullet.pubsub.rest.subscriber.max.uncommitted.messages: 100\nbullet.pubsub.rest.result.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.query.subscriber.min.wait.ms: 10\nbullet.pubsub.rest.result.url:  http://localhost:9901/api/bullet/pubsub/result \nbullet.pubsub.rest.query.urls:\n    -  http://localhost:9901/api/bullet/pubsub/query    bullet.pubsub.context.name: \"QUERY_SUBMISSION\"  - tells the PubSub that it is running in the Web Service  bullet.pubsub.class.name: \"com.yahoo.bullet.kafka.KafkaPubSub\"  - tells Bullet to use this class for it's PubSub  bullet.pubsub.rest.connect.timeout.ms: 5000  - sets the HTTP connect timeout to a half second  bullet.pubsub.rest.subscriber.max.uncommitted.messages: 100  - this is the maxiumum number of uncommitted messages allowed before blocking  bullet.pubsub.rest.query.subscriber.min.wait.ms: 10  - this setting is used to avoid making an http request too rapidly and overloading the http endpoint. It will force the backend to poll the query endpoint at most once every 10ms.  bullet.pubsub.rest.result.url: \"http://localhost:9901/api/bullet/pubsub/result\"  - this is the endpoint from which the WebService should read results - it should generally be the hostname of that machine the Web Service is running on (or \"localhost\").  bullet.pubsub.rest.query.urls  - in the Web Service this setting should contain  exactly one  url - the url to which queries should be written  - it should generally be the hostname of that machine the Web Service is running on (or \"localhost\").", 
            "title": "Plug into the Web Service"
        }, 
        {
            "location": "/pubsub/storm-drpc/", 
            "text": "Storm DRPC PubSub\n\n\n\n\nNOTE: This PubSub only works with old versions of the Storm Backend!\n\n\nSince DRPC is part of Storm, and can only support a single query/response model, this PubSub implementation can only be used with the Storm backend, and cannot support Windowed queries (bullet-storm 0.8.0 and later).\n\n\n\n\nBullet on \nStorm\n can use \nStorm DRPC\n as a PubSub layer. DRPC or Distributed Remote Procedure Call, is built into Storm and consists of a set of servers that are part of the Storm cluster.\n\n\nHow does it work?\n\n\nWhen a Storm topology that uses DRPC is launched, it registers a spout with a unique name (the procedure in the Distributed Remote Procedure Call) with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology that registered that name (Bullet). The result from topology is sent back to the client. We picked Storm to implement Bullet on first not only because it was the most popular Streaming framework at Yahoo but also since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.\n\n\nYou can communicate with DRPC using \nApache Thrift\n or REST. Our implementation uses REST. The Web Service sends a JSON serialized PubSubMessage with the query in it through HTTP and asynchronously waits for the results back through DRPC.\n\n\n\n\nREST and DRPC\n\n\nWhile DRPC exposes a \nThrift\n endpoint, the PubSub implementation uses REST. When you launch your topology with the DRPC PubSub, you can POST a JSON Bullet PubSubMessage containing a String JSON query to a DRPC server directly with the function name that you specify in the \nBullet configuration\n. For example,\n\nbash\n  curl -s -X POST -d '{\"id\":\"\", \"content\":\"{}\"}' http://\nDRPC_SERVER\n:\nDRPC_PORT\n/drpc/\nDRPC_FUNCTION_FROM_YOUR_BULLET_CONF\n\n to get a random record (inside a JSON representation of a PubSubMessage) from your data instantly if you left the Raw aggregation micro-batch size at the default of 1. The \ncontent\n above in the JSON is the actual (empty) Bullet query. This is a quick way to check if your topology is up and running!\n\n\n\n\nSetup\n\n\nThe DRPC PubSub is part of the \nBullet Storm\n starting with versions 0.6.2 and above.\n\n\nPlug into the Storm Backend\n\n\nWhen you are setting up your Bullet topology with your plug-in data source (a Spout or a topology), you will naturally build a JAR with all the dependencies or a \nfat\n JAR. This will include all the DRPC PubSub code and dependencies. You do not need anything else. For configuration, the YAML file that you probably already provide to your topology needs to have the additional settings listed below (the function name is optional but you should change the default since the DRPC function needs to be unique per Storm cluster). Now if you launch your topology, it should be wired up to use Storm DRPC.\n\n\nbullet.pubsub.context.name: \nQUERY_PROCESSING\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.storm.drpc.DRPCPubSub\n\nbullet.pubsub.storm.drpc.function: \ncustom-name\n\n\n\n\n\nSecurity\n\n\nIf your Storm  cluster is secured with \nKerberos\n (a standard for Big Data platforms), you will need to periodically refresh your Kerberos TGT and push the credentials to your Storm topology. This is generally done with \nkinit\n for your topology user, followed by a \nstorm upload-credentials \nTOPOLOGY_NAME\n. You would probably run this as a \ncron\n task.\n\n\nPlug into the Web Service\n\n\nWhen you're plugging in the DRPC PubSub layer into your Web Service, you will need the Bullet Storm JAR with dependencies that you can download from \nJCenter\n. The classifier for this JAR is \nfat\n if you are depending on it through Maven. You can also download the JAR for the 0.6.2 version directly through \nJCenter here\n.\n\n\nYou should then plug in this JAR to your Web Service following the instructions \nhere\n.\n\n\nFor configuration, you should \nfollow the steps here\n and add the context and class name listed above. You will need to point to your DRPC servers and set the function to the same value you chose \nabove\n. You can configure this and other settings that are explained further in the \nPubSub and PubSub Storm DRPC defaults section\n in the Bullet Storm defaults file.\n\n\nbullet.pubsub.context.name: \nQUERY_SUBMISSION\n\nbullet.pubsub.class.name: \ncom.yahoo.bullet.storm.drpc.DRPCPubSub\n\nbullet.pubsub.storm.drpc.servers:\n  - server1\n  - server2\n  - server3\nbullet.pubsub.storm.drpc.function: \ncustom-name\n\nbullet.pubsub.storm.drpc.http.protocol: \nhttp\n\nbullet.pubsub.storm.drpc.http.port: \n4080\n\nbullet.pubsub.storm.drpc.http.path: \ndrpc\n\nbullet.pubsub.storm.drpc.http.connect.retry.limit: 3\nbullet.pubsub.storm.drpc.http.connect.timeout.ms: 1000\n\n\n\n\nCaveats with Storm DRPC\n\n\nScalability\n\n\nDRPC servers are a shared resource per Storm cluster and it may be possible that you have to contend with other topologies in your multi-tenant cluster. While it is horizontally scalable, it does tie the scalability of the Bullet backend to it. If you only have a few DRPC servers in your Storm cluster, you may need to add more to support more simultaneous DRPC requests. We have \nfound that\n each server gives us about ~250 simultaneous queries. There is an Async implementation coming in Storm 2.0 that should increase the throughput.\n\n\nQuery Duration\n\n\nThe maximum time a query can run for depends on the maximum time Storm DRPC request can last in your Storm topology. Generally the default is set to 10 minutes. This means that the \nlongest query duration possible will be 10 minutes\n. The value of this is up to your cluster maintainers.\n\n\nRequest-Response\n\n\nOur PubSub uses DRPC using HTTP REST in a request-response model. This means that it will not support incremental results as it is! We could switch our usage of DRPC to send signals to the topology to fetch results and start queries. Depending on if there is demand, we may support this in our implementation in the future.\n\n\nReliability\n\n\nStorm DRPC follows the principle of leaving retries to the DRPC user (in our case, the Bullet web service). At this moment, we have not chosen to add reliability mechanisms to the query publishing, result publishing or result subscribing sides of our DRPC PubSub implementations but the query subscribers do use the \nBufferingSubscriber\n mentioned \nhere\n.", 
            "title": "Storm DRPC"
        }, 
        {
            "location": "/pubsub/storm-drpc/#storm-drpc-pubsub", 
            "text": "NOTE: This PubSub only works with old versions of the Storm Backend!  Since DRPC is part of Storm, and can only support a single query/response model, this PubSub implementation can only be used with the Storm backend, and cannot support Windowed queries (bullet-storm 0.8.0 and later).   Bullet on  Storm  can use  Storm DRPC  as a PubSub layer. DRPC or Distributed Remote Procedure Call, is built into Storm and consists of a set of servers that are part of the Storm cluster.", 
            "title": "Storm DRPC PubSub"
        }, 
        {
            "location": "/pubsub/storm-drpc/#how-does-it-work", 
            "text": "When a Storm topology that uses DRPC is launched, it registers a spout with a unique name (the procedure in the Distributed Remote Procedure Call) with the DRPC infrastructure. The DRPC Servers expose a REST endpoint where data can be POSTed to or a GET request can be made with this unique name. The DRPC infrastructure then sends the request (a query in Bullet) through the spout(s) to the topology that registered that name (Bullet). The result from topology is sent back to the client. We picked Storm to implement Bullet on first not only because it was the most popular Streaming framework at Yahoo but also since DRPC provides us a nice and simple way to handle getting queries into Bullet and sending responses back.  You can communicate with DRPC using  Apache Thrift  or REST. Our implementation uses REST. The Web Service sends a JSON serialized PubSubMessage with the query in it through HTTP and asynchronously waits for the results back through DRPC.   REST and DRPC  While DRPC exposes a  Thrift  endpoint, the PubSub implementation uses REST. When you launch your topology with the DRPC PubSub, you can POST a JSON Bullet PubSubMessage containing a String JSON query to a DRPC server directly with the function name that you specify in the  Bullet configuration . For example, bash\n  curl -s -X POST -d '{\"id\":\"\", \"content\":\"{}\"}' http:// DRPC_SERVER : DRPC_PORT /drpc/ DRPC_FUNCTION_FROM_YOUR_BULLET_CONF \n to get a random record (inside a JSON representation of a PubSubMessage) from your data instantly if you left the Raw aggregation micro-batch size at the default of 1. The  content  above in the JSON is the actual (empty) Bullet query. This is a quick way to check if your topology is up and running!", 
            "title": "How does it work?"
        }, 
        {
            "location": "/pubsub/storm-drpc/#setup", 
            "text": "The DRPC PubSub is part of the  Bullet Storm  starting with versions 0.6.2 and above.", 
            "title": "Setup"
        }, 
        {
            "location": "/pubsub/storm-drpc/#plug-into-the-storm-backend", 
            "text": "When you are setting up your Bullet topology with your plug-in data source (a Spout or a topology), you will naturally build a JAR with all the dependencies or a  fat  JAR. This will include all the DRPC PubSub code and dependencies. You do not need anything else. For configuration, the YAML file that you probably already provide to your topology needs to have the additional settings listed below (the function name is optional but you should change the default since the DRPC function needs to be unique per Storm cluster). Now if you launch your topology, it should be wired up to use Storm DRPC.  bullet.pubsub.context.name:  QUERY_PROCESSING \nbullet.pubsub.class.name:  com.yahoo.bullet.storm.drpc.DRPCPubSub \nbullet.pubsub.storm.drpc.function:  custom-name", 
            "title": "Plug into the Storm Backend"
        }, 
        {
            "location": "/pubsub/storm-drpc/#security", 
            "text": "If your Storm  cluster is secured with  Kerberos  (a standard for Big Data platforms), you will need to periodically refresh your Kerberos TGT and push the credentials to your Storm topology. This is generally done with  kinit  for your topology user, followed by a  storm upload-credentials  TOPOLOGY_NAME . You would probably run this as a  cron  task.", 
            "title": "Security"
        }, 
        {
            "location": "/pubsub/storm-drpc/#plug-into-the-web-service", 
            "text": "When you're plugging in the DRPC PubSub layer into your Web Service, you will need the Bullet Storm JAR with dependencies that you can download from  JCenter . The classifier for this JAR is  fat  if you are depending on it through Maven. You can also download the JAR for the 0.6.2 version directly through  JCenter here .  You should then plug in this JAR to your Web Service following the instructions  here .  For configuration, you should  follow the steps here  and add the context and class name listed above. You will need to point to your DRPC servers and set the function to the same value you chose  above . You can configure this and other settings that are explained further in the  PubSub and PubSub Storm DRPC defaults section  in the Bullet Storm defaults file.  bullet.pubsub.context.name:  QUERY_SUBMISSION \nbullet.pubsub.class.name:  com.yahoo.bullet.storm.drpc.DRPCPubSub \nbullet.pubsub.storm.drpc.servers:\n  - server1\n  - server2\n  - server3\nbullet.pubsub.storm.drpc.function:  custom-name \nbullet.pubsub.storm.drpc.http.protocol:  http \nbullet.pubsub.storm.drpc.http.port:  4080 \nbullet.pubsub.storm.drpc.http.path:  drpc \nbullet.pubsub.storm.drpc.http.connect.retry.limit: 3\nbullet.pubsub.storm.drpc.http.connect.timeout.ms: 1000", 
            "title": "Plug into the Web Service"
        }, 
        {
            "location": "/pubsub/storm-drpc/#caveats-with-storm-drpc", 
            "text": "", 
            "title": "Caveats with Storm DRPC"
        }, 
        {
            "location": "/pubsub/storm-drpc/#scalability", 
            "text": "DRPC servers are a shared resource per Storm cluster and it may be possible that you have to contend with other topologies in your multi-tenant cluster. While it is horizontally scalable, it does tie the scalability of the Bullet backend to it. If you only have a few DRPC servers in your Storm cluster, you may need to add more to support more simultaneous DRPC requests. We have  found that  each server gives us about ~250 simultaneous queries. There is an Async implementation coming in Storm 2.0 that should increase the throughput.", 
            "title": "Scalability"
        }, 
        {
            "location": "/pubsub/storm-drpc/#query-duration", 
            "text": "The maximum time a query can run for depends on the maximum time Storm DRPC request can last in your Storm topology. Generally the default is set to 10 minutes. This means that the  longest query duration possible will be 10 minutes . The value of this is up to your cluster maintainers.", 
            "title": "Query Duration"
        }, 
        {
            "location": "/pubsub/storm-drpc/#request-response", 
            "text": "Our PubSub uses DRPC using HTTP REST in a request-response model. This means that it will not support incremental results as it is! We could switch our usage of DRPC to send signals to the topology to fetch results and start queries. Depending on if there is demand, we may support this in our implementation in the future.", 
            "title": "Request-Response"
        }, 
        {
            "location": "/pubsub/storm-drpc/#reliability", 
            "text": "Storm DRPC follows the principle of leaving retries to the DRPC user (in our case, the Bullet web service). At this moment, we have not chosen to add reliability mechanisms to the query publishing, result publishing or result subscribing sides of our DRPC PubSub implementations but the query subscribers do use the  BufferingSubscriber  mentioned  here .", 
            "title": "Reliability"
        }, 
        {
            "location": "/ws/setup/", 
            "text": "The Web Service\n\n\nThe Web Service is a Java JAR file that you can deploy on a machine to communicate with the Bullet Backend. You then plug in a particular Bullet PubSub implementation such as \nKafka PubSub\n or \nStorm DRPC PubSub\n. For an example on how to set up a Bullet backend, see the \nStorm example setup\n.\n\n\nThere are two main purposes for this layer at this time:\n\n\n1) It provides an endpoint that can serve a \nJSON API schema\n for the Bullet UI. Currently, static schemas from a file are supported.\n\n\n2) It generates unique identifiers and other metadata for a JSON Bullet query before sending the query to the Bullet backend. It wraps errors if the backend is unreachable.\n\n\n\n\nThat's it?\n\n\nThe Web Service essentially just wraps the PubSub layer and provides some helpful endpoints. When incremental updates drop, it will translate a PubSub's streaming responses back into incremental results for the user. It is also there to be a point of abstraction for implementing things like security, monitoring, access-control, rate-limiting, sharding, different query formats (e.g. SQL Bullet queries) etc, which are planned in the near future.\n\n\n\n\nPrerequisites\n\n\nIn order for your Web Service to work with Bullet, you should have an instance of the Backend such as \nStorm\n and a PubSub instance such as \nStorm DRPC\n or \nKafka\n already set up. Alternitively you can run the RESTPubSub as part of the web service. See \nRESTPubSub\n for more info.\n\n\nInstallation\n\n\nYou can download the JAR file directly from \nJCenter\n. The Web Service is a \nSpring Boot\n application. It executes as a standalone application. Note that prior to version 0.1.1, bullet-service was a WAR file that you deployed onto a servlet container like Jetty. It now embeds a \nApache Tomcat\n servlet container.\n\n\nIf you need to depend on the source code directly (to add new endpoints for your own purposes or to build a WAR file out of the JAR), you need to add the JCenter repository and get the artifact through your dependency management system. Maven is shown below.\n\n\nrepositories\n\n    \nrepository\n\n        \nsnapshots\n\n            \nenabled\nfalse\n/enabled\n\n        \n/snapshots\n\n        \nid\ncentral\n/id\n\n        \nname\nbintray\n/name\n\n        \nurl\nhttp://jcenter.bintray.com\n/url\n\n    \n/repository\n\n\n/repositories\n\n\n\n\n\ndependency\n\n  \ngroupId\ncom.yahoo.bullet\n/groupId\n\n  \nartifactId\nbullet-service\n/artifactId\n\n  \nversion\n${bullet.version}\n/version\n\n\n/dependency\n\n\n\n\n\nYou can also add \nclassifier\nsources\n/classifier\n  or \nclassifier\njavadoc\n/classifier\n if you want the source or javadoc or \nclassifier\nembedded\n/classifier\n if you want the full JAR with the embedded web server.\n\n\nConfiguration\n\n\nThere are two levels of configuration:\n\n\n\n\nYou can configure the Web Service, the web server and other Spring Boot settings through a configuration file (or through the command line when launching). By default, this file is called \napplication.yaml\n. This is where settings like where to find your schema file (optional for if you want to power the \nUI\n schema with a static file) or how many publishers and subscribers to use for your PubSub etc.\n\n\nYou can configure what PubSub to use, the various settings for it through another configuration file. The location for this file is provided in the Web Service configuration step above.\n\n\n\n\nWeb Service Configuration\n\n\nTake a look at the \nsettings\n for a list of the settings that are configured. The Web Service settings start with \nbullet.\n.\n\n\nIf you provide a custom settings \napplication.yaml\n, you will \nneed\n to specify the default values in this file since the framework uses your file instead of these defaults.\n\n\nFile based schema\n\n\nThe Web Service can also provide a endpoint that serves your data schema to your UI. You do not necessarily have to use this to serve your schema. The UI can use any JSON API schema specification. But if your schema is fixed or does not change often, it might be simpler for you to use this endpoint to provide the schema for the UI, instead of creating a new one. The Web Service also takes care to provide the right \nCORS\n headers so that your UI can communicate with it.\n\n\nYou can use \nsample_columns.json\n as a guideline for what your actual schema file should look like or if you want to create your own Web Service that dynamically serves your schema to the UI if it changes frequently.\n\n\nOnce you have your schema file, you can provide it to the Web Service by setting the \nbullet.schema.file\n to the path to your file.\n\n\nSpring Boot Configuration\n\n\nYou can also configure various Spring and web server here. Take a look at \nthis page\n page for the various values you can supply.\n\n\nPubSub Configuration\n\n\nYou configure the PubSub by providing a configuration YAML file and setting the \nbullet.pubsub.config\n to its path. In \nthat\n file, you will set these two settings at a minimum:\n\n\n\n\nbullet.pubsub.class.name\n should be set to the fully qualified package to your PubSub implementation. Example: \ncom.yahoo.bullet.kafka.KafkaPubSub\n for the \nKafka PubSub\n.\n\n\nbullet.pubsub.context.name: QUERY_SUBMISSION\n. The Web Service requires the PubSub to be in the \nQUERY_SUBMISSION\n context.\n\n\n\n\nYou will also specify other parameters that your chosen PubSub requires or can use.\n\n\nLaunch\n\n\nTo launch, you will need your PubSub implementation JAR file and launch the application by providing the path to it. For example, if you only wished to provide the PubSub configuration and you had the Web Service jar and your chosen PubSub (say Kafka) in your current directory, you would run:\n\n\njava -Dloader.path=bullet-kafka.jar -jar bullet-service.jar --bullet.pubsub.config=pubsub_settings.yaml  --logging.level.root=INFO\n\n\n\n\nThis launches the Web Service using Kafka as the PubSub, no custom schema (the default sample columns) and the default values in \nsettings\n. It also uses a root logging level of \nINFO\n.\n\n\nYou could also tweak the various Bullet Web Service or Spring Boot settings by passing them in to the command above. For instance, you could also provide a path to your schema file using \n--bullet.schema.file=/path/to/schema.json\n. You could also have a custom \napplication.yaml\n file (you can change the name using \nspring.config.name\n) and pass it to the Web Service instead by running:\n\n\njava -Dloader.path=bullet-kafka.jar -jar bullet-service.jar --spring.config.location=application.yaml\n\n\n\n\nUsage\n\n\nOnce the Web Service is up, you should be able to test to see if it's able to talk to the Bullet Backend:\n\n\nYou can HTTP POST a Bullet query to the API with:\n\n\ncurl -s -H \nContent-Type: text/plain\n -X POST -d '{}' http://localhost:5555/api/bullet/query\n\n\n\n\nYou should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1 when launching the Bullet Backend).\n\n\n\n\nContext Path\n\n\nThe context path, or \"/api/bullet\" in the URL above can be changed using the Spring Boot setting \nserver.context-path\n. You can also change the port (defaults to port 5555) using \nserver.port\n.\n\n\n\n\nIf you provided a path to a schema file in your configuration file when you \nlaunch\n the Web Service, you can also HTTP GET your schema at \nhttp://localhost:5555/api/bullet/columns\n\n\nIf you did not, the schema in \nsample_columns.json\n is the response. The Web Service converts it to a JSON API response and provides the right headers for CORS.", 
            "title": "Setup"
        }, 
        {
            "location": "/ws/setup/#the-web-service", 
            "text": "The Web Service is a Java JAR file that you can deploy on a machine to communicate with the Bullet Backend. You then plug in a particular Bullet PubSub implementation such as  Kafka PubSub  or  Storm DRPC PubSub . For an example on how to set up a Bullet backend, see the  Storm example setup .  There are two main purposes for this layer at this time:  1) It provides an endpoint that can serve a  JSON API schema  for the Bullet UI. Currently, static schemas from a file are supported.  2) It generates unique identifiers and other metadata for a JSON Bullet query before sending the query to the Bullet backend. It wraps errors if the backend is unreachable.   That's it?  The Web Service essentially just wraps the PubSub layer and provides some helpful endpoints. When incremental updates drop, it will translate a PubSub's streaming responses back into incremental results for the user. It is also there to be a point of abstraction for implementing things like security, monitoring, access-control, rate-limiting, sharding, different query formats (e.g. SQL Bullet queries) etc, which are planned in the near future.", 
            "title": "The Web Service"
        }, 
        {
            "location": "/ws/setup/#prerequisites", 
            "text": "In order for your Web Service to work with Bullet, you should have an instance of the Backend such as  Storm  and a PubSub instance such as  Storm DRPC  or  Kafka  already set up. Alternitively you can run the RESTPubSub as part of the web service. See  RESTPubSub  for more info.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/ws/setup/#installation", 
            "text": "You can download the JAR file directly from  JCenter . The Web Service is a  Spring Boot  application. It executes as a standalone application. Note that prior to version 0.1.1, bullet-service was a WAR file that you deployed onto a servlet container like Jetty. It now embeds a  Apache Tomcat  servlet container.  If you need to depend on the source code directly (to add new endpoints for your own purposes or to build a WAR file out of the JAR), you need to add the JCenter repository and get the artifact through your dependency management system. Maven is shown below.  repositories \n     repository \n         snapshots \n             enabled false /enabled \n         /snapshots \n         id central /id \n         name bintray /name \n         url http://jcenter.bintray.com /url \n     /repository  /repositories   dependency \n   groupId com.yahoo.bullet /groupId \n   artifactId bullet-service /artifactId \n   version ${bullet.version} /version  /dependency   You can also add  classifier sources /classifier   or  classifier javadoc /classifier  if you want the source or javadoc or  classifier embedded /classifier  if you want the full JAR with the embedded web server.", 
            "title": "Installation"
        }, 
        {
            "location": "/ws/setup/#configuration", 
            "text": "There are two levels of configuration:   You can configure the Web Service, the web server and other Spring Boot settings through a configuration file (or through the command line when launching). By default, this file is called  application.yaml . This is where settings like where to find your schema file (optional for if you want to power the  UI  schema with a static file) or how many publishers and subscribers to use for your PubSub etc.  You can configure what PubSub to use, the various settings for it through another configuration file. The location for this file is provided in the Web Service configuration step above.", 
            "title": "Configuration"
        }, 
        {
            "location": "/ws/setup/#web-service-configuration", 
            "text": "Take a look at the  settings  for a list of the settings that are configured. The Web Service settings start with  bullet. .  If you provide a custom settings  application.yaml , you will  need  to specify the default values in this file since the framework uses your file instead of these defaults.", 
            "title": "Web Service Configuration"
        }, 
        {
            "location": "/ws/setup/#file-based-schema", 
            "text": "The Web Service can also provide a endpoint that serves your data schema to your UI. You do not necessarily have to use this to serve your schema. The UI can use any JSON API schema specification. But if your schema is fixed or does not change often, it might be simpler for you to use this endpoint to provide the schema for the UI, instead of creating a new one. The Web Service also takes care to provide the right  CORS  headers so that your UI can communicate with it.  You can use  sample_columns.json  as a guideline for what your actual schema file should look like or if you want to create your own Web Service that dynamically serves your schema to the UI if it changes frequently.  Once you have your schema file, you can provide it to the Web Service by setting the  bullet.schema.file  to the path to your file.", 
            "title": "File based schema"
        }, 
        {
            "location": "/ws/setup/#spring-boot-configuration", 
            "text": "You can also configure various Spring and web server here. Take a look at  this page  page for the various values you can supply.", 
            "title": "Spring Boot Configuration"
        }, 
        {
            "location": "/ws/setup/#pubsub-configuration", 
            "text": "You configure the PubSub by providing a configuration YAML file and setting the  bullet.pubsub.config  to its path. In  that  file, you will set these two settings at a minimum:   bullet.pubsub.class.name  should be set to the fully qualified package to your PubSub implementation. Example:  com.yahoo.bullet.kafka.KafkaPubSub  for the  Kafka PubSub .  bullet.pubsub.context.name: QUERY_SUBMISSION . The Web Service requires the PubSub to be in the  QUERY_SUBMISSION  context.   You will also specify other parameters that your chosen PubSub requires or can use.", 
            "title": "PubSub Configuration"
        }, 
        {
            "location": "/ws/setup/#launch", 
            "text": "To launch, you will need your PubSub implementation JAR file and launch the application by providing the path to it. For example, if you only wished to provide the PubSub configuration and you had the Web Service jar and your chosen PubSub (say Kafka) in your current directory, you would run:  java -Dloader.path=bullet-kafka.jar -jar bullet-service.jar --bullet.pubsub.config=pubsub_settings.yaml  --logging.level.root=INFO  This launches the Web Service using Kafka as the PubSub, no custom schema (the default sample columns) and the default values in  settings . It also uses a root logging level of  INFO .  You could also tweak the various Bullet Web Service or Spring Boot settings by passing them in to the command above. For instance, you could also provide a path to your schema file using  --bullet.schema.file=/path/to/schema.json . You could also have a custom  application.yaml  file (you can change the name using  spring.config.name ) and pass it to the Web Service instead by running:  java -Dloader.path=bullet-kafka.jar -jar bullet-service.jar --spring.config.location=application.yaml", 
            "title": "Launch"
        }, 
        {
            "location": "/ws/setup/#usage", 
            "text": "Once the Web Service is up, you should be able to test to see if it's able to talk to the Bullet Backend:  You can HTTP POST a Bullet query to the API with:  curl -s -H  Content-Type: text/plain  -X POST -d '{}' http://localhost:5555/api/bullet/query  You should receive a random record flowing through Bullet instantly (if you left the Raw aggregation micro-batch size at the default of 1 when launching the Bullet Backend).   Context Path  The context path, or \"/api/bullet\" in the URL above can be changed using the Spring Boot setting  server.context-path . You can also change the port (defaults to port 5555) using  server.port .   If you provided a path to a schema file in your configuration file when you  launch  the Web Service, you can also HTTP GET your schema at  http://localhost:5555/api/bullet/columns  If you did not, the schema in  sample_columns.json  is the response. The Web Service converts it to a JSON API response and provides the right headers for CORS.", 
            "title": "Usage"
        }, 
        {
            "location": "/ws/api-json/", 
            "text": "Bullet JSON API\n\n\nThis section gives a comprehensive overview of the Web Service API for launching Bullet JSON queries.\n\n\nThe JSON API is the actual Query format that is expected by the backend. \nThe BQL API\n is a more\nuser-friendly API which can also be used - the Web Service will automatically detect the BQL query and convert the\nquery to this JSON format before submitting it to the backend.\n\n\n\n\nFor info on how to use the UI, see the \nUI Usage section\n\n\nFor examples of specific queries see the \nExamples\n section\n\n\n\n\nThe main constituents of a Bullet JSON query are:\n\n\n\n\nfilters\n, which determine which records will be consumed by your query\n\n\nprojection\n, which determines which fields will be projected in the resulting output from Bullet\n\n\naggregation\n, which allows users to aggregate data and perform aggregation operations\n\n\nwindow\n, which can be used to return incremental results on \"windowed\" data\n\n\nduration\n, which determines the maximum duration of the query in milliseconds\n\n\n\n\nFields inside maps can be accessed using the '.' notation in queries. For example,\n\n\nmyMap.key\n\n\nwill access the \"key\" field inside the \"myMap\" map. There is no support for accessing fields inside Lists or inside nested Maps as of yet. Only the entire object can be operated on for now.\n\n\nThe main constituents of a Bullet query listed above create the top level fields of the Bullet query:\n\n\n{\n    \nfilters\n: [{}, {}, ...],\n    \nprojection\n: {},\n    \naggregation\n: {}.\n    \nwindow\n: {},\n    \nduration\n: 20000\n}\n\n\n\n\nWe will describe how to specify each of these top-level fields below:\n\n\nFilters\n\n\nBullet supports two kinds of filters:\n\n\n\n\nLogical filters\n\n\nRelational filters\n\n\n\n\nLogical Filters\n\n\nLogical filters allow you to combine other filter clauses with logical operations like \nAND\n, \nOR\n and \nNOT\n.\n\n\nThe current logical operators allowed in filters are:\n\n\n\n\n\n\n\n\nLogical Operator\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nAND\n\n\nAll filters must be true. The first false filter evaluated left to right will short-circuit the computation.\n\n\n\n\n\n\nOR\n\n\nAny filter must be true. The first true filter evaluated left to right will short-circuit the computation.\n\n\n\n\n\n\nNOT\n\n\nNegates the value of the first filter clause. The filter is satisfied iff the value is true.\n\n\n\n\n\n\n\n\nThe format for a \nsingle\n Logical filter is:\n\n\n{\n   \noperation\n: \nAND | OR | NOT\n\n   \nclauses\n: [\n      {\noperation\n: \n...\n, clauses: [{}, ...]},\n      {\nfield\n: \n...\n, \noperation\n: \n, values: [\n...\n]},\n      {\noperation\n: \n...\n, clauses: [{}, ...]}\n      ...\n   ]\n}\n\n\n\n\nAny other type of filter may be provided as a clause in clauses.\n\n\nNote that the \"filter\" field in the query is a \nlist\n of as many filters as you'd like.\n\n\nRelational Filters\n\n\nRelational filters allow you to specify conditions on a field, using a comparison operator and a list of values.\n\n\nThe current comparisons allowed in filters are:\n\n\n\n\n\n\n\n\nComparison\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\n==\n\n\nEqual to any value in values\n\n\n\n\n\n\n!=\n\n\nNot equal to any value in values\n\n\n\n\n\n\n=\n\n\nLess than or equal to any value in values\n\n\n\n\n\n\n=\n\n\nGreater than or equal to any value in values\n\n\n\n\n\n\n\n\nLess than any value in values\n\n\n\n\n\n\n\n\nGreater than any value in values\n\n\n\n\n\n\nRLIKE\n\n\nMatches using \nJava Regex notation\n, any Regex value in values\n\n\n\n\n\n\n\n\nNote: These operators are all typed based on the type of the \nleft hand side\n from the Bullet record. If the elements on the right hand side cannot be\ncasted to the types on the LHS, those items will be ignored for the comparison.\n\n\nThe format for a Relational filter is:\n\n\n{\n    \noperation\n: \n== | != | \n= | \n= | \n | \n | RLIKE\n\n    \nfield\n: \nrecord_field_name | map_field.subfield\n,\n    \nvalues\n: [\n        \nstring values\n,\n        \nthat go here\n,\n        \nwill be casted\n,\n        \nto the\n,\n        \ntype of field\n\n    ]\n}\n\n\n\n\nMultiple top level relational filters behave as if they are ANDed together.\n This is supported as a convenience to do a bunch of \nAND\ned relational filters without having to nest them in a logical clause.\n\n\nProjections\n\n\nProjections allow you to pull out only the fields needed and rename them (renaming is being supported in order to give\nbetter names to fields pulled out from maps). If projections are not specified, the entire record is returned. If you are querying\nfor raw records, you can use projections to help reduce the load on the system and network.\n\n\n{\n    \nfields\n: {\n        \nfieldA\n: \nnewNameA\n,\n        \nfieldB\n: \nnewNameB\n\n    }\n}\n\n\n\n\nAggregations\n\n\nAggregations allow you to perform some operation on the collected records. They take an optional size to restrict\nthe size of the aggregation (this applies for aggregations high cardinality aggregations and raw records).\n\n\nThe current aggregation types that are supported are:\n\n\n\n\n\n\n\n\nAggregation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nGROUP\n\n\nThe resulting output would be a record containing the result of an operation for each unique group in the specified fields\n\n\n\n\n\n\nCOUNT DISTINCT\n\n\nComputes the number of distinct elements in the fields. (May be approximate)\n\n\n\n\n\n\nLIMIT\n\n\nThe resulting output would be at most the number specified in size.\n\n\n\n\n\n\nDISTRIBUTION\n\n\nComputes distributions of the elements in the field. E.g. Find the median value or various percentile of a field, or get frequency or cumulative frequency distributions\n\n\n\n\n\n\nTOP K\n\n\nReturns the top K most frequently appearing values in the column\n\n\n\n\n\n\n\n\nThe current format for an aggregation is:\n\n\n{\n    \ntype\n: \nGROUP | COUNT DISTINCT | TOP | PERCENTILE | RAW\n,\n    \nsize\n: \na limit on the number of resulting records\n,\n    \nfields\n: {\n        \nfields\n: \nnewNameA\n,\n        \nthat go here\n: \nnewNameB\n,\n        \nare what the\n: \nnewNameC\n,\n        \naggregation type applies to\n: \nnewNameD\n\n    },\n    \nattributes\n: {\n        \nthese\n: \nchange\n,\n        \nper\n: [\n           \naggregation type\n\n        ]\n    }\n}\n\n\n\n\nYou can also use \nLIMIT\n as an alias for \nRAW\n. \nDISTINCT\n is also an alias for \nGROUP\n. These exist to make some queries read a bit better.\n\n\nCurrently we support GROUP aggregations on the following operations:\n\n\n\n\n\n\n\n\nOperation\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nCOUNT\n\n\nComputes the number of the elements in the group\n\n\n\n\n\n\nSUM\n\n\nComputes the sum of the elements in the group\n\n\n\n\n\n\nMIN\n\n\nReturns the minimum of the elements in the group\n\n\n\n\n\n\nMAX\n\n\nReturns the maximum of the elements in the group\n\n\n\n\n\n\nAVG\n\n\nComputes the average of the elements in the group\n\n\n\n\n\n\n\n\nAttributes\n\n\nThe \nattributes\n section changes per aggregation \ntype\n.\n\n\nGROUP\n\n\nThe following attributes are supported for \nGROUP\n:\n\n\n    \nattributes\n: {\n        \noperations\n: [\n            {\n                \ntype\n: \nCOUNT\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nSUM\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nMIN\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nMAX\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            },\n            {\n                \ntype\n: \nAVG\n,\n                \nfield\n: \nfieldName\n,\n                \nnewName\n: \nresultColumnName\n\n            }\n        ]\n    }\n\n\n\n\nYou can perform \nSUM\n, \nMIN\n, \nMAX\n, \nAVG\n on non-numeric fields. Bullet will attempt to \ncast the field to a number first.\n If it cannot, that record with the field will be ignored for the operation. For the purposes of \nAVG\n, Bullet will\nperform the average across the numeric values for a field only.\n\n\nCOUNT DISTINCT\n\n\nThe following attributes are supported for \nCOUNT DISTINCT\n:\n\n\n    \nattributes\n: {\n        \nnewName\n: \nresultCountColumnName\n\n    }\n\n\n\n\nNote that the new names you specify in the fields map for aggregations do not apply. You must use the attributes here to give your resulting output count column a name.\n\n\nDISTRIBUTION\n\n\nThe following attributes are supported for \nDISTRIBUTION\n:\n\n\n    \nattributes\n: {\n        \ntype\n: \nQUANTILE | PMF | CDF\n,\n        \nnumberOfPoints\n: \na number of evenly generated points to generate\n,\n        \npoints\n: [ a, free, form, list, of, numbers ],\n        \nstart\n: \na start of the range to generate points\n,\n        \nend\n: \nthe end of the range to generate points\n,\n        \nincrement\n: \nthe increment between the generated points\n,\n    }\n\n\n\n\nYou \nmust\n specify one and only one field using the \nfields\n section in \naggregation\n. Any \nnewName\n you provide will be ignored.\n\n\nThe \ntype\n field picks the type of distribution to apply.\n\n\n\n\n\n\n\n\nType\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nQUANTILE\n\n\nLets you pick out percentiles of the numeric field you provide.\n\n\n\n\n\n\nPMF\n\n\nThe Probability Mass Function distribution lets you get frequency counts and probabilities of ranges or intervals of your numeric field\n\n\n\n\n\n\nCDF\n\n\nThe Cumulative Distribution Function distribution lets you get cumulative frequency counts instead and is otherwise similar to PMF\n\n\n\n\n\n\n\n\nDepending on what \ntype\n you have chosen, the rest of the attributes define \npoints in the domain\n of that distribution.\n\n\nFor \nQUANTILE\n, the points you define will be the \nvalues\n to get the percentiles from. These percentiles are represented as numbers between 0 and 1. This means that your points \nmust\n be between 0 and 1.\n\n\nFor \nPMF\n and \nCDF\n, the points you define will \npartition\n the range of your field values into intervals, with the first interval going from -\n to the first point and the last interval from your last point to +\n. This means that if you generate \nN\n points, you will receive \nN+1\n intervals. The points you define should be in the range of the values for your field to get a meaningful distribution. The domain for your points is therefore, all real numbers but you should narrow down to valid values for the field to get meaningful results.\n\n\nYou have three options to generate points.\n\n\n\n\n\n\n\n\nMethod\n\n\nKeys\n\n\n\n\n\n\n\n\n\n\nNumber of Points\n\n\nYou can use the \nnumberOfPoints\n key to provide a number of points to generate evenly distributed in the full range of your domain\n\n\n\n\n\n\nGenerate Points in a range\n\n\nYou can use \nstart\n, \nend\n and \nincrement\n (\nstart\n \n \nend\n, \nincrement\n \n 0) to specify numbers to generate points in a narrower region of your domain\n\n\n\n\n\n\nSpecify free-form points\n\n\nYou can specify a free-form \narray\n of numbers which will be used as the points\n\n\n\n\n\n\n\n\nNote that If you specify more than one way to generate points, the API will use \nnumberOfPoints\n, followed by \npoints\n, followed by \nstart\n, \nend\n and \nincrement\n and whichever creates valid points will be used first.\n\n\nFor \nPMF\n and \nCDF\n, no matter how you specify your points, the first interval will always be \n(-\n, first point)\n and the last interval will be \n[last point, +\n)\n. You will also get a probability of how likely a value will land in the interval per interval in addition to a frequency (or cumulative frequency) count.\n\n\nAs with \nGROUP\n, Bullet will attempt to \ncast\n your field into a numeric type and ignore it if it cannot.\n\n\nTOP K\n\n\nThe following attributes are supported for \nTOP K\n:\n\n\n  \nattributes\n: {\n      \nthreshold\n: \nrestrict results to having at least this count value\n,\n      \nnewName\n: \nresultCountColumnName\n\n  }\n\n\n\n\nNote that the \nK\n in \nTOP K\n is specified using the \nsize\n field in the \naggregation\n object.\n\n\nWindow\n\n\nThe \"window\" field is \noptional\n and allows you to instruct Bullet to return incremental results. For example you might want to return the COUNT of a field and return that count every 2 seconds. \n\n\nIf \"window\" is omitted Bullet will emit only a single result at the very end of the query.\n\n\nAn example window might look like this:\n\n\nwindow\n: { \nemit\n: { \ntype\n: \nTIME/RECORD\n, \nevery\n: 5000 },\n            \ninclude\n: { \ntype\n: \nTIME/RECORD/ALL\n, \nfirst\n: 5000 } },\n\n\n\n\n\n\n\n\n\n\nField\n\n\nSubField\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nemit\n\n\n\n\nThis object specifies when a window should be emitted and the current results sent back to the user\n\n\n\n\n\n\n\n\ntype\n\n\nMust be \"TIME\" or \"RECORD\" - specifying if the window should be emitted after X number of milliseconds, or X number of records\n\n\n\n\n\n\n\n\nevery\n\n\nThe number of milliseconds or records (determined by \"type\" above) that will be contained in the emitted window\n\n\n\n\n\n\ninclude\n\n\n\n\nThis object specifies what will be included in the emitted window\n\n\n\n\n\n\n\n\ntype\n\n\nMust be \"TIME\", \"RECORD\" or \"ALL\" - specifying if the window should include X number of milliseconds, X number of records, or all results since the beginning of the whole query\n\n\n\n\n\n\n\n\nfirst\n\n\nSpecifies the number of records/milliseconds at the beginning of this window to include in the emitted result - it should be omitted if \"type\" is \"ALL\"\n\n\n\n\n\n\n\n\nNOTE: Not all windowing types are supported at this time.\n\n\nCurrently Bullet supports the following window types\n:\n\n\n\n\nTime-Based Tumbling Windows\n\n\nAdditive Tumbling Windows\n\n\nReactive Record-Based Windows\n\n\nNo Window\n\n\n\n\nSupport for more windows will be added in the future.\n\n\nEach currently supported window type will be described below:\n\n\nTime-Based Tumbling Windows\n\n\nCurrently time-based tumbling windows \nmust\n have emit == include. In other words, only the entire window can be emitted, and windows must be adjacent.\n\n\n\n\nThe above example windowing would be specified with the window:\n\n\nwindow\n: { \nemit\n: { \ntype\n: \nTIME\n, \nevery\n: 3000 },\n            \ninclude\n: { \ntype\n: \nTIME\n, \nfirst\n: 3000 } },\n\n\n\n\nAny aggregation can be done in each window, or the raw records themselves can be returned as specified in the \"aggregation\" object.\n\n\nIn this example the first window would include 3 records, the second would include 4 records, the third would include 3 records and the fourth would include 2 records.\n\n\nAdditive Tumbling Windows\n\n\nAdditive tumbling windows emit with the same logic as time-based tumbling windows, but include ALL results from the beginning of the query:\n\n\n\n\nThe above example would be specified with the window:\n\n\nwindow\n: { \nemit\n: { \ntype\n: \nTIME\n, \nevery\n: 3000 },\n            \ninclude\n: { \ntype\n: \nALL\n } },\n\n\n\n\nIn this example the first window would include 3 records, the second would include 7 records, the third would include 10 records and the fourth would include 12 records.\n\n\nSliding \"Reactive\" Windows\n\n\nSliding windows emit based on the arrival of an event, rather than after a certain period of time. In general sliding windows often do some aggregation on the previous X records, or on all records that arrived in the last X seconds.\nBullet will support this functionality in the future, at this time Bullet only supports \nSliding Windows of size 1\n, often referred to as \"reactive\" windows. It does not support sliding windows with an aggregation at this time.\nEffectively this query will simply return every event that matches the filters instantly to the user.\n\n\n\n\nThe above example would be specified with the window:\n\n\nwindow\n: { \nemit\n: { \ntype\n: \nRECORD\n, \nevery\n: 1 },\n            \ninclude\n: { \ntype\n: \nRECORD\n, \nlast\n: 1 } },\n\n\n\n\nNo Window\n\n\nIf the \"window\" field is optional. If it is  omitted, the query will only emit when the entire query is finished.\n\n\nResults\n\n\nBullet results are JSON objects with two fields:\n\n\n\n\n\n\n\n\nField\n\n\nContents\n\n\n\n\n\n\n\n\n\n\nrecords\n\n\nThis field contains the list of matching records\n\n\n\n\n\n\nmeta\n\n\nThis field is a map that contains meta information about the query, such as the time the query was received, error data, etc. These are configurable at launch time.\n\n\n\n\n\n\n\n\nFor a detailed description of how to perform these queries and see example results, see \nExamples\n.", 
            "title": "JSON"
        }, 
        {
            "location": "/ws/api-json/#bullet-json-api", 
            "text": "This section gives a comprehensive overview of the Web Service API for launching Bullet JSON queries.  The JSON API is the actual Query format that is expected by the backend.  The BQL API  is a more\nuser-friendly API which can also be used - the Web Service will automatically detect the BQL query and convert the\nquery to this JSON format before submitting it to the backend.   For info on how to use the UI, see the  UI Usage section  For examples of specific queries see the  Examples  section   The main constituents of a Bullet JSON query are:   filters , which determine which records will be consumed by your query  projection , which determines which fields will be projected in the resulting output from Bullet  aggregation , which allows users to aggregate data and perform aggregation operations  window , which can be used to return incremental results on \"windowed\" data  duration , which determines the maximum duration of the query in milliseconds   Fields inside maps can be accessed using the '.' notation in queries. For example,  myMap.key  will access the \"key\" field inside the \"myMap\" map. There is no support for accessing fields inside Lists or inside nested Maps as of yet. Only the entire object can be operated on for now.  The main constituents of a Bullet query listed above create the top level fields of the Bullet query:  {\n     filters : [{}, {}, ...],\n     projection : {},\n     aggregation : {}.\n     window : {},\n     duration : 20000\n}  We will describe how to specify each of these top-level fields below:", 
            "title": "Bullet JSON API"
        }, 
        {
            "location": "/ws/api-json/#filters", 
            "text": "Bullet supports two kinds of filters:   Logical filters  Relational filters", 
            "title": "Filters"
        }, 
        {
            "location": "/ws/api-json/#logical-filters", 
            "text": "Logical filters allow you to combine other filter clauses with logical operations like  AND ,  OR  and  NOT .  The current logical operators allowed in filters are:     Logical Operator  Meaning      AND  All filters must be true. The first false filter evaluated left to right will short-circuit the computation.    OR  Any filter must be true. The first true filter evaluated left to right will short-circuit the computation.    NOT  Negates the value of the first filter clause. The filter is satisfied iff the value is true.     The format for a  single  Logical filter is:  {\n    operation :  AND | OR | NOT \n    clauses : [\n      { operation :  ... , clauses: [{}, ...]},\n      { field :  ... ,  operation :  , values: [ ... ]},\n      { operation :  ... , clauses: [{}, ...]}\n      ...\n   ]\n}  Any other type of filter may be provided as a clause in clauses.  Note that the \"filter\" field in the query is a  list  of as many filters as you'd like.", 
            "title": "Logical Filters"
        }, 
        {
            "location": "/ws/api-json/#relational-filters", 
            "text": "Relational filters allow you to specify conditions on a field, using a comparison operator and a list of values.  The current comparisons allowed in filters are:     Comparison  Meaning      ==  Equal to any value in values    !=  Not equal to any value in values    =  Less than or equal to any value in values    =  Greater than or equal to any value in values     Less than any value in values     Greater than any value in values    RLIKE  Matches using  Java Regex notation , any Regex value in values     Note: These operators are all typed based on the type of the  left hand side  from the Bullet record. If the elements on the right hand side cannot be\ncasted to the types on the LHS, those items will be ignored for the comparison.  The format for a Relational filter is:  {\n     operation :  == | != |  = |  = |   |   | RLIKE \n     field :  record_field_name | map_field.subfield ,\n     values : [\n         string values ,\n         that go here ,\n         will be casted ,\n         to the ,\n         type of field \n    ]\n}  Multiple top level relational filters behave as if they are ANDed together.  This is supported as a convenience to do a bunch of  AND ed relational filters without having to nest them in a logical clause.", 
            "title": "Relational Filters"
        }, 
        {
            "location": "/ws/api-json/#projections", 
            "text": "Projections allow you to pull out only the fields needed and rename them (renaming is being supported in order to give\nbetter names to fields pulled out from maps). If projections are not specified, the entire record is returned. If you are querying\nfor raw records, you can use projections to help reduce the load on the system and network.  {\n     fields : {\n         fieldA :  newNameA ,\n         fieldB :  newNameB \n    }\n}", 
            "title": "Projections"
        }, 
        {
            "location": "/ws/api-json/#aggregations", 
            "text": "Aggregations allow you to perform some operation on the collected records. They take an optional size to restrict\nthe size of the aggregation (this applies for aggregations high cardinality aggregations and raw records).  The current aggregation types that are supported are:     Aggregation  Meaning      GROUP  The resulting output would be a record containing the result of an operation for each unique group in the specified fields    COUNT DISTINCT  Computes the number of distinct elements in the fields. (May be approximate)    LIMIT  The resulting output would be at most the number specified in size.    DISTRIBUTION  Computes distributions of the elements in the field. E.g. Find the median value or various percentile of a field, or get frequency or cumulative frequency distributions    TOP K  Returns the top K most frequently appearing values in the column     The current format for an aggregation is:  {\n     type :  GROUP | COUNT DISTINCT | TOP | PERCENTILE | RAW ,\n     size :  a limit on the number of resulting records ,\n     fields : {\n         fields :  newNameA ,\n         that go here :  newNameB ,\n         are what the :  newNameC ,\n         aggregation type applies to :  newNameD \n    },\n     attributes : {\n         these :  change ,\n         per : [\n            aggregation type \n        ]\n    }\n}  You can also use  LIMIT  as an alias for  RAW .  DISTINCT  is also an alias for  GROUP . These exist to make some queries read a bit better.  Currently we support GROUP aggregations on the following operations:     Operation  Meaning      COUNT  Computes the number of the elements in the group    SUM  Computes the sum of the elements in the group    MIN  Returns the minimum of the elements in the group    MAX  Returns the maximum of the elements in the group    AVG  Computes the average of the elements in the group", 
            "title": "Aggregations"
        }, 
        {
            "location": "/ws/api-json/#attributes", 
            "text": "The  attributes  section changes per aggregation  type .", 
            "title": "Attributes"
        }, 
        {
            "location": "/ws/api-json/#group", 
            "text": "The following attributes are supported for  GROUP :       attributes : {\n         operations : [\n            {\n                 type :  COUNT ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  SUM ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  MIN ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  MAX ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            },\n            {\n                 type :  AVG ,\n                 field :  fieldName ,\n                 newName :  resultColumnName \n            }\n        ]\n    }  You can perform  SUM ,  MIN ,  MAX ,  AVG  on non-numeric fields. Bullet will attempt to  cast the field to a number first.  If it cannot, that record with the field will be ignored for the operation. For the purposes of  AVG , Bullet will\nperform the average across the numeric values for a field only.", 
            "title": "GROUP"
        }, 
        {
            "location": "/ws/api-json/#count-distinct", 
            "text": "The following attributes are supported for  COUNT DISTINCT :       attributes : {\n         newName :  resultCountColumnName \n    }  Note that the new names you specify in the fields map for aggregations do not apply. You must use the attributes here to give your resulting output count column a name.", 
            "title": "COUNT DISTINCT"
        }, 
        {
            "location": "/ws/api-json/#distribution", 
            "text": "The following attributes are supported for  DISTRIBUTION :       attributes : {\n         type :  QUANTILE | PMF | CDF ,\n         numberOfPoints :  a number of evenly generated points to generate ,\n         points : [ a, free, form, list, of, numbers ],\n         start :  a start of the range to generate points ,\n         end :  the end of the range to generate points ,\n         increment :  the increment between the generated points ,\n    }  You  must  specify one and only one field using the  fields  section in  aggregation . Any  newName  you provide will be ignored.  The  type  field picks the type of distribution to apply.     Type  Meaning      QUANTILE  Lets you pick out percentiles of the numeric field you provide.    PMF  The Probability Mass Function distribution lets you get frequency counts and probabilities of ranges or intervals of your numeric field    CDF  The Cumulative Distribution Function distribution lets you get cumulative frequency counts instead and is otherwise similar to PMF     Depending on what  type  you have chosen, the rest of the attributes define  points in the domain  of that distribution.  For  QUANTILE , the points you define will be the  values  to get the percentiles from. These percentiles are represented as numbers between 0 and 1. This means that your points  must  be between 0 and 1.  For  PMF  and  CDF , the points you define will  partition  the range of your field values into intervals, with the first interval going from -  to the first point and the last interval from your last point to + . This means that if you generate  N  points, you will receive  N+1  intervals. The points you define should be in the range of the values for your field to get a meaningful distribution. The domain for your points is therefore, all real numbers but you should narrow down to valid values for the field to get meaningful results.  You have three options to generate points.     Method  Keys      Number of Points  You can use the  numberOfPoints  key to provide a number of points to generate evenly distributed in the full range of your domain    Generate Points in a range  You can use  start ,  end  and  increment  ( start     end ,  increment    0) to specify numbers to generate points in a narrower region of your domain    Specify free-form points  You can specify a free-form  array  of numbers which will be used as the points     Note that If you specify more than one way to generate points, the API will use  numberOfPoints , followed by  points , followed by  start ,  end  and  increment  and whichever creates valid points will be used first.  For  PMF  and  CDF , no matter how you specify your points, the first interval will always be  (- , first point)  and the last interval will be  [last point, + ) . You will also get a probability of how likely a value will land in the interval per interval in addition to a frequency (or cumulative frequency) count.  As with  GROUP , Bullet will attempt to  cast  your field into a numeric type and ignore it if it cannot.", 
            "title": "DISTRIBUTION"
        }, 
        {
            "location": "/ws/api-json/#top-k", 
            "text": "The following attributes are supported for  TOP K :     attributes : {\n       threshold :  restrict results to having at least this count value ,\n       newName :  resultCountColumnName \n  }  Note that the  K  in  TOP K  is specified using the  size  field in the  aggregation  object.", 
            "title": "TOP K"
        }, 
        {
            "location": "/ws/api-json/#window", 
            "text": "The \"window\" field is  optional  and allows you to instruct Bullet to return incremental results. For example you might want to return the COUNT of a field and return that count every 2 seconds.   If \"window\" is omitted Bullet will emit only a single result at the very end of the query.  An example window might look like this:  window : {  emit : {  type :  TIME/RECORD ,  every : 5000 },\n             include : {  type :  TIME/RECORD/ALL ,  first : 5000 } },     Field  SubField  Meaning      emit   This object specifies when a window should be emitted and the current results sent back to the user     type  Must be \"TIME\" or \"RECORD\" - specifying if the window should be emitted after X number of milliseconds, or X number of records     every  The number of milliseconds or records (determined by \"type\" above) that will be contained in the emitted window    include   This object specifies what will be included in the emitted window     type  Must be \"TIME\", \"RECORD\" or \"ALL\" - specifying if the window should include X number of milliseconds, X number of records, or all results since the beginning of the whole query     first  Specifies the number of records/milliseconds at the beginning of this window to include in the emitted result - it should be omitted if \"type\" is \"ALL\"     NOTE: Not all windowing types are supported at this time.  Currently Bullet supports the following window types :   Time-Based Tumbling Windows  Additive Tumbling Windows  Reactive Record-Based Windows  No Window   Support for more windows will be added in the future.  Each currently supported window type will be described below:", 
            "title": "Window"
        }, 
        {
            "location": "/ws/api-json/#time-based-tumbling-windows", 
            "text": "Currently time-based tumbling windows  must  have emit == include. In other words, only the entire window can be emitted, and windows must be adjacent.   The above example windowing would be specified with the window:  window : {  emit : {  type :  TIME ,  every : 3000 },\n             include : {  type :  TIME ,  first : 3000 } },  Any aggregation can be done in each window, or the raw records themselves can be returned as specified in the \"aggregation\" object.  In this example the first window would include 3 records, the second would include 4 records, the third would include 3 records and the fourth would include 2 records.", 
            "title": "Time-Based Tumbling Windows"
        }, 
        {
            "location": "/ws/api-json/#additive-tumbling-windows", 
            "text": "Additive tumbling windows emit with the same logic as time-based tumbling windows, but include ALL results from the beginning of the query:   The above example would be specified with the window:  window : {  emit : {  type :  TIME ,  every : 3000 },\n             include : {  type :  ALL  } },  In this example the first window would include 3 records, the second would include 7 records, the third would include 10 records and the fourth would include 12 records.", 
            "title": "Additive Tumbling Windows"
        }, 
        {
            "location": "/ws/api-json/#sliding-reactive-windows", 
            "text": "Sliding windows emit based on the arrival of an event, rather than after a certain period of time. In general sliding windows often do some aggregation on the previous X records, or on all records that arrived in the last X seconds.\nBullet will support this functionality in the future, at this time Bullet only supports  Sliding Windows of size 1 , often referred to as \"reactive\" windows. It does not support sliding windows with an aggregation at this time.\nEffectively this query will simply return every event that matches the filters instantly to the user.   The above example would be specified with the window:  window : {  emit : {  type :  RECORD ,  every : 1 },\n             include : {  type :  RECORD ,  last : 1 } },", 
            "title": "Sliding \"Reactive\" Windows"
        }, 
        {
            "location": "/ws/api-json/#no-window", 
            "text": "If the \"window\" field is optional. If it is  omitted, the query will only emit when the entire query is finished.", 
            "title": "No Window"
        }, 
        {
            "location": "/ws/api-json/#results", 
            "text": "Bullet results are JSON objects with two fields:     Field  Contents      records  This field contains the list of matching records    meta  This field is a map that contains meta information about the query, such as the time the query was received, error data, etc. These are configurable at launch time.     For a detailed description of how to perform these queries and see example results, see  Examples .", 
            "title": "Results"
        }, 
        {
            "location": "/ws/api-bql/", 
            "text": "Bullet BQL API\n\n\nThis section gives a comprehensive overview of the Web Service API for launching Bullet BQL queries.\n\n\nFor examples of BQL queries, see the \nexamples page\n.\n\n\nBQL queries that are received by the Web Service will be detenced and automatically converted to\n\nthe JSON format\n before being sent to the backend (which requires the basic JSON format). This converstion\nis done in the web service using \nthe bullet-bql library\n.\n\n\nOverview\n\n\nBullet-BQL provides users with a friendly SQL-like API to submit queries to the Web Service instead of using the more\ncumbersome \nJSON API\n.\n\n\nStatement Syntax\n\n\nSELECT DISTINCT? select_clause\nFROM from_clause\n( WHERE where_clause )?\n( GROUP BY groupBy_clause )?\n( HAVING having_clause )?\n( ORDER BY orderBy_clause )?\n( WINDOWING windowing_clause )?\n( LIMIT limit_clause )?;\n\n\n\nwhere \nselect_clause\n is one of\n\n\n*\nCOUNT( DISTINCT reference_expr ( , reference_expr )? )\ngroup_function ( AS? ColumnReference )? ( , group_function ( AS? ColumnReference )? )? ( , reference_expr ( AS? ColumnReference )? )?\nreference_expr ( AS? ColumnReference )? ( , reference_expr ( AS? ColumnReference )? )?\ndistribution_type( reference_expr, input_mode ) ( AS? ColumnReference )?\nTOP ( ( Integer | Long ) ( , Integer | Long ) )? , reference_expr ( , reference_expr )? ) ( AS? ColumnReference )?\n\n\n\ngroup_function\n is one of \nSUM(reference_expr)\n, \nMIN(reference_expr)\n, \nMAX(reference_expr)\n, \nAVG(reference_expr)\n and \nCOUNT(*)\n. \nreference_expr\n is one of ColumnReference and Dereference. \ndistribution_type\n is one of \nQUANTILE\n, \nFREQ\n and \nCUMFREQ\n. The 1st number in \nTOP\n is K, and the 2nd number is an optional threshold.  The \ninput_mode\n is one of\n\n\nLINEAR, ( Integer | Long )                                              evenly spaced\nREGION, ( Integer | Long ), ( Integer | Long ), ( Integer | Long )      evenly spaced in a region\nMANUAL, ( Integer | Long ) (, ( Integer | Long ) )*                     defined points\n\n\n\nand \nfrom_clause\n is one of\n\n\nSTREAM()                                                          default time duration will be set from BQLConfig\nSTREAM( ( Long | MAX ), TIME )                                    time based duration control.\nSTREAM( ( Long | MAX ), TIME, ( Long | MAX ), RECORD )            time and record based duration control.\n\n\n\nRECORD\n will be supported in the future.\n\n\nand \nwhere_clause\n is one of\n\n\nNOT where_clause\nwhere_clause AND where_clause\nwhere_clause OR where_clause\nreference_expr IS NOT? NULL\nreference_expr IS NOT? EMPTY\nreference_expr IS NOT? DISTINCT FROM value_expr\nreference_expr NOT? BETWEEN value_expr AND value_expr\nreference_expr NOT? IN ( value_expr ( , value_expr )* )\nreference_expr NOT? LIKE ( value_expr ( , value_expr )* )\nreference_expr ( = | \n | != | \n | \n | \n= | \n= ) value_expr\n\n\n\nvalue_expr\n is one of Null, Boolean, Integer, Long, Double, Decimal and String.\n\n\nand \ngroupBy_clause\n is one of\n\n\n()                                                                group all\nreference_expr ( , reference_expr )*                              group by\n( reference_expr ( , reference_expr )* )                          group by\n\n\n\nand \nHAVING\n and \nORDER BY\n are only supported for TopK. In which case, \nhaving_clause\n is\n\n\nCOUNT(*) \n= Integer\n\n\n\nand \norderBy_clause\n is\n\n\nCOUNT(*)\n\n\n\nand \nwindowing_clause\n is one of\n\n\n( EVERY, ( Integer | Long ), ( TIME | RECORD ), include )\n( TUMBLING, ( Integer | Long ), ( TIME | RECORD ) )\n\n\n\ninclude\n is one of\n\n\nALL\nFIRST, ( Integer | Long ), ( TIME | RECORD )\nLAST, ( Integer | Long ), ( TIME | RECORD )                       will be supported\n\n\n\nand \nlimit_clause\n is one of\n\n\nInteger | Long\nALL                                                               will be supported\n\n\n\nData Types\n\n\n\n\n\n\nNull\n: \nNULL\n.\n\n\n\n\n\n\nBoolean\n: \nTRUE\n, \nFALSE\n.\n\n\n\n\n\n\nInteger\n: 32-bit signed two\u2019s complement integer with a minimum value of \n-2^31\n and a maximum value of \n2^31 - 1\n. Example: \n65\n.\n\n\n\n\n\n\nLong\n: 64-bit signed two\u2019s complement integer with a minimum value of \n-2^63 + 1\n and a maximum value of \n2^63 - 1\n. Example: \n9223372036854775807\n, \n-9223372036854775807\n.\n\n\n\n\n\n\nDouble\n: 64-bit inexact, variable-precision with a minimum value of \n2^-1074\n and a maximum value of \n(2-2^-52)\u00b72^1023\n. Example: \n1.7976931348623157E+308\n, \n.17976931348623157E+309\n, \n4.9E-324\n.\n\n\n\n\n\n\nDecimal\n: decimal number can be treated as Double, String or ParsingException. This is controlled by \nParsingOptions\n. \n1.7976931348623157\n, \n.17976931348623157\n.\n\n\n\n\n\n\nString\n: character string which can have escapes. Example: \n'this is a string'\n, \n'this is ''another'' string'\n.\n\n\n\n\n\n\nColumnReference\n: representation of a single column. Unquoted ColumnReference must start with a letter or \n_\n. Quoted ColumnReference can have escape. Example: \ncolumn_name\n, \n\"#column\"\"with\"\"escape\"\n.\n\n\n\n\n\n\nDereference\n: representation of a column field. Example: \ncolumn_name.field_name\n.\n\n\n\n\n\n\nAll\n: representation of all columns. Example: \n*\n. \ncolumn_name.*\n is interpreted as \ncolumn_name\n.\n\n\n\n\n\n\nReserved Keywords\n\n\nReserved keywords must be double quoted in order to be used as ColumnReference or Dereference.\n\n\n\n\n\n\n\n\nKeyword\n\n\nSQL:2016\n\n\nSQL-92\n\n\n\n\n\n\n\n\n\n\nALTER\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nAND\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nAS\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nBETWEEN\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nBY\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nCASE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nCAST\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nCONSTRAINT\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nCREATE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nCROSS\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nCUBE\n\n\nreserved\n\n\n\n\n\n\n\n\nCURRENT_DATE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nCURRENT_TIME\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nCURRENT_TIMESTAMP\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nCURRENT_USER\n\n\nreserved\n\n\n\n\n\n\n\n\nDEALLOCATE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nDELETE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nDESCRIBE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nDISTINCT\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nDROP\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nELSE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nEND\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nESCAPE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nEXCEPT\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nEXECUTE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nEXISTS\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nEXTRACT\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nFALSE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nFOR\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nFROM\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nFULL\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nGROUP\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nGROUPING\n\n\nreserved\n\n\n\n\n\n\n\n\nHAVING\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nIN\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nINNER\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nINSERT\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nINTERSECT\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nINTO\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nIS\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nJOIN\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nLEFT\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nLIKE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nLOCALTIME\n\n\nreserved\n\n\n\n\n\n\n\n\nLOCALTIMESTAMP\n\n\nreserved\n\n\n\n\n\n\n\n\nNATURAL\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nNORMALIZE\n\n\nreserved\n\n\n\n\n\n\n\n\nNOT\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nNULL\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nON\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nOR\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nORDER\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nOUTER\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nPREPARE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nRECURSIVE\n\n\nreserved\n\n\n\n\n\n\n\n\nRIGHT\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nROLLUP\n\n\nreserved\n\n\n\n\n\n\n\n\nSELECT\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nTABLE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nTHEN\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nTRUE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nUESCAPE\n\n\nreserved\n\n\n\n\n\n\n\n\nUNION\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nUNNEST\n\n\nreserved\n\n\n\n\n\n\n\n\nUSING\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nVALUES\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nWHEN\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nWHERE\n\n\nreserved\n\n\nreserved\n\n\n\n\n\n\nWITH\n\n\nreserved\n\n\nreserved", 
            "title": "BQL"
        }, 
        {
            "location": "/ws/api-bql/#bullet-bql-api", 
            "text": "This section gives a comprehensive overview of the Web Service API for launching Bullet BQL queries.  For examples of BQL queries, see the  examples page .  BQL queries that are received by the Web Service will be detenced and automatically converted to the JSON format  before being sent to the backend (which requires the basic JSON format). This converstion\nis done in the web service using  the bullet-bql library .", 
            "title": "Bullet BQL API"
        }, 
        {
            "location": "/ws/api-bql/#overview", 
            "text": "Bullet-BQL provides users with a friendly SQL-like API to submit queries to the Web Service instead of using the more\ncumbersome  JSON API .", 
            "title": "Overview"
        }, 
        {
            "location": "/ws/api-bql/#statement-syntax", 
            "text": "SELECT DISTINCT? select_clause\nFROM from_clause\n( WHERE where_clause )?\n( GROUP BY groupBy_clause )?\n( HAVING having_clause )?\n( ORDER BY orderBy_clause )?\n( WINDOWING windowing_clause )?\n( LIMIT limit_clause )?;  where  select_clause  is one of  *\nCOUNT( DISTINCT reference_expr ( , reference_expr )? )\ngroup_function ( AS? ColumnReference )? ( , group_function ( AS? ColumnReference )? )? ( , reference_expr ( AS? ColumnReference )? )?\nreference_expr ( AS? ColumnReference )? ( , reference_expr ( AS? ColumnReference )? )?\ndistribution_type( reference_expr, input_mode ) ( AS? ColumnReference )?\nTOP ( ( Integer | Long ) ( , Integer | Long ) )? , reference_expr ( , reference_expr )? ) ( AS? ColumnReference )?  group_function  is one of  SUM(reference_expr) ,  MIN(reference_expr) ,  MAX(reference_expr) ,  AVG(reference_expr)  and  COUNT(*) .  reference_expr  is one of ColumnReference and Dereference.  distribution_type  is one of  QUANTILE ,  FREQ  and  CUMFREQ . The 1st number in  TOP  is K, and the 2nd number is an optional threshold.  The  input_mode  is one of  LINEAR, ( Integer | Long )                                              evenly spaced\nREGION, ( Integer | Long ), ( Integer | Long ), ( Integer | Long )      evenly spaced in a region\nMANUAL, ( Integer | Long ) (, ( Integer | Long ) )*                     defined points  and  from_clause  is one of  STREAM()                                                          default time duration will be set from BQLConfig\nSTREAM( ( Long | MAX ), TIME )                                    time based duration control.\nSTREAM( ( Long | MAX ), TIME, ( Long | MAX ), RECORD )            time and record based duration control.  RECORD  will be supported in the future.  and  where_clause  is one of  NOT where_clause\nwhere_clause AND where_clause\nwhere_clause OR where_clause\nreference_expr IS NOT? NULL\nreference_expr IS NOT? EMPTY\nreference_expr IS NOT? DISTINCT FROM value_expr\nreference_expr NOT? BETWEEN value_expr AND value_expr\nreference_expr NOT? IN ( value_expr ( , value_expr )* )\nreference_expr NOT? LIKE ( value_expr ( , value_expr )* )\nreference_expr ( = |   | != |   |   |  = |  = ) value_expr  value_expr  is one of Null, Boolean, Integer, Long, Double, Decimal and String.  and  groupBy_clause  is one of  ()                                                                group all\nreference_expr ( , reference_expr )*                              group by\n( reference_expr ( , reference_expr )* )                          group by  and  HAVING  and  ORDER BY  are only supported for TopK. In which case,  having_clause  is  COUNT(*)  = Integer  and  orderBy_clause  is  COUNT(*)  and  windowing_clause  is one of  ( EVERY, ( Integer | Long ), ( TIME | RECORD ), include )\n( TUMBLING, ( Integer | Long ), ( TIME | RECORD ) )  include  is one of  ALL\nFIRST, ( Integer | Long ), ( TIME | RECORD )\nLAST, ( Integer | Long ), ( TIME | RECORD )                       will be supported  and  limit_clause  is one of  Integer | Long\nALL                                                               will be supported", 
            "title": "Statement Syntax"
        }, 
        {
            "location": "/ws/api-bql/#data-types", 
            "text": "Null :  NULL .    Boolean :  TRUE ,  FALSE .    Integer : 32-bit signed two\u2019s complement integer with a minimum value of  -2^31  and a maximum value of  2^31 - 1 . Example:  65 .    Long : 64-bit signed two\u2019s complement integer with a minimum value of  -2^63 + 1  and a maximum value of  2^63 - 1 . Example:  9223372036854775807 ,  -9223372036854775807 .    Double : 64-bit inexact, variable-precision with a minimum value of  2^-1074  and a maximum value of  (2-2^-52)\u00b72^1023 . Example:  1.7976931348623157E+308 ,  .17976931348623157E+309 ,  4.9E-324 .    Decimal : decimal number can be treated as Double, String or ParsingException. This is controlled by  ParsingOptions .  1.7976931348623157 ,  .17976931348623157 .    String : character string which can have escapes. Example:  'this is a string' ,  'this is ''another'' string' .    ColumnReference : representation of a single column. Unquoted ColumnReference must start with a letter or  _ . Quoted ColumnReference can have escape. Example:  column_name ,  \"#column\"\"with\"\"escape\" .    Dereference : representation of a column field. Example:  column_name.field_name .    All : representation of all columns. Example:  * .  column_name.*  is interpreted as  column_name .", 
            "title": "Data Types"
        }, 
        {
            "location": "/ws/api-bql/#reserved-keywords", 
            "text": "Reserved keywords must be double quoted in order to be used as ColumnReference or Dereference.     Keyword  SQL:2016  SQL-92      ALTER  reserved  reserved    AND  reserved  reserved    AS  reserved  reserved    BETWEEN  reserved  reserved    BY  reserved  reserved    CASE  reserved  reserved    CAST  reserved  reserved    CONSTRAINT  reserved  reserved    CREATE  reserved  reserved    CROSS  reserved  reserved    CUBE  reserved     CURRENT_DATE  reserved  reserved    CURRENT_TIME  reserved  reserved    CURRENT_TIMESTAMP  reserved  reserved    CURRENT_USER  reserved     DEALLOCATE  reserved  reserved    DELETE  reserved  reserved    DESCRIBE  reserved  reserved    DISTINCT  reserved  reserved    DROP  reserved  reserved    ELSE  reserved  reserved    END  reserved  reserved    ESCAPE  reserved  reserved    EXCEPT  reserved  reserved    EXECUTE  reserved  reserved    EXISTS  reserved  reserved    EXTRACT  reserved  reserved    FALSE  reserved  reserved    FOR  reserved  reserved    FROM  reserved  reserved    FULL  reserved  reserved    GROUP  reserved  reserved    GROUPING  reserved     HAVING  reserved  reserved    IN  reserved  reserved    INNER  reserved  reserved    INSERT  reserved  reserved    INTERSECT  reserved  reserved    INTO  reserved  reserved    IS  reserved  reserved    JOIN  reserved  reserved    LEFT  reserved  reserved    LIKE  reserved  reserved    LOCALTIME  reserved     LOCALTIMESTAMP  reserved     NATURAL  reserved  reserved    NORMALIZE  reserved     NOT  reserved  reserved    NULL  reserved  reserved    ON  reserved  reserved    OR  reserved  reserved    ORDER  reserved  reserved    OUTER  reserved  reserved    PREPARE  reserved  reserved    RECURSIVE  reserved     RIGHT  reserved  reserved    ROLLUP  reserved     SELECT  reserved  reserved    TABLE  reserved  reserved    THEN  reserved  reserved    TRUE  reserved  reserved    UESCAPE  reserved     UNION  reserved  reserved    UNNEST  reserved     USING  reserved  reserved    VALUES  reserved  reserved    WHEN  reserved  reserved    WHERE  reserved  reserved    WITH  reserved  reserved", 
            "title": "Reserved Keywords"
        }, 
        {
            "location": "/ws/examples/", 
            "text": "Query Examples\n\n\nRather than sourcing the examples from the Quick Start, these examples are real-world ones sourced from Bullet running on raw, user events generated by instrumentation on Yahoo sites by Yahoo employees (not all Yahoo users).\n\n\n\n\nDisclaimer\n\n\nThe actual data shown here has been edited and is not how actual Yahoo user events look.\n\n\n\n\n\n\nSQL translation\n\n\nFor each query, we will also rewrite it to a SQL like syntax for readability. Eventually, we plan the API to support this format as input so you could just write this in place of the JSON syntax.\n\n\n\n\nSimplest Query\n\n\nThe simplest query you can write would be:\n\n\nBQL Query\n\n\nSELECT * FROM STREAM(30000, TIME) LIMIT 1;\n\n\n\n\nJSON Query\n\n\n{}\n\n\n\n\nWhile not a very useful query - this will get any one event record (no filters means that any record would be matched, no projection gets the entire record, and the default aggregation is \nLIMIT\nor \nRAW\n with size 1, default duration 30000 ms), this can be used to quickly test your connection to Bullet.\n\n\n\n\nWINDOW?\n\n\nThere is only one unified data stream in Bullet, so for clarity the \nFROM\n clause is replaced with a non-existent \nWINDOW\n function to denote the look-forward time window for the Bullet query. For the \nSQL\n examples, we pretend that this function understands various time granularities as well.\n\n\n\n\nSimple Filtering\n\n\nBQL Query\n\n\nSELECT *\nFROM STREAM(30000, TIME)\nWHERE id = 'btsg8l9b234ha'\nLIMIT 1;\n\n\n\n\nJSON Query\n\n\n{\n   \nfilters\n:[\n       {\n           \nfield\n:\nid\n,\n           \noperation\n:\n==\n,\n           \nvalues\n:[\n               \nbtsg8l9b234ha\n\n           ]\n       }\n    ]\n}\n\n\n\n\nBecause of the default constraints, this query would find at most 1 record with the id matching the value provided. The record would have all its fields.\n\n\nA sample response could be (it has been edited to remove PII and other Yahoo data). The response contains a single matching record, and the associated meta information.\n\n\n{\n   \nrecords\n:[\n       {\n           \nserver_name\n:\nEDITED\n,\n           \npage_uri\n:\n/\n,\n           \nis_page_view\n:true,\n           \ndevice\n:\ntablet\n,\n           \ndebug_codes\n:{\n               \nhttp_status_code\n:\n200\n\n           },\n           \nreferrer_domain\n:\nwww.yahoo.com\n,\n           \nis_logged_in\n:true,\n           \ntimestamp\n:1446842189000,\n           \nevent_family\n:\nview\n,\n           \nid\n:\nbtsg8l9b234ha\n,\n           \nos_name\n:\nmac os\n,\n           \ndemographics\n:{\n               \nage\n : \n25\n,\n               \ngender\n : \nm\n,\n            }\n       }\n    ],\n    \nmeta\n:{\n        \nquery_id\n:1167304238598842449,\n        \nquery_body\n:\n{}\n,\n        \nquery_finish_time\n:1480723799550,\n        \nquery_receive_time\n:1480723799540\n    }\n}\n\n\n\n\nRelational Filters and Projections\n\n\nBQL Query\n\n\nSELECT timestamp AS ts, device_timestamp AS device_ts,\n       event AS event, page_domain AS domain, page_id AS id\nFROM STREAM(20000, TIME)\nWHERE id = 'btsg8l9b234ha' AND page_id IS NOT NULL\nLIMIT 10;\n\n\n\n\nJSON Query\n\n\n{\n    \nfilters\n:[\n        {\n            \nfield\n:\nid\n,\n            \noperation\n:\n==\n,\n            \nvalues\n:[\n                \nbtsg8l9b234ha\n\n            ]\n        },\n        {\n            \nfield\n:\npage_id\n,\n            \noperation\n:\n!=\n,\n            \nvalues\n:[\n                \nnull\n\n            ]\n        }\n    ],\n    \nprojection\n:{\n        \nfields\n:{\n            \ntimestamp\n:\nts\n,\n            \ndevice_timestamp\n:\ndevice_ts\n,\n            \nevent\n:\nevent\n,\n            \npage_domain\n:\ndomain\n,\n            \npage_id\n:\nid\n\n        }\n    },\n    \naggregation\n:{\n        \ntype\n:\nRAW\n,\n        \nsize\n:10\n    },\n    \nduration\n:20000\n}\n\n\n\n\nThe above query finds all events with id set to 'btsg8l9b234ha' and page_id is not null, projects out the fields listed above with their new names (timestamp becomes ts etc) and limits the results to at most 10 such records. \nRAW\n indicates that the complete raw record fields will be returned, and more complicated aggregations such as \nCOUNT\n or \nSUM\n will not be performed. The duration would set the query to wait at most 20 seconds for records to show up.\n\n\nThe resulting response could look like (only 3 events were generated that matched the criteria):\n\n\n{\n    \nrecords\n: [\n        {\n            \ndomain\n: \nhttp://some.url.com\n,\n            \ndevice_ts\n: 1481152233788,\n            \nid\n: 2273844742998,\n            \nevent\n: \npage\n,\n            \nts\n: null\n        },\n        {\n            \ndomain\n: \nwww.yahoo.com\n,\n            \ndevice_ts\n: 1481152233788,\n            \nid\n: 227384472956,\n            \nevent\n: \nclick\n,\n            \nts\n: 1481152233888\n        },\n        {\n            \ndomain\n: \nhttps://news.yahoo.com\n,\n            \ndevice_ts\n: null,\n            \nid\n: 2273844742556,\n            \nevent\n: \npage\n,\n            \nts\n: null\n        }\n    ],\n    \nmeta\n: {\n        \nquery_id\n: -3239746252817510000,\n        \nquery_body\n: \nEDITED OUT\n,\n        \nquery_finish_time\n: 1481152233799,\n        \nquery_receive_time\n: 1481152233796\n    }\n}\n\n\n\n\nLogical Filters and Projections\n\n\nBQL Query\n\n\nSELECT id AS id, experience AS experience, page_id AS pid,\n       link_id AS lid, tags AS tags, demographics.age AS age\nFROM STREAM(60000, TIME)\nWHERE (id = 'c14plm1begla7' AND ((experience = 'web' AND page_id IN ('18025', '47729'))\n                                  OR link_id LIKE ('2.*')))\n      OR (tags.player='true' AND demographics.age \n '65')\nLIMIT 1;\n\n\n\n\nJSON Query\n\n\n{\n \nfilters\n: [\n                {\n                    \noperation\n: \nOR\n,\n                    \nclauses\n: [\n                        {\n                            \noperation\n: \nAND\n,\n                            \nclauses\n: [\n                                {\n                                    \nfield\n: \nid\n,\n                                    \noperation\n: \n==\n,\n                                    \nvalues\n: [\nc14plm1begla7\n]\n                                },\n                                {\n                                    \noperation\n: \nOR\n,\n                                    \nclauses\n: [\n                                        {\n                                            \noperation\n: \nAND\n,\n                                            \nclauses\n: [\n                                                {\n                                                    \nfield\n: \nexperience\n,\n                                                    \noperation\n: \n==\n,\n                                                    \nvalues\n: [\nweb\n]\n                                                },\n                                                {\n                                                    \nfield\n: \npage_id\n,\n                                                    \noperation\n: \n==\n,\n                                                    \nvalues\n: [\n18025\n, \n47729\n]\n                                                }\n                                            ]\n                                        },\n                                        {\n                                            \nfield\n: \nlink_id\n,\n                                            \noperation\n: \nRLIKE\n,\n                                            \nvalues\n: [\n2.*\n]\n                                        }\n                                    ]\n                                }\n                            ]\n                        },\n                        {\n                            \noperation\n: \nAND\n,\n                            \nclauses\n: [\n                                {\n                                    \nfield\n: \ntags.player\n,\n                                    \noperation\n: \n==\n,\n                                    \nvalues\n: [\ntrue\n]\n                                },\n                                {\n                                    \nfield\n: \ndemographics.age\n,\n                                    \noperation\n: \n,\n                                    \nvalues\n: [\n65\n]\n                                }\n                            ]\n                        }\n                    ]\n                }\n            ],\n \nprojection\n : {\n    \nfields\n: {\n        \nid\n: \nid\n,\n        \nexperience\n: \nexperience\n,\n        \npage_id\n: \npid\n,\n        \nlink_id\n: \nlid\n,\n        \ntags\n: \ntags\n,\n        \ndemographics.age\n: \nage\n\n    }\n },\n \naggregation\n: {\ntype\n : \nRAW\n, \nsize\n : 1},\n \nduration\n: 60000\n}\n\n\n\n\n\n\nTyping\n\n\nIf demographics[\"age\"] was of type Long, then Bullet will convert 85 to be an Long, but in this example, we are pretending that it is String.  So, no conversion is made. Similarly for link_id, id, experience and page_id. tags is a Map of String to Boolean so Bullet converts \n\"true\"\n to the Boolean \ntrue\n.\n\n\n\n\nThis query is looking for a single event with a specific id and either the page_id is in two specific pages on the \"web\" experience or with a link_id that starts with 2, or a player event where the age is greater than \"65\". In other words, it is looking for senior citizens who generate video player events or a particular person's (based on id) events on two specific pages or a group of pages that have link that have ids that start with 2. It then projects out only these fields with different names.\n\n\nA sample result could look like (it matched because of tags.player was true and demographics.age was \n 65):\n\n\n{\n    \nrecords\n: [\n        {\n            \npid\n:\n158\n,\n            \nid\n:\n0qcgofdbfqs9s\n,\n            \nexperience\n:\nweb\n,\n            \nlid\n:\n978500434\n,\n            \nage\n:\n66\n,\n            \ntags\n:{\nplayer\n:true}\n        }\n    ],\n    \nmeta\n: {\n        \nquery_id\n: 3239746252812284004,\n        \nquery_body\n: \nEDITED OUT\n,\n        \nquery_finish_time\n: 1481152233805,\n        \nquery_receive_time\n: 1481152233881\n    }\n}\n\n\n\n\nGROUP ALL COUNT Aggregation\n\n\nAn example of a query performing a COUNT all records aggregation would look like:\n\n\nBQL Query\n\n\nSELECT COUNT(*) AS numSeniors\nFROM STREAM(20000, TIME)\nWHERE demographics.age \n 65\nGROUP BY ();\n\n\n\n\nNote that the \nGROUP BY ()\n is optional.\n\n\nJSON Query\n\n\n{\n   \nfilters\n:[\n      {\n         \nfield\n:\ndemographics.age\n,\n         \noperation\n:\n,\n         \nvalues\n:[\n            \n65\n\n         ]\n      }\n   ],\n   \naggregation\n:{\n      \ntype\n:\nGROUP\n,\n      \nattributes\n:{\n         \noperations\n:[\n            {\n               \ntype\n:\nCOUNT\n,\n               \nnewName\n:\nnumSeniors\n\n            }\n         ]\n      }\n   },\n   \nduration\n:20000\n}\n\n\n\n\nThis query will count the number events for which demographics.age \n 65. The aggregation type GROUP indicates that it is a group aggregation. To group by a key, the \nfields\n key needs to be set in the \naggregation\n part of the query. If \nfields\n is empty or is omitted (as it is in the query above) and the \ntype\n is \nGROUP\n, it is as if all the records are collapsed into a single group - a \nGROUP ALL\n. Adding a \nCOUNT\n in the \noperations\n part of the \nattributes\n indicates that the number of records in this group will be counted, and the \"newName\" key denotes the name the resulting column \"numSeniors\" in the result. Setting the duration to 20000 counts matching records for\nthis duration.\n\n\nA sample result would look like:\n\n\n{\n    \nrecords\n: [\n        {\n            \nnumSeniors\n: 363201\n        }\n    ],\n    \nmeta\n: {}\n}\n\n\n\n\nThis result indicates that 363,201 records were counted with demographics.age \n 65 during the 20s the query was running.\n\n\nGROUP ALL Multiple Aggregations\n\n\nCOUNT is the only GROUP operation for which you can omit a \"field\".\n\n\nBQL Query\n\n\nSELECT COUNT(*) AS numCalifornians, AVG(demographics.age) AS avgAge,\n       MIN(demographics.age) AS minAge, MAX(demographics.age) AS maxAge\nFROM STREAM(20000, TIME)\nWHERE demographics.state = 'california'\nGROUP BY ();\n\n\n\n\nNote that the \nGROUP BY ()\n is optional.\n\n\nJSON Query\n\n\n{\n   \nfilters\n:[\n      {\n         \nfield\n:\ndemographics.state\n,\n         \noperation\n:\n==\n,\n         \nvalues\n:[\n            \ncalifornia\n\n         ]\n      }\n   ],\n   \naggregation\n:{\n      \ntype\n:\nGROUP\n,\n      \nattributes\n:{\n         \noperations\n:[\n            {\n               \ntype\n:\nCOUNT\n,\n               \nnewName\n:\nnumCalifornians\n\n            },\n            {\n               \ntype\n:\nAVG\n,\n               \nfield\n:\ndemographics.age\n,\n               \nnewName\n:\navgAge\n\n            },\n            {\n               \ntype\n:\nMIN\n,\n               \nfield\n:\ndemographics.age\n,\n               \nnewName\n:\nminAge\n\n            },\n            {\n               \ntype\n:\nMAX\n,\n               \nfield\n:\ndemographics.age\n,\n               \nnewName\n:\nmaxAge\n\n            }\n         ]\n      }\n   },\n   \nduration\n:20000\n}\n\n\n\n\nA sample result would look like:\n\n\n{\n    \nrecords\n: [\n        {\n            \nmaxAge\n: 94.0,\n            \nnumCalifornians\n: 188451,\n            \nminAge\n: 6.0,\n            \navgAge\n: 33.71828\n        }\n    ],\n    \nmeta\n: {\n        \nquery_id\n: 8051040987827161000,\n        \nquery_body\n: \nEDITED OUT\n,\n        \nquery_finish_time\n: 1482371927435,\n        \nquery_receive_time\n: 1482371916625\n    }\n}\n\n\n\n\nThis result indicates that, among the records observed during the 20s this query ran, there were 188,451 users with demographics.state equal to \"california\". Among these users the average age was 33.71828, the max age observed was 94, and the minimum age observed was 6.\n\n\nExact COUNT DISTINCT Aggregation\n\n\nBQL Query\n\n\nSELECT COUNT(DISTINCT browser_name, browser_version) AS \nCOUNT DISTINCT\n\nFROM STREAM(10000, TIME);\n\n\n\n\nJSON Query\n\n\n{\n   \naggregation\n:{\n      \ntype\n:\nCOUNT DISTINCT\n,\n      \nfields\n:{\n         \nbrowser_name\n:\n,\n         \nbrowser_version\n:\n\n      }\n   }\n}\n\n\n\n\nThis gets the count of the unique browser names and versions in the next 30s (default duration). Note that we do not specify values for the keys in fields. This is because they are not relevant\n\n\n{\n    \nrecords\n: [\n        {\n            \nCOUNT DISTINCT\n: 158.0\n        }\n    ],\n    \nmeta\n: {\n        \nquery_id\n: 4451146261377394443,\n        \nsketches\n: {\n            \nstandard_deviations\n: {\n                \n1\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                },\n                \n2\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                },\n                \n3\n: {\n                    \nupperBound\n: 158.0,\n                    \nlowerBound\n: 158.0\n                }\n            },\n            \nwas_estimated\n: false,\n            \nfamily\n: \nTHETA\n,\n            \ntheta\n: 1.0,\n            \nsize\n: 1280\n        },\n        \nquery_body\n: \nEDITED OUT\n,\n        \nquery_finish_time\n: 1484084869073,\n        \nquery_receive_time\n: 1484084832684\n    }\n}\n\n\n\n\nThere were 158 unique combinations on browser names and versions in our dataset for those 30 s. Note the new \nsketches\n object in the meta. It has various metadata about the result. In particular, the \nwas_estimated\n key denotes where the result\nwas estimated or not. The \nstandard_deviations\n key denotes the confidence at various sigmas: 1 (1 sigma = ~68% confidence, 2 sigma = ~95% confidence, 3 sigma = ~99% confidence). Since this result was not estimated, the result is the same as the upper and lower bounds for the result.\n\n\nApproximate COUNT DISTINCT\n\n\nBQL Query\n\n\nSELECT COUNT(DISTINCT ip_address) AS uniqueIPs\nFROM STREAM(10000, TIME);\n\n\n\n\nJSON Query\n\n\n{\n   \naggregation\n:{\n      \ntype\n:\nCOUNT DISTINCT\n,\n      \nfields\n:{\n         \nip_address\n:\n\n      },\n      \nattributes\n:{\n         \nnewName\n:\nuniqueIPs\n\n      }\n   },\n   \nduration\n:10000\n}\n\n\n\n\nThis query gets us the unique IP addresses in the next 10 s. It renames the result column from \"COUNT DISTINCT\" to \"uniqueIPs\".\n\n\n{\n   \nrecords\n:[\n      {\n         \nuniqueIPs\n:130551.07952805843\n      }\n   ],\n   \nmeta\n:{\n      \nquery_id\n:5377782455857451480,\n      \nsketches\n:{\n         \nstandard_deviations\n:{\n            \n1\n:{\n               \nupperBound\n:131512.85413760383,\n               \nlowerBound\n:129596.30223107953\n            },\n            \n2\n:{\n               \nupperBound\n:132477.15103015225,\n               \nlowerBound\n:128652.93906100772\n            },\n            \n3\n:{\n               \nupperBound\n:133448.49248615955,\n               \nlowerBound\n:127716.46773622213\n            }\n         },\n         \nwas_estimated\n:true,\n         \nfamily\n:\nTHETA\n,\n         \ntheta\n:0.12549877074343688,\n         \nsize\n:131096\n      },\n      \nquery_body\n:\nEDITED OUT\n,\n      \nquery_finish_time\n:1484090240812,\n      \nquery_receive_time\n:1484090223351\n   }\n}\n\n\n\n\nThe number of unique IPs in our dataset was 130551 in those 10s (approximately) with the true value between (129596, 131512) at 68% confidence, (128652, 132477) at 95% confidence and (127716, 133448) at 99% confidence. In the \nworst\n case at 3 sigma (99% confidence),\nour error is 2.17%. The final result was computed with 131096 bytes or ~128 KiB as denoted by \nsize\n. This happens to be maximum size the the COUNT DISTINCT sketch will take up at the default nominal entries, so even if we had billions of unique IPs, the size will be the same and the error may be higher (depends on the distribution). For example, the error when the same query was run for 30s was 2.28% at 99% confidence (actual unique IPs: 559428, upper bound: 572514). In fact, the worst the error can get at this\nSketch size is 2.34% as defined \nhere\n, \nregardless of the number of unique entries added to the Sketch!\n.\n\n\nDISTINCT Aggregation\n\n\nBQL Query\n\n\nSELECT browser_name AS browser\nFROM STREAM(30000, TIME)\nGROUP BY browser_name\nLIMIT 10;\n\n\n\n\nJSON Query\n\n\n{\n   \naggregation\n:{\n      \ntype\n:\nDISTINCT\n,\n      \nsize\n:10,\n      \nfields\n:{\n         \nbrowser_name\n:\nbrowser\n\n      }\n   }\n}\n\n\n\n\nThis query gets the distinct values for the browser_name field and limit the results to 10. It runs for 30 s.\n\n\n{\n   \nrecords\n:[\n      {\n         \nbrowser\n:\nopera\n\n      },\n      {\n         \nbrowser\n:\nflock\n\n      },\n      {\n         \nbrowser\n:\nlinks\n\n      },\n      {\n         \nbrowser\n:\nmozilla firefox\n\n      },\n      {\n         \nbrowser\n:\ndolfin\n\n      },\n      {\n         \nbrowser\n:\nlynx\n\n      },\n      {\n         \nbrowser\n:\nchrome\n\n      },\n      {\n         \nbrowser\n:\nmicrosoft internet explorer\n\n      },\n      {\n         \nbrowser\n:\naol browser\n\n      },\n      {\n         \nbrowser\n:\nedge\n\n      }\n   ],\n   \nmeta\n:{\n      \nquery_id\n:-4872093887360741287,\n      \nsketches\n:{\n         \nstandard_deviations\n:{\n            \n1\n:{\n               \nupperBound\n:28.0,\n               \nlowerBound\n:28.0\n            },\n            \n2\n:{\n               \nupperBound\n:28.0,\n               \nlowerBound\n:28.0\n            },\n            \n3\n:{\n               \nupperBound\n:28.0,\n               \nlowerBound\n:28.0\n            }\n         },\n         \nwas_estimated\n:false,\n         \nfamily\n:\nTUPLE\n,\n         \nuniques_estimate\n:28.0,\n         \ntheta\n:1.0\n      },\n      \nquery_body\n:\nEDITED OUT\n,\n      \nquery_finish_time\n:1485469087971,\n      \nquery_receive_time\n:1485469054070\n   }\n}\n\n\n\n\nThere were 28 unique results but we asked for 10, so the query returned a uniform sample across the 28 distinct values.\n\n\nDISTINCT is just an alias for GROUP. A GROUP by with no operations is exactly a DISTINCT.\n\n\nGROUP by Aggregation\n\n\nBQL Query\n\n\nSELECT demographics.country AS country, device AS device,\n       COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nGROUP BY demographics.country, device\nLIMIT 50;\n\n\n\n\nJSON Query\n\n\n{\n   \nfilters\n:[\n      {\n         \nfield\n:\ndemographics\n,\n         \noperation\n:\n!=\n,\n         \nvalues\n:[\n            \nnull\n\n         ]\n      }\n   ],\n   \naggregation\n:{\n      \ntype\n:\nGROUP\n,\n      \nsize\n:50,\n      \nfields\n:{\n         \ndemographics.country\n:\ncountry\n,\n         \ndevice\n:\n\n      },\n      \nattributes\n:{\n         \noperations\n:[\n            {\n               \ntype\n:\nCOUNT\n,\n               \nnewName\n:\ncount\n\n            },\n            {\n               \ntype\n:\nAVG\n,\n               \nfield\n:\ndemographics.age\n,\n               \nnewName\n:\naverageAge\n\n            },\n            {\n               \ntype\n:\nAVG\n,\n               \nfield\n:\ntimespent\n,\n               \nnewName\n:\naverageTimespent\n\n            }\n         ]\n      }\n   },\n   \nduration\n:20000\n}\n\n\n\n\nThis query groups by the country and the device and for each unique group gets the count, average age and time spent by the users for the next 20 seconds. It renames demographics.country to country and does not rename device. It limits the groups to 50. If there were more than\n50 groups, the results would be a uniform sampling of the groups (but each group in the result would have the correct result). These parameters can all be tweaked \nin the configuration\n.\n\n\n{\n   \nrecords\n:[\n      {\n         \ncountry\n:\nuk\n,\n         \ndevice\n:\ndesktop\n,\n         \ncount\n:203034,\n         \naverageAge\n:32.42523,\n         \naverageTimespent\n:1.342\n      },\n      {\n         \ncountry\n:\nus\n,\n         \ndevice\n:\ndesktop\n,\n         \ncount\n:1934030,\n         \naverageAge\n:29.42523,\n         \naverageTimespent\n:3.234520\n      },\n      \n...EDITED 41 other such records out for readability...\n,\n   ],\n   \nmeta\n:{\n      \nquery_id\n:1705911449584057747,\n      \nsketches\n:{\n         \nstandard_deviations\n:{\n            \n1\n:{\n               \nupperBound\n:43.0,\n               \nlowerBound\n:43.0\n            },\n            \n2\n:{\n               \nupperBound\n:43.0,\n               \nlowerBound\n:43.0\n            },\n            \n3\n:{\n               \nupperBound\n:43.0,\n               \nlowerBound\n:43.0\n            }\n         },\n         \nwas_estimated\n:false,\n         \nfamily\n:\nTUPLE\n,\n         \nuniques_estimate\n:43.0,\n         \ntheta\n:1.0\n      },\n      \nquery_body\n:\nEDITED OUT\n,\n      \nquery_finish_time\n:1485217172780,\n      \nquery_receive_time\n:1485217148840\n   }\n}\n\n\n\n\nWe received 43 rows for this result. The maximum groups that was allowed for the instance of Bullet was 512. If there were more groups than the maximum specified by your configuration, \na uniform sample\n across them would be chosen\nfor the result. However, for each group, the values computed (average, count) would be exact. The standard deviations, whether the result was estimated and the number of approximate uniques in the metadata would reflect the change.\n\n\nIf you asked for 50 rows in the aggregation (as the query did above) but there were more than 50 in the result (but \n 512), the metadata would reflect the fact that the result was not estimated. You would still get a uniform sample\nbut by increasing your aggregation size higher, you could get the rest.\n\n\nFor readability, if you were just trying to get the unique values for a field or a set of fields, you could leave out the attributes section and specify your fields section. You could also call the type \nDISTINCT\n instead of\n\nGROUP\n to make that explicit. \nDISTINCT\n is just an alias for \nGROUP\n. See \nthe DISTINCT example\n.\n\n\nQUANTILE DISTRIBUTION\n\n\nBQL Query\n\n\nSELECT QUANTILE(duration, LINEAR, 11)\nFROM STREAM(5000, TIME)\nLIMIT 11;\n\n\n\n\nJSON Query\n\n\n{\n   \naggregation\n:{\n      \ntype\n: \nDISTRIBUTION\n,\n      \nsize\n: 11,\n      \nfields\n: {\n          \nduration\n: \n\n      },\n      \nattributes\n: {\n          \ntype\n: \nQUANTILE\n,\n          \nnumberOfPoints\n: 11\n      }\n   },\n   \nduration\n: 5000\n}\n\n\n\n\nThis query creates 11 points from 0 to 1 (both inclusive) and finds the percentile values of the \nduration\n field (which contains an amount of time in ms) at \n0, 0.1, 0.2 ... 1.0\n or the 0th, 10th, 20th and 100th percentiles. It runs for 5 seconds and returns at most 11 points. As long as the \nsize\n is set to higher than the number of points you generate, \nDISTRIBUTION\n queries will return all your values.\n\n\nThe SQL is not really the same since it will produce one row instead of 11.\n\n\n{\n   \nrecords\n:[\n      {\n         \nValue\n:1,\n         \nQuantile\n:0\n      },\n      {\n         \nValue\n:1352,\n         \nQuantile\n:0.1\n      },\n      {\n         \nValue\n:3045,\n         \nQuantile\n:0.2\n      },\n      {\n         \nValue\n:6501,\n         \nQuantile\n:0.30000000000000004\n      },\n      {\n         \nValue\n:10700,\n         \nQuantile\n:0.4\n      },\n      {\n         \nValue\n:17488,\n         \nQuantile\n:0.5\n      },\n      {\n         \nValue\n:28659,\n         \nQuantile\n:0.6\n      },\n      {\n         \nValue\n:47929,\n         \nQuantile\n:0.7\n      },\n      {\n         \nValue\n:83447,\n         \nQuantile\n:0.7999999999999999\n      },\n      {\n         \nValue\n:177548,\n         \nQuantile\n:0.8999999999999999\n      },\n      {\n         \nValue\n:83525609,\n         \nQuantile\n:1\n      }\n   ],\n   \nmeta\n:{\n      \nquery_finish_time\n:1493748546533,\n      \nquery_body\n: \nEDITED OUT\n,\n      \nquery_id\n:2981902209347343400,\n      \nsketches\n:{\n         \nnormalized_rank_error\n:0.002389303789572841,\n         \nsize\n:16416,\n         \nminimum_value\n:1,\n         \nitems_seen\n:1414,\n         \nmaximum_value\n:83525609,\n         \nfamily\n:\nQUANTILES\n,\n         \nwas_estimated\n:false\n      },\n      \nquery_receive_time\n:1493748538259\n   }\n}\n\n\n\n\nThe result shows the values at the 0th, 10th percentiles etc. The \nwas_estimated\n key indicates that the result was not approximated. Note the the \nminimum_value\n and the \nmaximum_value\n correspond to the 0th and 100th percentiles. There is also a \nnormalized_rank_error\n that describes the error (see \nbelow\n for a detailed explanation) This is constant for all \nDISTRIBUTION\n queries and does not depend on the data or the query.\n\n\nNormalized Rank Error\n\n\nUnlike \nGROUP\n and \nCOUNT DISTINCT\n, the order in which the data arrives to Bullet can affect the results of a \nDISTRIBUTION\n query. The error when the result is estimated is not a Gaussian error function\nand is not described in terms of the values of your field. In other words, if the 50th percentile was estimated to some value, you could not bound the true median by using the the estimated\nvalue +/- constant (see below for a good approximation). The error is expressed in terms of the \nnormalized rank\n. If one were to sort the true data stream, you would obtain a rank for each item from\n0 to the stream length (\nitems_seen\n in the metadata above). If you then divided each rank by length, you would get ranks from 0 to 1. In this domain, a \nnormalized rank error\n of 0.002, for example, means that a value\nreturned for the 0.50 or 50th percentile could actually lie between 0.498 and 0.502 in the normalized ranks with 99% confidence.\n\n\nDistribution Accuracy\n lists the normalized rank error as a percentage for the maximum size of the Sketch used. If you obtain successive quantile values at\ngranularities lower than this rank error, the results may not be accurate. While the sketch speaks of the normalized rank error, you can still obtain reasonable bounds for values. For example, if the normalized rank\nerror was 1% and you obtained quantile values at 0.48, 0.50 and 0.52, you could use the values at 0.52 and 0.48 as very reasonable upper and lower bounds on your true median (you might even be able to use 0.49 and 0.51\nif the error was 1%).\n\n\nPMF DISTRIBUTION Aggregation\n\n\nBQL Query\n\n\nSELECT FREQ(duration, REGION, 2000, 20000, 500)\nFROM STREAM(5000, TIME)\nLIMIT 100;\n\n\n\n\nJSON Query\n\n\n{\n   \naggregation\n:{\n      \ntype\n:\nDISTRIBUTION\n,\n      \nsize\n:100,\n      \nfields\n:{\n         \nduration\n:\n\n      },\n      \nattributes\n:{\n         \ntype\n:\nPMF\n,\n         \nstart\n:2000,\n         \nend\n:20000,\n         \nincrement\n:500\n      }\n   },\n   \nduration\n:5000\n}\n\n\n\n\nThis query creates 37 points from 2000 to 20000 in 500 increments to bucketize the duration field using these points as split locations and finds the count of duration values that fall into these intervals. It runs for 5s and returns at most 100 records (this means it will return the 38 records).\n\n\nThe SQL does not include the \n(-\n to 2000)\n and the \n[20000 to +\n)\n intervals and does not produce a probability.\n\n\n{\n   \nrecords\n:[\n      {\n         \nProbability\n:0.1518054532056006,\n         \nCount\n:206,\n         \nRange\n:\n(-\u221e to 2000.0)\n\n      },\n      {\n         \nProbability\n:0.0397936624907885,\n         \nCount\n:53.99999999999999,\n         \nRange\n:\n[2000.0 to 2500.0)\n\n      },\n      \n...EDITED 34 other such records out for readability...\n,\n      {\n         \nProbability\n:0.0058953574060427415,\n         \nCount\n:8,\n         \nRange\n:\n[19500.0 to 20000.0)\n\n      },\n      {\n         \nProbability\n:0.45689019896831246,\n         \nCount\n:620,\n         \nRange\n:\n[20000.0 to +\u221e)\n\n      }\n   ],\n   \nmeta\n:{\n      \nquery_finish_time\n:1493750074795,\n      \nquery_body\n: \nEDITED OUT\n,\n      \nquery_id\n:-2590566941995678000,\n      \nsketches\n:{\n         \nnormalized_rank_error\n:0.002389303789572841,\n         \nsize\n:16416,\n         \nminimum_value\n:1,\n         \nitems_seen\n:1357,\n         \nmaximum_value\n:78240570,\n         \nfamily\n:\nQUANTILES\n,\n         \nwas_estimated\n:false\n      },\n      \nquery_receive_time\n:1493750066022\n   }\n}\n\n\n\n\nThe result consists of 38 records, each denoting an interval in the domain we asked for. The result was not estimated. Note that the interval is denoted by the \nRange\n key and the count by the \nCount\n key. There is also a probability that estimates how likely a value for duration is likely to fall into that range.\n\n\nCDF DISTRIBUTION Aggregation\n\n\nBQL Query\n\n\nSELECT CUMFREQ(duration, MANUAL, 20000, 2000, 15000, 45000)\nFROM STREAM(5000, TIME)\nLIMIT 100;\n\n\n\n\nJSON Query\n\n\n{\n   \naggregation\n:{\n      \ntype\n: \nDISTRIBUTION\n,\n      \nsize\n: 100,\n      \nfields\n: {\n          \nduration\n: \n\n      },\n      \nattributes\n: {\n          \ntype\n: \nCDF\n,\n          \npoints\n: [20000, 2000, 15000, 45000]\n      }\n   },\n   \nduration\n: 5000\n}\n\n\n\n\nThis query specifies a list of points manually using \npoints\n property in \nattributes\n. It runs for 5s and finds the\ncumulative frequency distribution using the specified points as break points. It returns at most 100 records (which means we will\nget all of the intervals).\n\n\nThere is no easy SQL equivalent because the points are free-form. It does not produce a probability field like Bullet does.\n\n\n{\n   \nrecords\n:[\n      {\n         \nProbability\n:0.14382632293080055,\n         \nCount\n:212.00000000000003,\n         \nRange\n:\n(-\u221e to 2000.0)\n\n      },\n      {\n         \nProbability\n:0.5210312075983717,\n         \nCount\n:767.9999999999999,\n         \nRange\n:\n(-\u221e to 15000.0)\n\n      },\n      {\n         \nProbability\n:0.5603799185888738,\n         \nCount\n:826,\n         \nRange\n:\n(-\u221e to 20000.0)\n\n      },\n      {\n         \nProbability\n:0.6994572591587517,\n         \nCount\n:1031,\n         \nRange\n:\n(-\u221e to 45000.0)\n\n      },\n      {\n         \nProbability\n:1,\n         \nCount\n:1474,\n         \nRange\n:\n(-\u221e to +\u221e)\n\n      }\n   ],\n   \nmeta\n:{\n      \nquery_finish_time\n:1493755151660,\n      \nquery_body\n: \nEDITED OUT\n,\n      \nquery_id\n:-8460702488693518000,\n      \nsketches\n:{\n         \nnormalized_rank_error\n:0.002389303789572841,\n         \nsize\n:16416,\n         \nminimum_value\n:2,\n         \nitems_seen\n:1474,\n         \nmaximum_value\n:10851113,\n         \nfamily\n:\nQUANTILES\n,\n         \nwas_estimated\n:false\n      },\n      \nquery_receive_time\n:1493755143626\n   }\n}\n\n\n\n\nThe result contains the 5 intervals produced by the split points. It was not estimated so these counts are exact. Note that the start of each interval is \n-\n because\nit is the cumulative frequency distribution.\n\n\nExact TOP K Aggregation\n\n\nBQL Query\n\n\nThere are two methods for executing a TOP K aggregation in BQL:\n\n\nSELECT TOP(500, 100, demographics.country, browser_name) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE demographics.country IS NOT NULL AND browser_name IS NOT NULL;\n\n\n\n\nOR:\n\n\nSELECT demographics.country, browser_name, COUNT(*) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE demographics.country IS NOT NULL AND browser_name IS NOT NULL\nGROUP BY demographics.country, browser_name\nHAVING COUNT(*) \n= 100\nORDER BY COUNT(*) DESC\nLIMIT 500;\n\n\n\n\nJSON Query\n\n\n{\n   \nfilters\n:[\n      {\n         \nfield\n: \ndemographics.country\n,\n         \noperation\n: \n!=\n,\n         \nvalues\n: [\nnull\n]\n      },\n      {\n         \nfield\n: \nbrowser_name\n,\n         \noperation\n: \n!=\n,\n         \nvalues\n: [\nnull\n]\n      }\n   ],\n   \naggregation\n:{\n      \ntype\n: \nTOP K\n,\n      \nsize\n: 500,\n      \nfields\n: {\n          \nbrowser_name\n: \nbrowser\n,\n          \ndemographics.country\n: \ncountry\n\n      },\n      \nattributes\n: {\n          \nthreshold\n: 100,\n          \nnewName\n: \nnumEvents\n\n      }\n   },\n   \nduration\n: 10000\n}\n\n\n\n\nThis query gets the top 500 country, browser combinations where the count of records for each combination is at least 100. It runs for 10s.\n\n\n{\n   \nrecords\n:[\n      {\n         \ncountry\n:\nus\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:2729\n      },\n      {\n         \ncountry\n:\nus\n,\n         \nbrowser\n:\nmozilla firefox\n,\n         \nnumEvents\n:1072\n      },\n      {\n         \ncountry\n:\nuk\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:703\n      },\n      {\n         \ncountry\n:\nfr\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:383\n      },\n      {\n         \ncountry\n:\nfr\n,\n         \nbrowser\n:\nmozilla firefox\n,\n         \nnumEvents\n:278\n      },\n      {\n         \ncountry\n:\nes\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:234\n      },\n      \n...EDITED 10 other such records here for readability\n,\n      {\n         \ncountry\n:\nes\n,\n         \nbrowser\n:\nmozilla firefox\n,\n         \nnumEvents\n:102\n      },\n      {\n         \ncountry\n:\nfr\n,\n         \nbrowser\n:\napple safari\n,\n         \nnumEvents\n:101\n      }\n   ],\n   \nmeta\n:{\n      \nquery_finish_time\n:1493760034414,\n      \nquery_body\n: \nEDITED OUT\n,\n      \nquery_id\n:7515243052399540000,\n      \nsketches\n:{\n         \nmaximum_count_error\n:0,\n         \nactive_items\n:431,\n         \nitems_seen\n:10784,\n         \nfamily\n:\nFREQUENCY\n,\n         \nwas_estimated\n:false\n      },\n      \nquery_receive_time\n:1493760020807\n   }\n}\n\n\n\n\nThe results gave us the top 18 country, browser combinations that had counts over a 100. Note the \nmaximum_count_error\n key in the metadata. This represents how off the count is. It is 0 because these counts are exact.\nIn our data stream, we only had 18 unique combinations of countries and browser names at the time the query was run.\n\n\nApproximate TOP K Aggregation\n\n\nBQL Query\n\n\nThere are two methods for executing a TOP K aggregation in BQL:\n\n\nSELECT TOP(10, 100, browser_name, browser_version, os_name, os_version, demographics.country, demographics.state) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE os_name IS NOT NULL AND browser_name IS NOT NULL;\n\n\n\n\nOR:\n\n\nSELECT browser_name, browser_version, os_name, os_version, demographics.country, demographics.state, COUNT(*) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE os_name IS NOT NULL AND browser_name IS NOT NULL\nGROUP BY browser_name, browser_version, os_name, os_version, demographics.country, demographics.state\nHAVING COUNT(*) \n= 100\nORDER BY COUNT(*) DESC\nLIMIT 10;\n\n\n\n\nJSON Query\n\n\n{\n   \nfilters\n:[\n      {\n         \nfield\n: \nbrowser_name\n,\n         \noperation\n: \n!=\n,\n         \nvalues\n: [\nnull\n]\n      },\n      {\n         \nfield\n: \nos_name\n,\n         \noperation\n: \n!=\n,\n         \nvalues\n: [\nnull\n]\n      }\n   ],\n   \naggregation\n:{\n      \ntype\n: \nTOP K\n,\n      \nsize\n: 10,\n      \nfields\n: {\n          \nbrowser_name\n: \nbrowser\n,\n          \nbrowser_version\n: \nbversion\n,\n          \nos_name\n: \nos\n,\n          \nos_version\n: \noversion\n,\n          \ndemographics.country\n: \ncountry\n,\n          \ndemographics.state\n: \nstate\n\n      },\n      \nattributes\n: {\n          \nthreshold\n: 100,\n          \nnewName\n: \nnumEvents\n\n      }\n   },\n   \nduration\n: 30000\n}\n\n\n\n\nIn order to make the result approximate, this query adds more dimensions to the \nExact TOP K\n query. It runs for 30s and looks for the top \n10\n combinations for these events.\n\n\n{\n   \nrecords\n:[\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nmac os x\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:120823,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n56\n,\n         \noversion\n:\n10.12\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nmac os x\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:4539,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n35\n,\n         \noversion\n:\n10.9\n\n      },\n      {\n         \ncountry\n:\nus\n,\n         \nos\n:\nmac os x\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:3827,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n57\n,\n         \noversion\n:\n10.12\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nios\n,\n         \nbrowser\n:\napple safari\n,\n         \nnumEvents\n:3426,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n9.0\n,\n         \noversion\n:\n9.1\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nwindows nt\n,\n         \nbrowser\n:\nmicrosoft internet explorer\n,\n         \nnumEvents\n:2264,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n6.0\n,\n         \noversion\n:\n5.1\n\n      },\n      {\n         \ncountry\n:\nus\n,\n         \nos\n:\nmac os x\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:1995,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n58\n,\n         \noversion\n:\n10.12\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nwindows nt\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:1416,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n57\n,\n         \noversion\n:\n10.0\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nwindows nt\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:1327,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n58\n,\n         \noversion\n:\n10.0\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nmac os x\n,\n         \nbrowser\n:\ngoogle chrome\n,\n         \nnumEvents\n:1187,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n57\n,\n         \noversion\n:\n10.12\n\n      },\n      {\n         \ncountry\n:\nnull\n,\n         \nos\n:\nios\n,\n         \nbrowser\n:\napple safari\n,\n         \nnumEvents\n:1119,\n         \nstate\n:\nnull\n,\n         \nbversion\n:\n4.0\n,\n         \noversion\n:\n3.0\n\n      }\n   ],\n   \nmeta\n:{\n      \nquery_finish_time\n:1493761419611,\n      \nquery_body\n: \nEDITED OUT\n,\n      \nquery_id\n:-8797534873217479000,\n      \nsketches\n:{\n         \nmaximum_count_error\n:24,\n         \nactive_items\n:746,\n         \nitems_seen\n:187075,\n         \nfamily\n:\nFREQUENCY\n,\n         \nwas_estimated\n:true\n      },\n      \nquery_receive_time\n:1493761386294\n   }\n}\n\n\n\n\nLike \nDISTRIBUTION\n, the distribution of the data matters for \nTOP K\n. Depending on the distribution, your results could produce different counts and errors bounds if approximate.\n\n\nSince we only filtered for nulls in a couple of fields, the top results end up being fields with null values. Note that the \nmaximum_count_error\n is now 24 and the \nwas_estimated\n property is\nset to true. 24 means that the upper bound - the lower bound for the \nCount\n field for each combination could be off by at most 24. Since Bullet gives you the upper bound, this means that if you\nsubtract 24 from it, you get the lower bound of the true count.\n\n\nNote that this also means the order of the items could be off. If two items had \nCount\n within 24 of each other, it is possible that the higher one \nmay\n actually have had a true count \nlower\n than\nthe second one and possibly be ranked higher. There is no such situation in this result set.\n\n\nWindow - Tumbling Group-By\n\n\nBQL Query\n\n\nSELECT demographics.country AS country, COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nGROUP BY demographics.country\nWINDOWING(TUMBLING, 5000, TIME)\nLIMIT 50;\n\n\n\n\nJSON Query\n\n\n{\n   \nfilters\n:[\n      {\n         \nfield\n:\ndemographics\n,\n         \noperation\n:\n!=\n,\n         \nvalues\n:[\n            \nnull\n\n         ]\n      }\n   ],\n   \naggregation\n:{\n      \ntype\n:\nGROUP\n,\n      \nsize\n:5,\n      \nfields\n:{\n         \ndemographics.country\n:\ncountry\n\n      },\n      \nattributes\n:{\n         \noperations\n:[\n            {\n               \ntype\n:\nCOUNT\n,\n               \nnewName\n:\ncount\n\n            },\n            {\n               \ntype\n:\nAVG\n,\n               \nfield\n:\ndemographics.age\n,\n               \nnewName\n:\naverageAge\n\n            }\n         ]\n      }\n   },\n   \nwindow\n:{\n       \nemit\n:{\n           \ntype\n: \nTIME\n,\n           \nevery\n: 5000\n       },\n       \ninclude\n:{\n           \ntype\n: \nTIME\n,\n           \nfirst\n: 5000\n       }\n   },\n   \nduration\n:20000\n}\n\n\n\n\nThis query specifies a tumbling window that will emit every 5 seconds and contain 5 seconds of data per window. Results will come back to the user every 5 seconds, and since the duration of the query is 20 seconds,\nthe user will receive a total of 4 results. Since the aggregation size is set to 5, each returned window will contain only 5 groups (which will be chosen randomly). The result might look like this:\n\n\nrecords\n:[\n    {\n        \ncountry\n:\nGermany\n,\n        \ncount\n:1,\n        \naverageAge\n:25.0\n    },\n    {\n        \ncountry\n:\nCanada\n,\n        \ncount\n:106,\n        \naverageAge\n:22.58490566037736\n    },\n    {\n        \ncountry\n:\nUSA\n,\n        \ncount\n:1,\n        \naverageAge\n:28.0\n    },\n    {\n        \ncountry\n:\nEngland\n,\n        \ncount\n:8,\n        \naverageAge\n:34.25\n    },\n    {\n        \ncountry\n:\nPeru\n,\n        \ncount\n:9,\n        \naverageAge\n:30.0\n    }\n],\n\nmeta\n:{\n    \nWindow\n:{\n        \nNumber\n:1,\n        \nEmit Time\n:1529458403038,\n        \nExpected Emit Time\n:1529458403023,\n        \nName\n:\nTumbling\n\n        },\n    \nQuery\n:{\n        \nID\n:\n448d228a-1eed-471f-8777-c800cc866535\n,\n        \nReceive Time\n:1529458398023,\n        \nBody\n:\n...(query body)...},\n        \nSketch\n:{\n            \nWas Estimated\n:false,\n            \nUniques Estimate\n:100.0,\n            \nFamily\n:\nTUPLE\n,\n            \nTheta\n:1.0,\n            \nStandard Deviations\n:{\n                \n1\n:{\n                    \nupperBound\n:100.0,\n                    \nlowerBound\n:100.0\n                },\n                \n2\n:{\n                    \nupperBound\n:100.0,\n                    \nlowerBound\n:100.0\n                },\n                \n3\n:{\n                    \nupperBound\n:100.0,\n                    \nlowerBound\n:100.0\n                }\n            }\n        }\n    }\n}\n\n\n\nrecords\n:[  \n   {  \n      \ncountry\n:\nCanada\n,\n      \ncount\n:101,\n      \naverageAge\n:32.742574257425744\n   },\n   {  \n      \ncountry\n:\nht\n,\n      \ncount\n:2,\n      \naverageAge\n:32.0\n   },\n   {  \n      \ncountry\n:\nEngland\n,\n      \ncount\n:16,\n      \naverageAge\n:27.0625\n   },\n   {  \n      \ncountry\n:\nPeru\n,\n      \ncount\n:8,\n      \naverageAge\n:23.625\n   },\n   {  \n      \ncountry\n:\nBangladesh\n,\n      \ncount\n:3,\n      \naverageAge\n:27.66666666666667\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:2,\n      \nEmit Time\n:1529458408036,\n      \nExpected Emit Time\n:1529458408023,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n448d228a-1eed-471f-8777-c800cc866535\n,\n      \nReceive Time\n:1529458398023,\n      \nBody\n:\n...(query body)...\n\n   },\n   \nSketch\n:{  \n      \nWas Estimated\n:false,\n      \nUniques Estimate\n:98.0,\n      \nFamily\n:\nTUPLE\n,\n      \nTheta\n:1.0,\n      \nStandard Deviations\n:{  \n         \n1\n:{  \n            \nupperBound\n:98.0,\n            \nlowerBound\n:98.0\n         },\n         \n2\n:{  \n            \nupperBound\n:98.0,\n            \nlowerBound\n:98.0\n         },\n         \n3\n:{  \n            \nupperBound\n:98.0,\n            \nlowerBound\n:98.0\n         }\n      }\n   }\n}\n\n\n\nrecords\n:[  \n   {  \n      \ncountry\n:\nCanada\n,\n      \ncount\n:121,\n      \naverageAge\n:27.97520661157025\n   },\n   {  \n      \ncountry\n:\nHaiti\n,\n      \ncount\n:3,\n      \naverageAge\n:39.0\n   },\n   {  \n      \ncountry\n:\nCabuyao laguna\n,\n      \ncount\n:2,\n      \naverageAge\n:28.0\n   },\n   {  \n      \ncountry\n:\nUSA\n,\n      \ncount\n:1,\n      \naverageAge\n:20.0\n   },\n   {  \n      \ncountry\n:\nEngland\n,\n      \ncount\n:23,\n      \naverageAge\n:40.869565217391305\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:3,\n      \nEmit Time\n:1529458413031,\n      \nExpected Emit Time\n:1529458413023,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n448d228a-1eed-471f-8777-c800cc866535\n,\n      \nReceive Time\n:1529458398023,\n      \nBody\n:\n...(query body)...\n\n   },\n   \nSketch\n:{  \n      \nWas Estimated\n:false,\n      \nUniques Estimate\n:104.0,\n      \nFamily\n:\nTUPLE\n,\n      \nTheta\n:1.0,\n      \nStandard Deviations\n:{  \n         \n1\n:{  \n            \nupperBound\n:104.0,\n            \nlowerBound\n:104.0\n         },\n         \n2\n:{  \n            \nupperBound\n:104.0,\n            \nlowerBound\n:104.0\n         },\n         \n3\n:{  \n            \nupperBound\n:104.0,\n            \nlowerBound\n:104.0\n         }\n      }\n   }\n}\n\n\n\nrecords\n:[  \n   {  \n      \ncountry\n:\nCanada\n,\n      \ncount\n:117,\n      \naverageAge\n:21.82051282051282\n   },\n   {  \n      \ncountry\n:\nAzerbaijan\n,\n      \ncount\n:1,\n      \naverageAge\n:30.0\n   },\n   {  \n      \ncountry\n:\nEngland\n,\n      \ncount\n:13,\n      \naverageAge\n:30.923076923076923\n   },\n   {  \n      \ncountry\n:\nCongo\n,\n      \ncount\n:1,\n      \naverageAge\n:32.0\n   },\n   {  \n      \ncountry\n:\nBangladesh\n,\n      \ncount\n:3,\n      \naverageAge\n:24.333333333333336\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:4,\n      \nEmit Time\n:1529458418030,\n      \nExpected Emit Time\n:1529458418023,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nFinish Time\n:1529458418030,\n      \nID\n:\n448d228a-1eed-471f-8777-c800cc866535\n,\n      \nReceive Time\n:1529458398023,\n      \nBody\n:\n...(query body)...\n\n   },\n   \nSketch\n:{  \n      \nWas Estimated\n:false,\n      \nUniques Estimate\n:108.0,\n      \nFamily\n:\nTUPLE\n,\n      \nTheta\n:1.0,\n      \nStandard Deviations\n:{  \n         \n1\n:{  \n            \nupperBound\n:108.0,\n            \nlowerBound\n:108.0\n         },\n         \n2\n:{  \n            \nupperBound\n:108.0,\n            \nlowerBound\n:108.0\n         },\n         \n3\n:{  \n            \nupperBound\n:108.0,\n            \nlowerBound\n:108.0\n         }\n      }\n   }\n}\n\n\n\n\nWindow - Additive Tumbling\n\n\nBQL Query\n\n\nSELECT COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nGROUP BY ()\nWINDOWING(EVERY, 5000, TIME, ALL)\nLIMIT 50;\n\n\n\n\nJSON Query\n\n\n{\n   \nfilters\n:[\n      {\n         \nfield\n:\ndemographics\n,\n         \noperation\n:\n!=\n,\n         \nvalues\n:[\n            \nnull\n\n         ]\n      }\n   ],\n   \naggregation\n:{\n      \ntype\n:\nGROUP\n,\n      \nattributes\n:{\n         \noperations\n:[\n            {\n               \ntype\n:\nCOUNT\n,\n               \nnewName\n:\ncount\n\n            },\n            {\n               \ntype\n:\nAVG\n,\n               \nfield\n:\ndemographics.age\n,\n               \nnewName\n:\naverageAge\n\n            }\n         ]\n      }\n   },\n   \nwindow\n:{\n       \nemit\n:{\n           \ntype\n: \nTIME\n,\n           \nevery\n: 5000\n       },\n       \ninclude\n:{\n           \ntype\n: \nALL\n\n       }\n   },\n   \nduration\n:20000\n}\n\n\n\n\nThe above query will run for 20 seconds and emit a result every 5 seconds. The result will contain the average age and the count of the records seen since the very beginning of the query. Results might look like this:\n\n\nrecords\n:[  \n   {  \n      \ncount\n:8493,\n      \naverageAge\n:28.8828796983622\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:1,\n      \nEmit Time\n:1529522392188,\n      \nExpected Emit Time\n:1529522392089,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n12e48fbd-a20f-4f5e-8135-0f012d9ba3ef\n,\n      \nReceive Time\n:1529522387089,\n      \nBody\n:\n...(query body)...\n\n   }\n}\n\n\n\nrecords\n:[  \n   {  \n      \ncount\n:17580,\n      \naverageAge\n:29.842629482071715\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:2,\n      \nEmit Time\n:1529522397191,\n      \nExpected Emit Time\n:1529522397089,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n12e48fbd-a20f-4f5e-8135-0f012d9ba3ef\n,\n      \nReceive Time\n:1529522387089,\n      \nBody\n:\n...(query body)...\n\n   }\n}\n\n\n\nrecords\n:[  \n   {  \n      \ncount\n:26317,\n      \naverageAge\n:29.86675792835957\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:3,\n      \nEmit Time\n:1529522402185,\n      \nExpected Emit Time\n:1529522402089,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n12e48fbd-a20f-4f5e-8135-0f012d9ba3ef\n,\n      \nReceive Time\n:1529522387089,\n      \nBody\n:\n...(query body)...\n\n   }\n}\n\n\n\n\nrecords\n:[  \n   {  \n      \ncount\n:35259,\n      \naverageAge\n:29.8303102557552\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:4,\n      \nEmit Time\n:1529522407182,\n      \nExpected Emit Time\n:1529522407089,\n      \nName\n:\nTumbling\n\n   },\n   \nQuery\n:{  \n      \nFinish Time\n:1529522407182,\n      \nID\n:\n12e48fbd-a20f-4f5e-8135-0f012d9ba3ef\n,\n      \nReceive Time\n:1529522387089,\n      \nBody\n:\n...(query body)...\n\n   }\n}\n\n\n\n\nSliding \"Reactive\" Window with Max Duration\n\n\nBQL Query\n\n\nSELECT *\nFROM STREAM(MAX, TIME)\nWHERE bcookie='2siknmdd6kaqm'\nWINDOWING(EVERY, 1, RECORD, FIRST, 1, RECORD)\n\n\n\n\nJSON Query\n\n\n{  \n   \nfilters\n:[  \n      {  \n         \nfield\n:\nbrowser-id\n,\n         \noperation\n:\n==\n,\n         \nvalues\n:[  \n            \n2siknmdd6kaqm\n\n         ]\n      }\n   ],\n   \naggregation\n:{  \n      \nsize\n:1,\n      \ntype\n:\nRAW\n,\n      \nfields\n:null,\n      \nattributes\n:null\n   },\n   \nwindow\n:{  \n      \nemit\n:{  \n         \ntype\n:\nRECORD\n,\n         \nevery\n:1\n      },\n      \ninclude\n:{  \n         \ntype\n:\nRECORD\n,\n         \nfirst\n:1\n      }\n   },\n   \nprojection\n:{  \n      \nfields\n:{  \n         \nbrowser-id\n:\nbrowser-id\n,\n         \nevent\n:\nevent\n,\n         \ndemographics.country\n:\ncountry\n\n      }\n   },\n   \nduration\n:9223372036854775807\n}\n\n\n\n\nThis is a query that will capture raw data, and has a sliding window of size 1. This query will return window results immedietly whenever a single record that matches the filters flows through the system. The filters in this example\nwill only match records from a particular browser.\n\n\nThis query will run for the maxiumum amount of time that the backend is configured to allow.\n\n\nResults might look like this:\n\n\nrecords\n:[  \n   {  \n      \ncountry\n:\nUSA\n,\n      \nevent\n:\npage\n,\n      \nbrowser-id\n:\n2siknmdd6kaqm\n\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:1,\n      \nSize\n:1,\n      \nEmit Time\n:1529521479235,\n      \nName\n:\nSliding\n\n   },\n   \nQuery\n:{  \n      \nID\n:\n31d65a12-ed56-4cc8-81ec-6a8bfe9301ba\n,\n      \nReceive Time\n:1529521475015,\n      \nBody\n:\n...(query body)... \n\n   }\n}\n\n\n\nrecords\n:[  \n   {  \n      \ncountry\n:\nUSA\n,\n      \nevent\n:\nclick\n,\n      \nbrowser-id\n:\n2siknmdd6kaqm\n\n   }\n],\n\nmeta\n:{  \n   \nWindow\n:{  \n      \nNumber\n:6,\n      \nSize\n:1,\n      \nEmit Time\n:1529521764875,\n      \nName\n:\nSliding\n\n   },\n   \nQuery\n:{  \n      \nID\n:\ne9595eb4-ea95-418b-8cff-d00736bf216f\n,\n      \nReceive Time\n:1529521757459,\n      \nBody\n:\n...(query body)...\n\n   }\n}\n\n... (one result returned for each record found for as long as the backend is configured to allow) ...", 
            "title": "Query Examples"
        }, 
        {
            "location": "/ws/examples/#query-examples", 
            "text": "Rather than sourcing the examples from the Quick Start, these examples are real-world ones sourced from Bullet running on raw, user events generated by instrumentation on Yahoo sites by Yahoo employees (not all Yahoo users).   Disclaimer  The actual data shown here has been edited and is not how actual Yahoo user events look.    SQL translation  For each query, we will also rewrite it to a SQL like syntax for readability. Eventually, we plan the API to support this format as input so you could just write this in place of the JSON syntax.", 
            "title": "Query Examples"
        }, 
        {
            "location": "/ws/examples/#simplest-query", 
            "text": "The simplest query you can write would be:  BQL Query  SELECT * FROM STREAM(30000, TIME) LIMIT 1;  JSON Query  {}  While not a very useful query - this will get any one event record (no filters means that any record would be matched, no projection gets the entire record, and the default aggregation is  LIMIT or  RAW  with size 1, default duration 30000 ms), this can be used to quickly test your connection to Bullet.   WINDOW?  There is only one unified data stream in Bullet, so for clarity the  FROM  clause is replaced with a non-existent  WINDOW  function to denote the look-forward time window for the Bullet query. For the  SQL  examples, we pretend that this function understands various time granularities as well.", 
            "title": "Simplest Query"
        }, 
        {
            "location": "/ws/examples/#simple-filtering", 
            "text": "BQL Query  SELECT *\nFROM STREAM(30000, TIME)\nWHERE id = 'btsg8l9b234ha'\nLIMIT 1;  JSON Query  {\n    filters :[\n       {\n            field : id ,\n            operation : == ,\n            values :[\n                btsg8l9b234ha \n           ]\n       }\n    ]\n}  Because of the default constraints, this query would find at most 1 record with the id matching the value provided. The record would have all its fields.  A sample response could be (it has been edited to remove PII and other Yahoo data). The response contains a single matching record, and the associated meta information.  {\n    records :[\n       {\n            server_name : EDITED ,\n            page_uri : / ,\n            is_page_view :true,\n            device : tablet ,\n            debug_codes :{\n                http_status_code : 200 \n           },\n            referrer_domain : www.yahoo.com ,\n            is_logged_in :true,\n            timestamp :1446842189000,\n            event_family : view ,\n            id : btsg8l9b234ha ,\n            os_name : mac os ,\n            demographics :{\n                age  :  25 ,\n                gender  :  m ,\n            }\n       }\n    ],\n     meta :{\n         query_id :1167304238598842449,\n         query_body : {} ,\n         query_finish_time :1480723799550,\n         query_receive_time :1480723799540\n    }\n}", 
            "title": "Simple Filtering"
        }, 
        {
            "location": "/ws/examples/#relational-filters-and-projections", 
            "text": "BQL Query  SELECT timestamp AS ts, device_timestamp AS device_ts,\n       event AS event, page_domain AS domain, page_id AS id\nFROM STREAM(20000, TIME)\nWHERE id = 'btsg8l9b234ha' AND page_id IS NOT NULL\nLIMIT 10;  JSON Query  {\n     filters :[\n        {\n             field : id ,\n             operation : == ,\n             values :[\n                 btsg8l9b234ha \n            ]\n        },\n        {\n             field : page_id ,\n             operation : != ,\n             values :[\n                 null \n            ]\n        }\n    ],\n     projection :{\n         fields :{\n             timestamp : ts ,\n             device_timestamp : device_ts ,\n             event : event ,\n             page_domain : domain ,\n             page_id : id \n        }\n    },\n     aggregation :{\n         type : RAW ,\n         size :10\n    },\n     duration :20000\n}  The above query finds all events with id set to 'btsg8l9b234ha' and page_id is not null, projects out the fields listed above with their new names (timestamp becomes ts etc) and limits the results to at most 10 such records.  RAW  indicates that the complete raw record fields will be returned, and more complicated aggregations such as  COUNT  or  SUM  will not be performed. The duration would set the query to wait at most 20 seconds for records to show up.  The resulting response could look like (only 3 events were generated that matched the criteria):  {\n     records : [\n        {\n             domain :  http://some.url.com ,\n             device_ts : 1481152233788,\n             id : 2273844742998,\n             event :  page ,\n             ts : null\n        },\n        {\n             domain :  www.yahoo.com ,\n             device_ts : 1481152233788,\n             id : 227384472956,\n             event :  click ,\n             ts : 1481152233888\n        },\n        {\n             domain :  https://news.yahoo.com ,\n             device_ts : null,\n             id : 2273844742556,\n             event :  page ,\n             ts : null\n        }\n    ],\n     meta : {\n         query_id : -3239746252817510000,\n         query_body :  EDITED OUT ,\n         query_finish_time : 1481152233799,\n         query_receive_time : 1481152233796\n    }\n}", 
            "title": "Relational Filters and Projections"
        }, 
        {
            "location": "/ws/examples/#logical-filters-and-projections", 
            "text": "BQL Query  SELECT id AS id, experience AS experience, page_id AS pid,\n       link_id AS lid, tags AS tags, demographics.age AS age\nFROM STREAM(60000, TIME)\nWHERE (id = 'c14plm1begla7' AND ((experience = 'web' AND page_id IN ('18025', '47729'))\n                                  OR link_id LIKE ('2.*')))\n      OR (tags.player='true' AND demographics.age   '65')\nLIMIT 1;  JSON Query  {\n  filters : [\n                {\n                     operation :  OR ,\n                     clauses : [\n                        {\n                             operation :  AND ,\n                             clauses : [\n                                {\n                                     field :  id ,\n                                     operation :  == ,\n                                     values : [ c14plm1begla7 ]\n                                },\n                                {\n                                     operation :  OR ,\n                                     clauses : [\n                                        {\n                                             operation :  AND ,\n                                             clauses : [\n                                                {\n                                                     field :  experience ,\n                                                     operation :  == ,\n                                                     values : [ web ]\n                                                },\n                                                {\n                                                     field :  page_id ,\n                                                     operation :  == ,\n                                                     values : [ 18025 ,  47729 ]\n                                                }\n                                            ]\n                                        },\n                                        {\n                                             field :  link_id ,\n                                             operation :  RLIKE ,\n                                             values : [ 2.* ]\n                                        }\n                                    ]\n                                }\n                            ]\n                        },\n                        {\n                             operation :  AND ,\n                             clauses : [\n                                {\n                                     field :  tags.player ,\n                                     operation :  == ,\n                                     values : [ true ]\n                                },\n                                {\n                                     field :  demographics.age ,\n                                     operation :  ,\n                                     values : [ 65 ]\n                                }\n                            ]\n                        }\n                    ]\n                }\n            ],\n  projection  : {\n     fields : {\n         id :  id ,\n         experience :  experience ,\n         page_id :  pid ,\n         link_id :  lid ,\n         tags :  tags ,\n         demographics.age :  age \n    }\n },\n  aggregation : { type  :  RAW ,  size  : 1},\n  duration : 60000\n}   Typing  If demographics[\"age\"] was of type Long, then Bullet will convert 85 to be an Long, but in this example, we are pretending that it is String.  So, no conversion is made. Similarly for link_id, id, experience and page_id. tags is a Map of String to Boolean so Bullet converts  \"true\"  to the Boolean  true .   This query is looking for a single event with a specific id and either the page_id is in two specific pages on the \"web\" experience or with a link_id that starts with 2, or a player event where the age is greater than \"65\". In other words, it is looking for senior citizens who generate video player events or a particular person's (based on id) events on two specific pages or a group of pages that have link that have ids that start with 2. It then projects out only these fields with different names.  A sample result could look like (it matched because of tags.player was true and demographics.age was   65):  {\n     records : [\n        {\n             pid : 158 ,\n             id : 0qcgofdbfqs9s ,\n             experience : web ,\n             lid : 978500434 ,\n             age : 66 ,\n             tags :{ player :true}\n        }\n    ],\n     meta : {\n         query_id : 3239746252812284004,\n         query_body :  EDITED OUT ,\n         query_finish_time : 1481152233805,\n         query_receive_time : 1481152233881\n    }\n}", 
            "title": "Logical Filters and Projections"
        }, 
        {
            "location": "/ws/examples/#group-all-count-aggregation", 
            "text": "An example of a query performing a COUNT all records aggregation would look like:  BQL Query  SELECT COUNT(*) AS numSeniors\nFROM STREAM(20000, TIME)\nWHERE demographics.age   65\nGROUP BY ();  Note that the  GROUP BY ()  is optional.  JSON Query  {\n    filters :[\n      {\n          field : demographics.age ,\n          operation : ,\n          values :[\n             65 \n         ]\n      }\n   ],\n    aggregation :{\n       type : GROUP ,\n       attributes :{\n          operations :[\n            {\n                type : COUNT ,\n                newName : numSeniors \n            }\n         ]\n      }\n   },\n    duration :20000\n}  This query will count the number events for which demographics.age   65. The aggregation type GROUP indicates that it is a group aggregation. To group by a key, the  fields  key needs to be set in the  aggregation  part of the query. If  fields  is empty or is omitted (as it is in the query above) and the  type  is  GROUP , it is as if all the records are collapsed into a single group - a  GROUP ALL . Adding a  COUNT  in the  operations  part of the  attributes  indicates that the number of records in this group will be counted, and the \"newName\" key denotes the name the resulting column \"numSeniors\" in the result. Setting the duration to 20000 counts matching records for\nthis duration.  A sample result would look like:  {\n     records : [\n        {\n             numSeniors : 363201\n        }\n    ],\n     meta : {}\n}  This result indicates that 363,201 records were counted with demographics.age   65 during the 20s the query was running.", 
            "title": "GROUP ALL COUNT Aggregation"
        }, 
        {
            "location": "/ws/examples/#group-all-multiple-aggregations", 
            "text": "COUNT is the only GROUP operation for which you can omit a \"field\".  BQL Query  SELECT COUNT(*) AS numCalifornians, AVG(demographics.age) AS avgAge,\n       MIN(demographics.age) AS minAge, MAX(demographics.age) AS maxAge\nFROM STREAM(20000, TIME)\nWHERE demographics.state = 'california'\nGROUP BY ();  Note that the  GROUP BY ()  is optional.  JSON Query  {\n    filters :[\n      {\n          field : demographics.state ,\n          operation : == ,\n          values :[\n             california \n         ]\n      }\n   ],\n    aggregation :{\n       type : GROUP ,\n       attributes :{\n          operations :[\n            {\n                type : COUNT ,\n                newName : numCalifornians \n            },\n            {\n                type : AVG ,\n                field : demographics.age ,\n                newName : avgAge \n            },\n            {\n                type : MIN ,\n                field : demographics.age ,\n                newName : minAge \n            },\n            {\n                type : MAX ,\n                field : demographics.age ,\n                newName : maxAge \n            }\n         ]\n      }\n   },\n    duration :20000\n}  A sample result would look like:  {\n     records : [\n        {\n             maxAge : 94.0,\n             numCalifornians : 188451,\n             minAge : 6.0,\n             avgAge : 33.71828\n        }\n    ],\n     meta : {\n         query_id : 8051040987827161000,\n         query_body :  EDITED OUT ,\n         query_finish_time : 1482371927435,\n         query_receive_time : 1482371916625\n    }\n}  This result indicates that, among the records observed during the 20s this query ran, there were 188,451 users with demographics.state equal to \"california\". Among these users the average age was 33.71828, the max age observed was 94, and the minimum age observed was 6.", 
            "title": "GROUP ALL Multiple Aggregations"
        }, 
        {
            "location": "/ws/examples/#exact-count-distinct-aggregation", 
            "text": "BQL Query  SELECT COUNT(DISTINCT browser_name, browser_version) AS  COUNT DISTINCT \nFROM STREAM(10000, TIME);  JSON Query  {\n    aggregation :{\n       type : COUNT DISTINCT ,\n       fields :{\n          browser_name : ,\n          browser_version : \n      }\n   }\n}  This gets the count of the unique browser names and versions in the next 30s (default duration). Note that we do not specify values for the keys in fields. This is because they are not relevant  {\n     records : [\n        {\n             COUNT DISTINCT : 158.0\n        }\n    ],\n     meta : {\n         query_id : 4451146261377394443,\n         sketches : {\n             standard_deviations : {\n                 1 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                },\n                 2 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                },\n                 3 : {\n                     upperBound : 158.0,\n                     lowerBound : 158.0\n                }\n            },\n             was_estimated : false,\n             family :  THETA ,\n             theta : 1.0,\n             size : 1280\n        },\n         query_body :  EDITED OUT ,\n         query_finish_time : 1484084869073,\n         query_receive_time : 1484084832684\n    }\n}  There were 158 unique combinations on browser names and versions in our dataset for those 30 s. Note the new  sketches  object in the meta. It has various metadata about the result. In particular, the  was_estimated  key denotes where the result\nwas estimated or not. The  standard_deviations  key denotes the confidence at various sigmas: 1 (1 sigma = ~68% confidence, 2 sigma = ~95% confidence, 3 sigma = ~99% confidence). Since this result was not estimated, the result is the same as the upper and lower bounds for the result.", 
            "title": "Exact COUNT DISTINCT Aggregation"
        }, 
        {
            "location": "/ws/examples/#approximate-count-distinct", 
            "text": "BQL Query  SELECT COUNT(DISTINCT ip_address) AS uniqueIPs\nFROM STREAM(10000, TIME);  JSON Query  {\n    aggregation :{\n       type : COUNT DISTINCT ,\n       fields :{\n          ip_address : \n      },\n       attributes :{\n          newName : uniqueIPs \n      }\n   },\n    duration :10000\n}  This query gets us the unique IP addresses in the next 10 s. It renames the result column from \"COUNT DISTINCT\" to \"uniqueIPs\".  {\n    records :[\n      {\n          uniqueIPs :130551.07952805843\n      }\n   ],\n    meta :{\n       query_id :5377782455857451480,\n       sketches :{\n          standard_deviations :{\n             1 :{\n                upperBound :131512.85413760383,\n                lowerBound :129596.30223107953\n            },\n             2 :{\n                upperBound :132477.15103015225,\n                lowerBound :128652.93906100772\n            },\n             3 :{\n                upperBound :133448.49248615955,\n                lowerBound :127716.46773622213\n            }\n         },\n          was_estimated :true,\n          family : THETA ,\n          theta :0.12549877074343688,\n          size :131096\n      },\n       query_body : EDITED OUT ,\n       query_finish_time :1484090240812,\n       query_receive_time :1484090223351\n   }\n}  The number of unique IPs in our dataset was 130551 in those 10s (approximately) with the true value between (129596, 131512) at 68% confidence, (128652, 132477) at 95% confidence and (127716, 133448) at 99% confidence. In the  worst  case at 3 sigma (99% confidence),\nour error is 2.17%. The final result was computed with 131096 bytes or ~128 KiB as denoted by  size . This happens to be maximum size the the COUNT DISTINCT sketch will take up at the default nominal entries, so even if we had billions of unique IPs, the size will be the same and the error may be higher (depends on the distribution). For example, the error when the same query was run for 30s was 2.28% at 99% confidence (actual unique IPs: 559428, upper bound: 572514). In fact, the worst the error can get at this\nSketch size is 2.34% as defined  here ,  regardless of the number of unique entries added to the Sketch! .", 
            "title": "Approximate COUNT DISTINCT"
        }, 
        {
            "location": "/ws/examples/#distinct-aggregation", 
            "text": "BQL Query  SELECT browser_name AS browser\nFROM STREAM(30000, TIME)\nGROUP BY browser_name\nLIMIT 10;  JSON Query  {\n    aggregation :{\n       type : DISTINCT ,\n       size :10,\n       fields :{\n          browser_name : browser \n      }\n   }\n}  This query gets the distinct values for the browser_name field and limit the results to 10. It runs for 30 s.  {\n    records :[\n      {\n          browser : opera \n      },\n      {\n          browser : flock \n      },\n      {\n          browser : links \n      },\n      {\n          browser : mozilla firefox \n      },\n      {\n          browser : dolfin \n      },\n      {\n          browser : lynx \n      },\n      {\n          browser : chrome \n      },\n      {\n          browser : microsoft internet explorer \n      },\n      {\n          browser : aol browser \n      },\n      {\n          browser : edge \n      }\n   ],\n    meta :{\n       query_id :-4872093887360741287,\n       sketches :{\n          standard_deviations :{\n             1 :{\n                upperBound :28.0,\n                lowerBound :28.0\n            },\n             2 :{\n                upperBound :28.0,\n                lowerBound :28.0\n            },\n             3 :{\n                upperBound :28.0,\n                lowerBound :28.0\n            }\n         },\n          was_estimated :false,\n          family : TUPLE ,\n          uniques_estimate :28.0,\n          theta :1.0\n      },\n       query_body : EDITED OUT ,\n       query_finish_time :1485469087971,\n       query_receive_time :1485469054070\n   }\n}  There were 28 unique results but we asked for 10, so the query returned a uniform sample across the 28 distinct values.  DISTINCT is just an alias for GROUP. A GROUP by with no operations is exactly a DISTINCT.", 
            "title": "DISTINCT Aggregation"
        }, 
        {
            "location": "/ws/examples/#group-by-aggregation", 
            "text": "BQL Query  SELECT demographics.country AS country, device AS device,\n       COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nGROUP BY demographics.country, device\nLIMIT 50;  JSON Query  {\n    filters :[\n      {\n          field : demographics ,\n          operation : != ,\n          values :[\n             null \n         ]\n      }\n   ],\n    aggregation :{\n       type : GROUP ,\n       size :50,\n       fields :{\n          demographics.country : country ,\n          device : \n      },\n       attributes :{\n          operations :[\n            {\n                type : COUNT ,\n                newName : count \n            },\n            {\n                type : AVG ,\n                field : demographics.age ,\n                newName : averageAge \n            },\n            {\n                type : AVG ,\n                field : timespent ,\n                newName : averageTimespent \n            }\n         ]\n      }\n   },\n    duration :20000\n}  This query groups by the country and the device and for each unique group gets the count, average age and time spent by the users for the next 20 seconds. It renames demographics.country to country and does not rename device. It limits the groups to 50. If there were more than\n50 groups, the results would be a uniform sampling of the groups (but each group in the result would have the correct result). These parameters can all be tweaked  in the configuration .  {\n    records :[\n      {\n          country : uk ,\n          device : desktop ,\n          count :203034,\n          averageAge :32.42523,\n          averageTimespent :1.342\n      },\n      {\n          country : us ,\n          device : desktop ,\n          count :1934030,\n          averageAge :29.42523,\n          averageTimespent :3.234520\n      },\n       ...EDITED 41 other such records out for readability... ,\n   ],\n    meta :{\n       query_id :1705911449584057747,\n       sketches :{\n          standard_deviations :{\n             1 :{\n                upperBound :43.0,\n                lowerBound :43.0\n            },\n             2 :{\n                upperBound :43.0,\n                lowerBound :43.0\n            },\n             3 :{\n                upperBound :43.0,\n                lowerBound :43.0\n            }\n         },\n          was_estimated :false,\n          family : TUPLE ,\n          uniques_estimate :43.0,\n          theta :1.0\n      },\n       query_body : EDITED OUT ,\n       query_finish_time :1485217172780,\n       query_receive_time :1485217148840\n   }\n}  We received 43 rows for this result. The maximum groups that was allowed for the instance of Bullet was 512. If there were more groups than the maximum specified by your configuration,  a uniform sample  across them would be chosen\nfor the result. However, for each group, the values computed (average, count) would be exact. The standard deviations, whether the result was estimated and the number of approximate uniques in the metadata would reflect the change.  If you asked for 50 rows in the aggregation (as the query did above) but there were more than 50 in the result (but   512), the metadata would reflect the fact that the result was not estimated. You would still get a uniform sample\nbut by increasing your aggregation size higher, you could get the rest.  For readability, if you were just trying to get the unique values for a field or a set of fields, you could leave out the attributes section and specify your fields section. You could also call the type  DISTINCT  instead of GROUP  to make that explicit.  DISTINCT  is just an alias for  GROUP . See  the DISTINCT example .", 
            "title": "GROUP by Aggregation"
        }, 
        {
            "location": "/ws/examples/#quantile-distribution", 
            "text": "BQL Query  SELECT QUANTILE(duration, LINEAR, 11)\nFROM STREAM(5000, TIME)\nLIMIT 11;  JSON Query  {\n    aggregation :{\n       type :  DISTRIBUTION ,\n       size : 11,\n       fields : {\n           duration :  \n      },\n       attributes : {\n           type :  QUANTILE ,\n           numberOfPoints : 11\n      }\n   },\n    duration : 5000\n}  This query creates 11 points from 0 to 1 (both inclusive) and finds the percentile values of the  duration  field (which contains an amount of time in ms) at  0, 0.1, 0.2 ... 1.0  or the 0th, 10th, 20th and 100th percentiles. It runs for 5 seconds and returns at most 11 points. As long as the  size  is set to higher than the number of points you generate,  DISTRIBUTION  queries will return all your values.  The SQL is not really the same since it will produce one row instead of 11.  {\n    records :[\n      {\n          Value :1,\n          Quantile :0\n      },\n      {\n          Value :1352,\n          Quantile :0.1\n      },\n      {\n          Value :3045,\n          Quantile :0.2\n      },\n      {\n          Value :6501,\n          Quantile :0.30000000000000004\n      },\n      {\n          Value :10700,\n          Quantile :0.4\n      },\n      {\n          Value :17488,\n          Quantile :0.5\n      },\n      {\n          Value :28659,\n          Quantile :0.6\n      },\n      {\n          Value :47929,\n          Quantile :0.7\n      },\n      {\n          Value :83447,\n          Quantile :0.7999999999999999\n      },\n      {\n          Value :177548,\n          Quantile :0.8999999999999999\n      },\n      {\n          Value :83525609,\n          Quantile :1\n      }\n   ],\n    meta :{\n       query_finish_time :1493748546533,\n       query_body :  EDITED OUT ,\n       query_id :2981902209347343400,\n       sketches :{\n          normalized_rank_error :0.002389303789572841,\n          size :16416,\n          minimum_value :1,\n          items_seen :1414,\n          maximum_value :83525609,\n          family : QUANTILES ,\n          was_estimated :false\n      },\n       query_receive_time :1493748538259\n   }\n}  The result shows the values at the 0th, 10th percentiles etc. The  was_estimated  key indicates that the result was not approximated. Note the the  minimum_value  and the  maximum_value  correspond to the 0th and 100th percentiles. There is also a  normalized_rank_error  that describes the error (see  below  for a detailed explanation) This is constant for all  DISTRIBUTION  queries and does not depend on the data or the query.", 
            "title": "QUANTILE DISTRIBUTION"
        }, 
        {
            "location": "/ws/examples/#normalized-rank-error", 
            "text": "Unlike  GROUP  and  COUNT DISTINCT , the order in which the data arrives to Bullet can affect the results of a  DISTRIBUTION  query. The error when the result is estimated is not a Gaussian error function\nand is not described in terms of the values of your field. In other words, if the 50th percentile was estimated to some value, you could not bound the true median by using the the estimated\nvalue +/- constant (see below for a good approximation). The error is expressed in terms of the  normalized rank . If one were to sort the true data stream, you would obtain a rank for each item from\n0 to the stream length ( items_seen  in the metadata above). If you then divided each rank by length, you would get ranks from 0 to 1. In this domain, a  normalized rank error  of 0.002, for example, means that a value\nreturned for the 0.50 or 50th percentile could actually lie between 0.498 and 0.502 in the normalized ranks with 99% confidence.  Distribution Accuracy  lists the normalized rank error as a percentage for the maximum size of the Sketch used. If you obtain successive quantile values at\ngranularities lower than this rank error, the results may not be accurate. While the sketch speaks of the normalized rank error, you can still obtain reasonable bounds for values. For example, if the normalized rank\nerror was 1% and you obtained quantile values at 0.48, 0.50 and 0.52, you could use the values at 0.52 and 0.48 as very reasonable upper and lower bounds on your true median (you might even be able to use 0.49 and 0.51\nif the error was 1%).", 
            "title": "Normalized Rank Error"
        }, 
        {
            "location": "/ws/examples/#pmf-distribution-aggregation", 
            "text": "BQL Query  SELECT FREQ(duration, REGION, 2000, 20000, 500)\nFROM STREAM(5000, TIME)\nLIMIT 100;  JSON Query  {\n    aggregation :{\n       type : DISTRIBUTION ,\n       size :100,\n       fields :{\n          duration : \n      },\n       attributes :{\n          type : PMF ,\n          start :2000,\n          end :20000,\n          increment :500\n      }\n   },\n    duration :5000\n}  This query creates 37 points from 2000 to 20000 in 500 increments to bucketize the duration field using these points as split locations and finds the count of duration values that fall into these intervals. It runs for 5s and returns at most 100 records (this means it will return the 38 records).  The SQL does not include the  (-  to 2000)  and the  [20000 to + )  intervals and does not produce a probability.  {\n    records :[\n      {\n          Probability :0.1518054532056006,\n          Count :206,\n          Range : (-\u221e to 2000.0) \n      },\n      {\n          Probability :0.0397936624907885,\n          Count :53.99999999999999,\n          Range : [2000.0 to 2500.0) \n      },\n       ...EDITED 34 other such records out for readability... ,\n      {\n          Probability :0.0058953574060427415,\n          Count :8,\n          Range : [19500.0 to 20000.0) \n      },\n      {\n          Probability :0.45689019896831246,\n          Count :620,\n          Range : [20000.0 to +\u221e) \n      }\n   ],\n    meta :{\n       query_finish_time :1493750074795,\n       query_body :  EDITED OUT ,\n       query_id :-2590566941995678000,\n       sketches :{\n          normalized_rank_error :0.002389303789572841,\n          size :16416,\n          minimum_value :1,\n          items_seen :1357,\n          maximum_value :78240570,\n          family : QUANTILES ,\n          was_estimated :false\n      },\n       query_receive_time :1493750066022\n   }\n}  The result consists of 38 records, each denoting an interval in the domain we asked for. The result was not estimated. Note that the interval is denoted by the  Range  key and the count by the  Count  key. There is also a probability that estimates how likely a value for duration is likely to fall into that range.", 
            "title": "PMF DISTRIBUTION Aggregation"
        }, 
        {
            "location": "/ws/examples/#cdf-distribution-aggregation", 
            "text": "BQL Query  SELECT CUMFREQ(duration, MANUAL, 20000, 2000, 15000, 45000)\nFROM STREAM(5000, TIME)\nLIMIT 100;  JSON Query  {\n    aggregation :{\n       type :  DISTRIBUTION ,\n       size : 100,\n       fields : {\n           duration :  \n      },\n       attributes : {\n           type :  CDF ,\n           points : [20000, 2000, 15000, 45000]\n      }\n   },\n    duration : 5000\n}  This query specifies a list of points manually using  points  property in  attributes . It runs for 5s and finds the\ncumulative frequency distribution using the specified points as break points. It returns at most 100 records (which means we will\nget all of the intervals).  There is no easy SQL equivalent because the points are free-form. It does not produce a probability field like Bullet does.  {\n    records :[\n      {\n          Probability :0.14382632293080055,\n          Count :212.00000000000003,\n          Range : (-\u221e to 2000.0) \n      },\n      {\n          Probability :0.5210312075983717,\n          Count :767.9999999999999,\n          Range : (-\u221e to 15000.0) \n      },\n      {\n          Probability :0.5603799185888738,\n          Count :826,\n          Range : (-\u221e to 20000.0) \n      },\n      {\n          Probability :0.6994572591587517,\n          Count :1031,\n          Range : (-\u221e to 45000.0) \n      },\n      {\n          Probability :1,\n          Count :1474,\n          Range : (-\u221e to +\u221e) \n      }\n   ],\n    meta :{\n       query_finish_time :1493755151660,\n       query_body :  EDITED OUT ,\n       query_id :-8460702488693518000,\n       sketches :{\n          normalized_rank_error :0.002389303789572841,\n          size :16416,\n          minimum_value :2,\n          items_seen :1474,\n          maximum_value :10851113,\n          family : QUANTILES ,\n          was_estimated :false\n      },\n       query_receive_time :1493755143626\n   }\n}  The result contains the 5 intervals produced by the split points. It was not estimated so these counts are exact. Note that the start of each interval is  -  because\nit is the cumulative frequency distribution.", 
            "title": "CDF DISTRIBUTION Aggregation"
        }, 
        {
            "location": "/ws/examples/#exact-top-k-aggregation", 
            "text": "BQL Query  There are two methods for executing a TOP K aggregation in BQL:  SELECT TOP(500, 100, demographics.country, browser_name) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE demographics.country IS NOT NULL AND browser_name IS NOT NULL;  OR:  SELECT demographics.country, browser_name, COUNT(*) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE demographics.country IS NOT NULL AND browser_name IS NOT NULL\nGROUP BY demographics.country, browser_name\nHAVING COUNT(*)  = 100\nORDER BY COUNT(*) DESC\nLIMIT 500;  JSON Query  {\n    filters :[\n      {\n          field :  demographics.country ,\n          operation :  != ,\n          values : [ null ]\n      },\n      {\n          field :  browser_name ,\n          operation :  != ,\n          values : [ null ]\n      }\n   ],\n    aggregation :{\n       type :  TOP K ,\n       size : 500,\n       fields : {\n           browser_name :  browser ,\n           demographics.country :  country \n      },\n       attributes : {\n           threshold : 100,\n           newName :  numEvents \n      }\n   },\n    duration : 10000\n}  This query gets the top 500 country, browser combinations where the count of records for each combination is at least 100. It runs for 10s.  {\n    records :[\n      {\n          country : us ,\n          browser : google chrome ,\n          numEvents :2729\n      },\n      {\n          country : us ,\n          browser : mozilla firefox ,\n          numEvents :1072\n      },\n      {\n          country : uk ,\n          browser : google chrome ,\n          numEvents :703\n      },\n      {\n          country : fr ,\n          browser : google chrome ,\n          numEvents :383\n      },\n      {\n          country : fr ,\n          browser : mozilla firefox ,\n          numEvents :278\n      },\n      {\n          country : es ,\n          browser : google chrome ,\n          numEvents :234\n      },\n       ...EDITED 10 other such records here for readability ,\n      {\n          country : es ,\n          browser : mozilla firefox ,\n          numEvents :102\n      },\n      {\n          country : fr ,\n          browser : apple safari ,\n          numEvents :101\n      }\n   ],\n    meta :{\n       query_finish_time :1493760034414,\n       query_body :  EDITED OUT ,\n       query_id :7515243052399540000,\n       sketches :{\n          maximum_count_error :0,\n          active_items :431,\n          items_seen :10784,\n          family : FREQUENCY ,\n          was_estimated :false\n      },\n       query_receive_time :1493760020807\n   }\n}  The results gave us the top 18 country, browser combinations that had counts over a 100. Note the  maximum_count_error  key in the metadata. This represents how off the count is. It is 0 because these counts are exact.\nIn our data stream, we only had 18 unique combinations of countries and browser names at the time the query was run.", 
            "title": "Exact TOP K Aggregation"
        }, 
        {
            "location": "/ws/examples/#approximate-top-k-aggregation", 
            "text": "BQL Query  There are two methods for executing a TOP K aggregation in BQL:  SELECT TOP(10, 100, browser_name, browser_version, os_name, os_version, demographics.country, demographics.state) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE os_name IS NOT NULL AND browser_name IS NOT NULL;  OR:  SELECT browser_name, browser_version, os_name, os_version, demographics.country, demographics.state, COUNT(*) AS numEvents\nFROM STREAM(10000, TIME)\nWHERE os_name IS NOT NULL AND browser_name IS NOT NULL\nGROUP BY browser_name, browser_version, os_name, os_version, demographics.country, demographics.state\nHAVING COUNT(*)  = 100\nORDER BY COUNT(*) DESC\nLIMIT 10;  JSON Query  {\n    filters :[\n      {\n          field :  browser_name ,\n          operation :  != ,\n          values : [ null ]\n      },\n      {\n          field :  os_name ,\n          operation :  != ,\n          values : [ null ]\n      }\n   ],\n    aggregation :{\n       type :  TOP K ,\n       size : 10,\n       fields : {\n           browser_name :  browser ,\n           browser_version :  bversion ,\n           os_name :  os ,\n           os_version :  oversion ,\n           demographics.country :  country ,\n           demographics.state :  state \n      },\n       attributes : {\n           threshold : 100,\n           newName :  numEvents \n      }\n   },\n    duration : 30000\n}  In order to make the result approximate, this query adds more dimensions to the  Exact TOP K  query. It runs for 30s and looks for the top  10  combinations for these events.  {\n    records :[\n      {\n          country : null ,\n          os : mac os x ,\n          browser : google chrome ,\n          numEvents :120823,\n          state : null ,\n          bversion : 56 ,\n          oversion : 10.12 \n      },\n      {\n          country : null ,\n          os : mac os x ,\n          browser : google chrome ,\n          numEvents :4539,\n          state : null ,\n          bversion : 35 ,\n          oversion : 10.9 \n      },\n      {\n          country : us ,\n          os : mac os x ,\n          browser : google chrome ,\n          numEvents :3827,\n          state : null ,\n          bversion : 57 ,\n          oversion : 10.12 \n      },\n      {\n          country : null ,\n          os : ios ,\n          browser : apple safari ,\n          numEvents :3426,\n          state : null ,\n          bversion : 9.0 ,\n          oversion : 9.1 \n      },\n      {\n          country : null ,\n          os : windows nt ,\n          browser : microsoft internet explorer ,\n          numEvents :2264,\n          state : null ,\n          bversion : 6.0 ,\n          oversion : 5.1 \n      },\n      {\n          country : us ,\n          os : mac os x ,\n          browser : google chrome ,\n          numEvents :1995,\n          state : null ,\n          bversion : 58 ,\n          oversion : 10.12 \n      },\n      {\n          country : null ,\n          os : windows nt ,\n          browser : google chrome ,\n          numEvents :1416,\n          state : null ,\n          bversion : 57 ,\n          oversion : 10.0 \n      },\n      {\n          country : null ,\n          os : windows nt ,\n          browser : google chrome ,\n          numEvents :1327,\n          state : null ,\n          bversion : 58 ,\n          oversion : 10.0 \n      },\n      {\n          country : null ,\n          os : mac os x ,\n          browser : google chrome ,\n          numEvents :1187,\n          state : null ,\n          bversion : 57 ,\n          oversion : 10.12 \n      },\n      {\n          country : null ,\n          os : ios ,\n          browser : apple safari ,\n          numEvents :1119,\n          state : null ,\n          bversion : 4.0 ,\n          oversion : 3.0 \n      }\n   ],\n    meta :{\n       query_finish_time :1493761419611,\n       query_body :  EDITED OUT ,\n       query_id :-8797534873217479000,\n       sketches :{\n          maximum_count_error :24,\n          active_items :746,\n          items_seen :187075,\n          family : FREQUENCY ,\n          was_estimated :true\n      },\n       query_receive_time :1493761386294\n   }\n}  Like  DISTRIBUTION , the distribution of the data matters for  TOP K . Depending on the distribution, your results could produce different counts and errors bounds if approximate.  Since we only filtered for nulls in a couple of fields, the top results end up being fields with null values. Note that the  maximum_count_error  is now 24 and the  was_estimated  property is\nset to true. 24 means that the upper bound - the lower bound for the  Count  field for each combination could be off by at most 24. Since Bullet gives you the upper bound, this means that if you\nsubtract 24 from it, you get the lower bound of the true count.  Note that this also means the order of the items could be off. If two items had  Count  within 24 of each other, it is possible that the higher one  may  actually have had a true count  lower  than\nthe second one and possibly be ranked higher. There is no such situation in this result set.", 
            "title": "Approximate TOP K Aggregation"
        }, 
        {
            "location": "/ws/examples/#window-tumbling-group-by", 
            "text": "BQL Query  SELECT demographics.country AS country, COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nGROUP BY demographics.country\nWINDOWING(TUMBLING, 5000, TIME)\nLIMIT 50;  JSON Query  {\n    filters :[\n      {\n          field : demographics ,\n          operation : != ,\n          values :[\n             null \n         ]\n      }\n   ],\n    aggregation :{\n       type : GROUP ,\n       size :5,\n       fields :{\n          demographics.country : country \n      },\n       attributes :{\n          operations :[\n            {\n                type : COUNT ,\n                newName : count \n            },\n            {\n                type : AVG ,\n                field : demographics.age ,\n                newName : averageAge \n            }\n         ]\n      }\n   },\n    window :{\n        emit :{\n            type :  TIME ,\n            every : 5000\n       },\n        include :{\n            type :  TIME ,\n            first : 5000\n       }\n   },\n    duration :20000\n}  This query specifies a tumbling window that will emit every 5 seconds and contain 5 seconds of data per window. Results will come back to the user every 5 seconds, and since the duration of the query is 20 seconds,\nthe user will receive a total of 4 results. Since the aggregation size is set to 5, each returned window will contain only 5 groups (which will be chosen randomly). The result might look like this:  records :[\n    {\n         country : Germany ,\n         count :1,\n         averageAge :25.0\n    },\n    {\n         country : Canada ,\n         count :106,\n         averageAge :22.58490566037736\n    },\n    {\n         country : USA ,\n         count :1,\n         averageAge :28.0\n    },\n    {\n         country : England ,\n         count :8,\n         averageAge :34.25\n    },\n    {\n         country : Peru ,\n         count :9,\n         averageAge :30.0\n    }\n], meta :{\n     Window :{\n         Number :1,\n         Emit Time :1529458403038,\n         Expected Emit Time :1529458403023,\n         Name : Tumbling \n        },\n     Query :{\n         ID : 448d228a-1eed-471f-8777-c800cc866535 ,\n         Receive Time :1529458398023,\n         Body : ...(query body)...},\n         Sketch :{\n             Was Estimated :false,\n             Uniques Estimate :100.0,\n             Family : TUPLE ,\n             Theta :1.0,\n             Standard Deviations :{\n                 1 :{\n                     upperBound :100.0,\n                     lowerBound :100.0\n                },\n                 2 :{\n                     upperBound :100.0,\n                     lowerBound :100.0\n                },\n                 3 :{\n                     upperBound :100.0,\n                     lowerBound :100.0\n                }\n            }\n        }\n    }\n} records :[  \n   {  \n       country : Canada ,\n       count :101,\n       averageAge :32.742574257425744\n   },\n   {  \n       country : ht ,\n       count :2,\n       averageAge :32.0\n   },\n   {  \n       country : England ,\n       count :16,\n       averageAge :27.0625\n   },\n   {  \n       country : Peru ,\n       count :8,\n       averageAge :23.625\n   },\n   {  \n       country : Bangladesh ,\n       count :3,\n       averageAge :27.66666666666667\n   }\n], meta :{  \n    Window :{  \n       Number :2,\n       Emit Time :1529458408036,\n       Expected Emit Time :1529458408023,\n       Name : Tumbling \n   },\n    Query :{  \n       ID : 448d228a-1eed-471f-8777-c800cc866535 ,\n       Receive Time :1529458398023,\n       Body : ...(query body)... \n   },\n    Sketch :{  \n       Was Estimated :false,\n       Uniques Estimate :98.0,\n       Family : TUPLE ,\n       Theta :1.0,\n       Standard Deviations :{  \n          1 :{  \n             upperBound :98.0,\n             lowerBound :98.0\n         },\n          2 :{  \n             upperBound :98.0,\n             lowerBound :98.0\n         },\n          3 :{  \n             upperBound :98.0,\n             lowerBound :98.0\n         }\n      }\n   }\n} records :[  \n   {  \n       country : Canada ,\n       count :121,\n       averageAge :27.97520661157025\n   },\n   {  \n       country : Haiti ,\n       count :3,\n       averageAge :39.0\n   },\n   {  \n       country : Cabuyao laguna ,\n       count :2,\n       averageAge :28.0\n   },\n   {  \n       country : USA ,\n       count :1,\n       averageAge :20.0\n   },\n   {  \n       country : England ,\n       count :23,\n       averageAge :40.869565217391305\n   }\n], meta :{  \n    Window :{  \n       Number :3,\n       Emit Time :1529458413031,\n       Expected Emit Time :1529458413023,\n       Name : Tumbling \n   },\n    Query :{  \n       ID : 448d228a-1eed-471f-8777-c800cc866535 ,\n       Receive Time :1529458398023,\n       Body : ...(query body)... \n   },\n    Sketch :{  \n       Was Estimated :false,\n       Uniques Estimate :104.0,\n       Family : TUPLE ,\n       Theta :1.0,\n       Standard Deviations :{  \n          1 :{  \n             upperBound :104.0,\n             lowerBound :104.0\n         },\n          2 :{  \n             upperBound :104.0,\n             lowerBound :104.0\n         },\n          3 :{  \n             upperBound :104.0,\n             lowerBound :104.0\n         }\n      }\n   }\n} records :[  \n   {  \n       country : Canada ,\n       count :117,\n       averageAge :21.82051282051282\n   },\n   {  \n       country : Azerbaijan ,\n       count :1,\n       averageAge :30.0\n   },\n   {  \n       country : England ,\n       count :13,\n       averageAge :30.923076923076923\n   },\n   {  \n       country : Congo ,\n       count :1,\n       averageAge :32.0\n   },\n   {  \n       country : Bangladesh ,\n       count :3,\n       averageAge :24.333333333333336\n   }\n], meta :{  \n    Window :{  \n       Number :4,\n       Emit Time :1529458418030,\n       Expected Emit Time :1529458418023,\n       Name : Tumbling \n   },\n    Query :{  \n       Finish Time :1529458418030,\n       ID : 448d228a-1eed-471f-8777-c800cc866535 ,\n       Receive Time :1529458398023,\n       Body : ...(query body)... \n   },\n    Sketch :{  \n       Was Estimated :false,\n       Uniques Estimate :108.0,\n       Family : TUPLE ,\n       Theta :1.0,\n       Standard Deviations :{  \n          1 :{  \n             upperBound :108.0,\n             lowerBound :108.0\n         },\n          2 :{  \n             upperBound :108.0,\n             lowerBound :108.0\n         },\n          3 :{  \n             upperBound :108.0,\n             lowerBound :108.0\n         }\n      }\n   }\n}", 
            "title": "Window - Tumbling Group-By"
        }, 
        {
            "location": "/ws/examples/#window-additive-tumbling", 
            "text": "BQL Query  SELECT COUNT(*) AS count, AVG(demographics.age) AS averageAge,\n       AVG(timespent) AS averageTimespent\nFROM STREAM(20000, TIME)\nWHERE demographics IS NOT NULL\nGROUP BY ()\nWINDOWING(EVERY, 5000, TIME, ALL)\nLIMIT 50;  JSON Query  {\n    filters :[\n      {\n          field : demographics ,\n          operation : != ,\n          values :[\n             null \n         ]\n      }\n   ],\n    aggregation :{\n       type : GROUP ,\n       attributes :{\n          operations :[\n            {\n                type : COUNT ,\n                newName : count \n            },\n            {\n                type : AVG ,\n                field : demographics.age ,\n                newName : averageAge \n            }\n         ]\n      }\n   },\n    window :{\n        emit :{\n            type :  TIME ,\n            every : 5000\n       },\n        include :{\n            type :  ALL \n       }\n   },\n    duration :20000\n}  The above query will run for 20 seconds and emit a result every 5 seconds. The result will contain the average age and the count of the records seen since the very beginning of the query. Results might look like this:  records :[  \n   {  \n       count :8493,\n       averageAge :28.8828796983622\n   }\n], meta :{  \n    Window :{  \n       Number :1,\n       Emit Time :1529522392188,\n       Expected Emit Time :1529522392089,\n       Name : Tumbling \n   },\n    Query :{  \n       ID : 12e48fbd-a20f-4f5e-8135-0f012d9ba3ef ,\n       Receive Time :1529522387089,\n       Body : ...(query body)... \n   }\n} records :[  \n   {  \n       count :17580,\n       averageAge :29.842629482071715\n   }\n], meta :{  \n    Window :{  \n       Number :2,\n       Emit Time :1529522397191,\n       Expected Emit Time :1529522397089,\n       Name : Tumbling \n   },\n    Query :{  \n       ID : 12e48fbd-a20f-4f5e-8135-0f012d9ba3ef ,\n       Receive Time :1529522387089,\n       Body : ...(query body)... \n   }\n} records :[  \n   {  \n       count :26317,\n       averageAge :29.86675792835957\n   }\n], meta :{  \n    Window :{  \n       Number :3,\n       Emit Time :1529522402185,\n       Expected Emit Time :1529522402089,\n       Name : Tumbling \n   },\n    Query :{  \n       ID : 12e48fbd-a20f-4f5e-8135-0f012d9ba3ef ,\n       Receive Time :1529522387089,\n       Body : ...(query body)... \n   }\n} records :[  \n   {  \n       count :35259,\n       averageAge :29.8303102557552\n   }\n], meta :{  \n    Window :{  \n       Number :4,\n       Emit Time :1529522407182,\n       Expected Emit Time :1529522407089,\n       Name : Tumbling \n   },\n    Query :{  \n       Finish Time :1529522407182,\n       ID : 12e48fbd-a20f-4f5e-8135-0f012d9ba3ef ,\n       Receive Time :1529522387089,\n       Body : ...(query body)... \n   }\n}", 
            "title": "Window - Additive Tumbling"
        }, 
        {
            "location": "/ws/examples/#sliding-reactive-window-with-max-duration", 
            "text": "BQL Query  SELECT *\nFROM STREAM(MAX, TIME)\nWHERE bcookie='2siknmdd6kaqm'\nWINDOWING(EVERY, 1, RECORD, FIRST, 1, RECORD)  JSON Query  {  \n    filters :[  \n      {  \n          field : browser-id ,\n          operation : == ,\n          values :[  \n             2siknmdd6kaqm \n         ]\n      }\n   ],\n    aggregation :{  \n       size :1,\n       type : RAW ,\n       fields :null,\n       attributes :null\n   },\n    window :{  \n       emit :{  \n          type : RECORD ,\n          every :1\n      },\n       include :{  \n          type : RECORD ,\n          first :1\n      }\n   },\n    projection :{  \n       fields :{  \n          browser-id : browser-id ,\n          event : event ,\n          demographics.country : country \n      }\n   },\n    duration :9223372036854775807\n}  This is a query that will capture raw data, and has a sliding window of size 1. This query will return window results immedietly whenever a single record that matches the filters flows through the system. The filters in this example\nwill only match records from a particular browser.  This query will run for the maxiumum amount of time that the backend is configured to allow.  Results might look like this:  records :[  \n   {  \n       country : USA ,\n       event : page ,\n       browser-id : 2siknmdd6kaqm \n   }\n], meta :{  \n    Window :{  \n       Number :1,\n       Size :1,\n       Emit Time :1529521479235,\n       Name : Sliding \n   },\n    Query :{  \n       ID : 31d65a12-ed56-4cc8-81ec-6a8bfe9301ba ,\n       Receive Time :1529521475015,\n       Body : ...(query body)...  \n   }\n} records :[  \n   {  \n       country : USA ,\n       event : click ,\n       browser-id : 2siknmdd6kaqm \n   }\n], meta :{  \n    Window :{  \n       Number :6,\n       Size :1,\n       Emit Time :1529521764875,\n       Name : Sliding \n   },\n    Query :{  \n       ID : e9595eb4-ea95-418b-8cff-d00736bf216f ,\n       Receive Time :1529521757459,\n       Body : ...(query body)... \n   }\n}\n\n... (one result returned for each record found for as long as the backend is configured to allow) ...", 
            "title": "Sliding \"Reactive\" Window with Max Duration"
        }, 
        {
            "location": "/ui/setup/", 
            "text": "The UI Layer\n\n\nThe Bullet UI lets you easily create and work with Bullet queries and results for your custom data. It stores all created queries, results and other metadata in the local browser storage or \nIndexedDB\n.\n\n\n\n\nReally!? Browser Storage only!?\n\n\nWe're serious about the no persistence thing with Bullet! And while we're at it, we are also not interested in supporting old browsers. Joking aside though, we wanted to keep Bullet as light and simple as possible to start with. We can look into extending support from the server-side by adding a database or the like if needed. In practice, we have found that this isn't as important as it initially seems.\n\n\n\n\nPrerequisites\n\n\nIn order for your UI to work with Bullet, you should have:\n\n\n\n\nAn instance of the \nbackend\n set up\n\n\nAn instance of the \nWeb Service\n set up\n\n\nYou should also have a Web Service serving your schema (either by using the \nfile based serving\n from the Web Service or your own somewhere else)\n\n\n\n\nInstallation\n\n\nWe are considering various packaging options at the moment like Docker etc. In the meantime, the following two options are available:\n\n\nGitHub Releases\n\n\n\n\nHead to the \nReleases page\n and grab the latest release\n\n\nDownload the bullet-ui-vX.X.X.tar.gz archive\n\n\nUnarchive it into your web server where you wish to run the UI.\n\n\nInstall \nNode\n (recommend using \nnvm\n to manage Node versions) on the web server\n\n\n\n\nBuild from source\n\n\n\n\nInstall \nNode\n (recommend using \nnvm\n to manage Node versions).\n\n\nInstall \nYarn\n. Use NPM to install it with \nnpm install -g yarn\n\n\nInstall \nEmber\n. \nsudo npm install -g ember-cli\n (sudo required only if not using nvm)\n\n\ngit clone git@github.com:bullet-db/bullet-ui.git\n\n\ncd bullet-ui\n\n\nyarn\n\n\nember build --environment production\n\n\n\n\nThe entire application with all its assets and dependencies are compiled and placed into dist/. You could point a web server directly at this folder but you will \nonly\n be able to use the default configuration (see \nbelow\n).\n\n\nRunning\n\n\nThere is a Node.js server endpoint defined at \nserver/index.js\n to serve the UI. This dynamically injects the settings (see configuration \nbelow\n) into the served UI based on the environment variable NODE_ENV. You should not need to worry about if you only have one environment.\n\n\nThe entry-point for the UI is the \nExpress\n endpoint defined as the main in package.json that simply adds the server/index.js as a middleware.\n\n\nRegardless of which \ninstallation\n option you chose, you need the following folder structure in order to run the UI:\n\n\ndist/*\nconfig/env-settings.json\nserver/index.js\nexpress-server.js\n\n\n\n\nYou can use node to launch the UI from the top-level of the folder structure above.\n\n\nTo launch the UI with the default settings (without specifying proper API endpoints you will not be able to create or run a query):\n\n\nPORT=8800 node express-server.js\n\n\n\n\nTo launch with custom settings:\n\n\nNODE_ENV=\nyour_property_name_from_env-settings.json\n PORT=8800 node express-server.js\n\n\n\n\nVisit localhost:8800 to see your UI that should be configured with the right settings.\n\n\nConfiguration\n\n\nThe configuration for the UI lets you have different instances of Bullet for different environments (e.g. CI, Staging, Production). These settings can be found in \nenv-settings.json\n.\n\n\n\n\n\n\n\n\nSetting\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nqueryHost\n\n\nThe end point (port included) of your Web Service machine that is talking to the Bullet backend\n\n\n\n\n\n\nqueryNamespace\n\n\nAny qualifiers you have after your host and port on your Web Service running on your \nqueryHost\n\n\n\n\n\n\nqueryPath\n\n\nThe path fragment after the \nqueryNamespace\n on your Web Service running on your \nqueryHost\n for the WebSocket endpoint\n\n\n\n\n\n\nqueryStompRequestChannel\n\n\nThe fragment after this is the Stomp Request channel as configured in your Web Service for the WebSocket endpoint\n\n\n\n\n\n\nqueryStompResponseChannel\n\n\nThe fragment after this is the Stomp Response channel as configured in your Web Service for the WebSocket endpoint\n\n\n\n\n\n\nschemaHost\n\n\nThe end point (port included) of your Web Service machine that is serving your schema in the JSON API format (see \nWeb Service setup\n for details.)\n\n\n\n\n\n\nschemaNamespace\n\n\nThe path fragment on your schema Web Service running on the \nschemaHost\n. There is no \nschemaPath\n because it \nmust\n be \ncolumns\n in order for the UI to be able fetch the column resource (the fields in your schema).\n\n\n\n\n\n\nmodelVersion\n\n\nThis is used an indicator to apply changes to the stored queries, results etc. It is monotonically increasing. On startup, changes specified in \nmigrations\n will be applied if the old modelVersion is not present or is \n than this number\n\n\n\n\n\n\nmigrations\n\n\nis an object that currently supports one key: \ndeletions\n of type string. The value can be set to either \nresult\n or \nquery\n. The former wipes all existing results. The latter wipes everything. See \nmodelVersion\n above.\n\n\n\n\n\n\nhelpLinks\n\n\nIs a list of objects, where each object is a help link. These links populate the \"Help\" drop-down on the UI's top navbar. You can add links to explain your data for example\n\n\n\n\n\n\ndefaultQuery\n\n\nCan either be a \nAPI Query\n or a URL from which one could be fetched dynamically. The UI makes this the query created on every newly created Query. You could use this as a way to have user specific (for example, cookie based) filters created for your users or customize an aggregation when they create a new query in the UI. Note that if you have are accessing a map subfield and your field value in the filter is set as \nfoo.bar\n and you want \nbar\n to be the subfield in the UI query builder, you will need to add a key called \nsubfield\n in the filter (not supported by the API) and set its value to \ntrue\n\n\n\n\n\n\nbugLink\n\n\nIs a URL that by default points to the issues page for the UI GitHub repository. You can change it to point to your own custom JIRA queue or something else\n\n\n\n\n\n\ndefaultValues\n\n\nIs an object that lets you configures defaults for various query parameters and lets you tie your custom backend settings to the UI\n\n\n\n\n\n\n\n\nThese are the properties in the \ndefaultValues\n object. The Validated column denotes if the value is used when validating a query for correctness and the In Help column denotes if the value is displayed in the popover help messages in the UI.\n\n\n\n\n\n\n\n\nDefault Values\n\n\nValidated\n\n\nIn Help\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\naggregationMaxSize\n\n\nYes\n\n\nYes\n\n\nThe size used when doing a Count Distinct, Distinct, Group By, or Distribution query. Set this to your max aggregations size in your backend configuration\n\n\n\n\n\n\nrawMaxSize\n\n\nYes\n\n\nYes\n\n\nThe maximum size for a Raw query. Set this to your max raw aggregation size in your backend configuration\n\n\n\n\n\n\ndurationMaxSecs\n\n\nYes\n\n\nYes\n\n\nThe maximum duration for a query. Set this to the seconds version of milliseconds max duration in your backend configuration\n\n\n\n\n\n\ndistributionNumberOfPoints\n\n\nYes\n\n\nNo\n\n\nThe default value filled in for the Number of Points field for all Distribution aggregations\n\n\n\n\n\n\ndistributionQuantilePoints\n\n\nNo\n\n\nNo\n\n\nThe default value filled in for the Points field for Quantile Distribution aggregations\n\n\n\n\n\n\ndistributionQuantileStart\n\n\nNo\n\n\nNo\n\n\nThe default value filled in for the Start field for Quantile Distribution aggregations\n\n\n\n\n\n\ndistributionQuantileEnd\n\n\nNo\n\n\nNo\n\n\nThe default value filled in for the End field for Quantile Distribution aggregations\n\n\n\n\n\n\ndistributionQuantileIncrement\n\n\nNo\n\n\nNo\n\n\nThe default value filled in for the Increment field for Quantile Distribution aggregations\n\n\n\n\n\n\nwindowEmitFrequencyMinSecs\n\n\nYes\n\n\nNo\n\n\nThe minimum time interval at which a time based window can be returned. Set this to the minimum window emit frequency from your backend configuration\n\n\n\n\n\n\neveryForRecordBasedWindow\n\n\nNo\n\n\nNo\n\n\nThe default value for the number of records in a window for a record based window\n\n\n\n\n\n\neveryForTimeBasedWindow\n\n\nNo\n\n\nNo\n\n\nThe default value for the number of records in a window for a time based window\n\n\n\n\n\n\nsketches.countDistinctMaxEntries\n\n\nNo\n\n\nYes\n\n\nThe maximum entries configured for your Count Distinct sketch in your backend configuration\n\n\n\n\n\n\nsketches.groupByMaxEntries\n\n\nNo\n\n\nYes\n\n\nThe maximum entries configured for your Group sketch in your backend configuration\n\n\n\n\n\n\nsketches.distributionMaxEntries\n\n\nNo\n\n\nYes\n\n\nThe maximum entries configured for your Distribution sketch in your backend configuration\n\n\n\n\n\n\nsketches.distributionMaxNumberOfPoints\n\n\nYes\n\n\nYes\n\n\nThe maximum number of points allowed for Distribution aggregations in your backend configuration\n\n\n\n\n\n\nsketches.topKMaxEntries\n\n\nNo\n\n\nYes\n\n\nThe maximum entries configured for your Top K sketch in your backend configuration\n\n\n\n\n\n\nsketches.topKErrorType\n\n\nNo\n\n\nYes\n\n\nThe ErrorType used for your Top K sketch in your backend configuration. You should set this to the full String rather than \nNFN\n or \nNFP\n\n\n\n\n\n\nmetadataKeyMapping.querySection\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Query Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.windowSection\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Window Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.sketchSection\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Theta Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.theta\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Theta Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.uniquesEstimate\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Uniques Estimate Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.queryCreationTime\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Query Creation Time Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.queryTerminationTime\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Query Termination Time Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.estimatedResult\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Estimated Result Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.standardDeviations\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Standard Deviations Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.normalizedRankError\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Normalized Rank Error Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.maximumCountError\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Maximum Count Error Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.itemsSeen\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Items Seen Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.minimumValue\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Minimum Value Concept in your backend configuration\n\n\n\n\n\n\nmetadataKeyMapping.maximumValue\n\n\nNo\n\n\nYes\n\n\nThe name of the Metadata key for the Maximum Value Concept in your backend configuration\n\n\n\n\n\n\n\n\nYou can specify values for each property above in the \nenv-settings.json\n file. These will be used when running a custom instance of the UI (see \nabove\n).\n\n\nThe \ndefault\n property in the \nenv-settings.json\n that loads default settings for the UI that can be selectively overridden based on which environment you are running on. All settings explained above have default values\nthat are the same as the \ndefault backend settings\n. However, the defaults do not add the \ndefaultQuery\n setting explained above.\n\n\n{\n  \ndefault\n: {\n    \nqueryHost\n: \nhttp://localhost:5555\n,\n    \nqueryNamespace\n: \napi/bullet\n,\n    \nqueryPath\n: \nquery\n,\n    \nschemaHost\n: \nhttp://localhost:5555\n,\n    \nschemaNamespace\n: \napi/bullet\n,\n    \nhelpLinks\n: [\n      {\n        \nname\n: \nTutorials\n,\n        \nlink\n: \nhttps://bullet-db.github.io/ui/usage\n\n      }\n    ],\n    \nbugLink\n: \nhttps://github.com/bullet-db/bullet-ui/issues\n,\n    \nmodelVersion\n: 1,\n    \ndefaultValues\n: {\n      \naggregationMaxSize\n: 512,\n      \nrawMaxSize\n: 100,\n      \ndurationMaxSecs\n: 120,\n      \ndistributionNumberOfPoints\n: 11,\n      \ndistributionQuantilePoints\n: \n0, 0.25, 0.5, 0.75, 0.9, 1\n,\n      \ndistributionQuantileStart\n: 0,\n      \ndistributionQuantileEnd\n: 1,\n      \ndistributionQuantileIncrement\n: 0.1,\n      \nqueryTimeoutSecs\n: 3,\n      \nsketches\n: {\n        \ncountDistinctMaxEntries\n: 16384,\n        \ngroupByMaxEntries\n: 512,\n        \ndistributionMaxEntries\n: 1024,\n        \ndistributionMaxNumberOfPoints\n: 100,\n        \ntopKMaxEntries\n: 1024,\n        \ntopKErrorType\n: \nNo False Negatives\n\n      },\n      \nmetadataKeyMapping\n: {\n        \ntheta\n: \ntheta\n,\n        \nuniquesEstimate\n: \nuniques_estimate\n,\n        \nqueryCreationTime\n: \nquery_receive_time\n,\n        \nqueryTerminationTime\n: \nquery_finish_time\n,\n        \nestimatedResult\n: \nwas_estimated\n,\n        \nstandardDeviations\n: \nstandard_deviations\n,\n        \nnormalizedRankError\n: \nnormalized_rank_error\n,\n        \nmaximumCountError\n: \nmaximum_count_error\n,\n        \nitemsSeen\n: \nitems_seen\n,\n        \nminimumValue\n: \nminimum_value\n,\n        \nmaximumValue\n: \nmaximum_value\n\n      }\n    }\n  }\n}\n\n\n\n\nYou can add more properties for each environment you have the UI running on and \noverride\n the properties in the \ndefault\n object. See \nbelow\n for an example.\n\n\n\n\nCORS\n\n\nAll your Web Service endpoints must support CORS (return the right headers) in order for the UI to be able to communicate with it. The Bullet Web Service already does this for the Query and Schema endpoints.\n\n\n\n\nExample\n\n\nTo cement all this, if you wanted an instance of the UI in your CI environment, you could add another property to the \nenv-settings.json\n file.\n\n\n{\n    \nci\n: {\n        \nqueryHost\n: \nhttp://bullet-ws.dev.domain.com:4080\n,\n        \nschemaHost\n: \nhttp://bullet-ws.dev.domain.com:4080\n,\n        \nhelpLinks\n: [\n          {\n            \nname\n: \nCustom Documentation\n,\n            \nlink\n: \nhttp://data.docs.domain.com\n\n          }\n        ],\n        \ndefaultValues\n : {\n            \ndurationMaxSecs\n: 300,\n            \nsketches\n: {\n                \ncountDistinctMaxEntries\n: 32768,\n                \ndistributionMaxNumberOfPoints\n: 50\n            }\n        },\n        \ndefaultQuery\n: \nhttp://bullet-ws.dev.domain.com:4080/custom-endpoint/api/defaultQuery\n\n    }\n}\n\n\n\n\nYour UI on your CI environment will:\n\n\n\n\nPOST to \nhttp://bullet-ws.dev.domain.com:4080/bullet/api/drpc\n for UI created Bullet queries\n\n\nGET the schema from \nhttp://bullet-ws.dev.domain.com:4080/bullet/api/columns\n\n\nPopulate an additional link on the Help drop-down pointing to \nhttp://data.docs.domain.com\n\n\nAllow queries to run as long as 300 seconds\n\n\nUse 32768 in the help menu for the max number of unique elements that can be counted exactly\n\n\nAllow only 50 points to be generated for Distribution queries\n\n\nGET and cache a defaultQuery from \nhttp://bullet-ws.dev.domain.com:4080/custom-endpoint/api/defaultQuery\n\n\n\n\nYou would make express use these settings by running\n\n\nNODE_ENV=ci PORT=8800 node express-server.js", 
            "title": "Setup"
        }, 
        {
            "location": "/ui/setup/#the-ui-layer", 
            "text": "The Bullet UI lets you easily create and work with Bullet queries and results for your custom data. It stores all created queries, results and other metadata in the local browser storage or  IndexedDB .   Really!? Browser Storage only!?  We're serious about the no persistence thing with Bullet! And while we're at it, we are also not interested in supporting old browsers. Joking aside though, we wanted to keep Bullet as light and simple as possible to start with. We can look into extending support from the server-side by adding a database or the like if needed. In practice, we have found that this isn't as important as it initially seems.", 
            "title": "The UI Layer"
        }, 
        {
            "location": "/ui/setup/#prerequisites", 
            "text": "In order for your UI to work with Bullet, you should have:   An instance of the  backend  set up  An instance of the  Web Service  set up  You should also have a Web Service serving your schema (either by using the  file based serving  from the Web Service or your own somewhere else)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/ui/setup/#installation", 
            "text": "We are considering various packaging options at the moment like Docker etc. In the meantime, the following two options are available:", 
            "title": "Installation"
        }, 
        {
            "location": "/ui/setup/#github-releases", 
            "text": "Head to the  Releases page  and grab the latest release  Download the bullet-ui-vX.X.X.tar.gz archive  Unarchive it into your web server where you wish to run the UI.  Install  Node  (recommend using  nvm  to manage Node versions) on the web server", 
            "title": "GitHub Releases"
        }, 
        {
            "location": "/ui/setup/#build-from-source", 
            "text": "Install  Node  (recommend using  nvm  to manage Node versions).  Install  Yarn . Use NPM to install it with  npm install -g yarn  Install  Ember .  sudo npm install -g ember-cli  (sudo required only if not using nvm)  git clone git@github.com:bullet-db/bullet-ui.git  cd bullet-ui  yarn  ember build --environment production   The entire application with all its assets and dependencies are compiled and placed into dist/. You could point a web server directly at this folder but you will  only  be able to use the default configuration (see  below ).", 
            "title": "Build from source"
        }, 
        {
            "location": "/ui/setup/#running", 
            "text": "There is a Node.js server endpoint defined at  server/index.js  to serve the UI. This dynamically injects the settings (see configuration  below ) into the served UI based on the environment variable NODE_ENV. You should not need to worry about if you only have one environment.  The entry-point for the UI is the  Express  endpoint defined as the main in package.json that simply adds the server/index.js as a middleware.  Regardless of which  installation  option you chose, you need the following folder structure in order to run the UI:  dist/*\nconfig/env-settings.json\nserver/index.js\nexpress-server.js  You can use node to launch the UI from the top-level of the folder structure above.  To launch the UI with the default settings (without specifying proper API endpoints you will not be able to create or run a query):  PORT=8800 node express-server.js  To launch with custom settings:  NODE_ENV= your_property_name_from_env-settings.json  PORT=8800 node express-server.js  Visit localhost:8800 to see your UI that should be configured with the right settings.", 
            "title": "Running"
        }, 
        {
            "location": "/ui/setup/#configuration", 
            "text": "The configuration for the UI lets you have different instances of Bullet for different environments (e.g. CI, Staging, Production). These settings can be found in  env-settings.json .     Setting  Meaning      queryHost  The end point (port included) of your Web Service machine that is talking to the Bullet backend    queryNamespace  Any qualifiers you have after your host and port on your Web Service running on your  queryHost    queryPath  The path fragment after the  queryNamespace  on your Web Service running on your  queryHost  for the WebSocket endpoint    queryStompRequestChannel  The fragment after this is the Stomp Request channel as configured in your Web Service for the WebSocket endpoint    queryStompResponseChannel  The fragment after this is the Stomp Response channel as configured in your Web Service for the WebSocket endpoint    schemaHost  The end point (port included) of your Web Service machine that is serving your schema in the JSON API format (see  Web Service setup  for details.)    schemaNamespace  The path fragment on your schema Web Service running on the  schemaHost . There is no  schemaPath  because it  must  be  columns  in order for the UI to be able fetch the column resource (the fields in your schema).    modelVersion  This is used an indicator to apply changes to the stored queries, results etc. It is monotonically increasing. On startup, changes specified in  migrations  will be applied if the old modelVersion is not present or is   than this number    migrations  is an object that currently supports one key:  deletions  of type string. The value can be set to either  result  or  query . The former wipes all existing results. The latter wipes everything. See  modelVersion  above.    helpLinks  Is a list of objects, where each object is a help link. These links populate the \"Help\" drop-down on the UI's top navbar. You can add links to explain your data for example    defaultQuery  Can either be a  API Query  or a URL from which one could be fetched dynamically. The UI makes this the query created on every newly created Query. You could use this as a way to have user specific (for example, cookie based) filters created for your users or customize an aggregation when they create a new query in the UI. Note that if you have are accessing a map subfield and your field value in the filter is set as  foo.bar  and you want  bar  to be the subfield in the UI query builder, you will need to add a key called  subfield  in the filter (not supported by the API) and set its value to  true    bugLink  Is a URL that by default points to the issues page for the UI GitHub repository. You can change it to point to your own custom JIRA queue or something else    defaultValues  Is an object that lets you configures defaults for various query parameters and lets you tie your custom backend settings to the UI     These are the properties in the  defaultValues  object. The Validated column denotes if the value is used when validating a query for correctness and the In Help column denotes if the value is displayed in the popover help messages in the UI.     Default Values  Validated  In Help  Meaning      aggregationMaxSize  Yes  Yes  The size used when doing a Count Distinct, Distinct, Group By, or Distribution query. Set this to your max aggregations size in your backend configuration    rawMaxSize  Yes  Yes  The maximum size for a Raw query. Set this to your max raw aggregation size in your backend configuration    durationMaxSecs  Yes  Yes  The maximum duration for a query. Set this to the seconds version of milliseconds max duration in your backend configuration    distributionNumberOfPoints  Yes  No  The default value filled in for the Number of Points field for all Distribution aggregations    distributionQuantilePoints  No  No  The default value filled in for the Points field for Quantile Distribution aggregations    distributionQuantileStart  No  No  The default value filled in for the Start field for Quantile Distribution aggregations    distributionQuantileEnd  No  No  The default value filled in for the End field for Quantile Distribution aggregations    distributionQuantileIncrement  No  No  The default value filled in for the Increment field for Quantile Distribution aggregations    windowEmitFrequencyMinSecs  Yes  No  The minimum time interval at which a time based window can be returned. Set this to the minimum window emit frequency from your backend configuration    everyForRecordBasedWindow  No  No  The default value for the number of records in a window for a record based window    everyForTimeBasedWindow  No  No  The default value for the number of records in a window for a time based window    sketches.countDistinctMaxEntries  No  Yes  The maximum entries configured for your Count Distinct sketch in your backend configuration    sketches.groupByMaxEntries  No  Yes  The maximum entries configured for your Group sketch in your backend configuration    sketches.distributionMaxEntries  No  Yes  The maximum entries configured for your Distribution sketch in your backend configuration    sketches.distributionMaxNumberOfPoints  Yes  Yes  The maximum number of points allowed for Distribution aggregations in your backend configuration    sketches.topKMaxEntries  No  Yes  The maximum entries configured for your Top K sketch in your backend configuration    sketches.topKErrorType  No  Yes  The ErrorType used for your Top K sketch in your backend configuration. You should set this to the full String rather than  NFN  or  NFP    metadataKeyMapping.querySection  No  Yes  The name of the Metadata key for the Query Concept in your backend configuration    metadataKeyMapping.windowSection  No  Yes  The name of the Metadata key for the Window Concept in your backend configuration    metadataKeyMapping.sketchSection  No  Yes  The name of the Metadata key for the Theta Concept in your backend configuration    metadataKeyMapping.theta  No  Yes  The name of the Metadata key for the Theta Concept in your backend configuration    metadataKeyMapping.uniquesEstimate  No  Yes  The name of the Metadata key for the Uniques Estimate Concept in your backend configuration    metadataKeyMapping.queryCreationTime  No  Yes  The name of the Metadata key for the Query Creation Time Concept in your backend configuration    metadataKeyMapping.queryTerminationTime  No  Yes  The name of the Metadata key for the Query Termination Time Concept in your backend configuration    metadataKeyMapping.estimatedResult  No  Yes  The name of the Metadata key for the Estimated Result Concept in your backend configuration    metadataKeyMapping.standardDeviations  No  Yes  The name of the Metadata key for the Standard Deviations Concept in your backend configuration    metadataKeyMapping.normalizedRankError  No  Yes  The name of the Metadata key for the Normalized Rank Error Concept in your backend configuration    metadataKeyMapping.maximumCountError  No  Yes  The name of the Metadata key for the Maximum Count Error Concept in your backend configuration    metadataKeyMapping.itemsSeen  No  Yes  The name of the Metadata key for the Items Seen Concept in your backend configuration    metadataKeyMapping.minimumValue  No  Yes  The name of the Metadata key for the Minimum Value Concept in your backend configuration    metadataKeyMapping.maximumValue  No  Yes  The name of the Metadata key for the Maximum Value Concept in your backend configuration     You can specify values for each property above in the  env-settings.json  file. These will be used when running a custom instance of the UI (see  above ).  The  default  property in the  env-settings.json  that loads default settings for the UI that can be selectively overridden based on which environment you are running on. All settings explained above have default values\nthat are the same as the  default backend settings . However, the defaults do not add the  defaultQuery  setting explained above.  {\n   default : {\n     queryHost :  http://localhost:5555 ,\n     queryNamespace :  api/bullet ,\n     queryPath :  query ,\n     schemaHost :  http://localhost:5555 ,\n     schemaNamespace :  api/bullet ,\n     helpLinks : [\n      {\n         name :  Tutorials ,\n         link :  https://bullet-db.github.io/ui/usage \n      }\n    ],\n     bugLink :  https://github.com/bullet-db/bullet-ui/issues ,\n     modelVersion : 1,\n     defaultValues : {\n       aggregationMaxSize : 512,\n       rawMaxSize : 100,\n       durationMaxSecs : 120,\n       distributionNumberOfPoints : 11,\n       distributionQuantilePoints :  0, 0.25, 0.5, 0.75, 0.9, 1 ,\n       distributionQuantileStart : 0,\n       distributionQuantileEnd : 1,\n       distributionQuantileIncrement : 0.1,\n       queryTimeoutSecs : 3,\n       sketches : {\n         countDistinctMaxEntries : 16384,\n         groupByMaxEntries : 512,\n         distributionMaxEntries : 1024,\n         distributionMaxNumberOfPoints : 100,\n         topKMaxEntries : 1024,\n         topKErrorType :  No False Negatives \n      },\n       metadataKeyMapping : {\n         theta :  theta ,\n         uniquesEstimate :  uniques_estimate ,\n         queryCreationTime :  query_receive_time ,\n         queryTerminationTime :  query_finish_time ,\n         estimatedResult :  was_estimated ,\n         standardDeviations :  standard_deviations ,\n         normalizedRankError :  normalized_rank_error ,\n         maximumCountError :  maximum_count_error ,\n         itemsSeen :  items_seen ,\n         minimumValue :  minimum_value ,\n         maximumValue :  maximum_value \n      }\n    }\n  }\n}  You can add more properties for each environment you have the UI running on and  override  the properties in the  default  object. See  below  for an example.   CORS  All your Web Service endpoints must support CORS (return the right headers) in order for the UI to be able to communicate with it. The Bullet Web Service already does this for the Query and Schema endpoints.", 
            "title": "Configuration"
        }, 
        {
            "location": "/ui/setup/#example", 
            "text": "To cement all this, if you wanted an instance of the UI in your CI environment, you could add another property to the  env-settings.json  file.  {\n     ci : {\n         queryHost :  http://bullet-ws.dev.domain.com:4080 ,\n         schemaHost :  http://bullet-ws.dev.domain.com:4080 ,\n         helpLinks : [\n          {\n             name :  Custom Documentation ,\n             link :  http://data.docs.domain.com \n          }\n        ],\n         defaultValues  : {\n             durationMaxSecs : 300,\n             sketches : {\n                 countDistinctMaxEntries : 32768,\n                 distributionMaxNumberOfPoints : 50\n            }\n        },\n         defaultQuery :  http://bullet-ws.dev.domain.com:4080/custom-endpoint/api/defaultQuery \n    }\n}  Your UI on your CI environment will:   POST to  http://bullet-ws.dev.domain.com:4080/bullet/api/drpc  for UI created Bullet queries  GET the schema from  http://bullet-ws.dev.domain.com:4080/bullet/api/columns  Populate an additional link on the Help drop-down pointing to  http://data.docs.domain.com  Allow queries to run as long as 300 seconds  Use 32768 in the help menu for the max number of unique elements that can be counted exactly  Allow only 50 points to be generated for Distribution queries  GET and cache a defaultQuery from  http://bullet-ws.dev.domain.com:4080/custom-endpoint/api/defaultQuery   You would make express use these settings by running  NODE_ENV=ci PORT=8800 node express-server.js", 
            "title": "Example"
        }, 
        {
            "location": "/ui/usage/", 
            "text": "Navigating the UI\n\n\nThe UI should (hopefully) be self-explanatory. Any particular section that requires additional information has the \n icon next to it. Clicking this will display information relevant to that section.\n\n\nThe interactions in this page are running on the topology that was set up in the \nQuick Start on Storm\n.  Recall that that example backend is configured to produce \n20 data records every 101 ms.\n.\n\n\n\n\nNOTE: Some of these videos use an old version of the Bullet UI\n\n\nWe are currently in progress adding new videos with windowing and other new features from the latest UI version etc.\n\n\n\n\nLanding page\n\n\nLoading the UI takes you to a page that shows all the queries and past results. You can edit, delete and copy your existing queries here. You can also view or clear your past results for the queries.\n\n\nThe help links you \nconfigure  for the UI\n are shown in the Help menu.\n\n\nSchema\n\n\nThe schema you \nplug into the UI\n is shown here so the users can better understand what the columns mean. Enumerated map fails can be expanded and their nested fields are also described.\n\n\nExample: The landing and schema pages\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nAbout tables\n\n\nAll tables in the UI, the table for the existing queries, the existing results, the schema or for a query result are all infinite-scroll type tables - they only show a fixed amount but if you scroll to the end, they automatically load more. The tables can all be sorted by clicking on the column name.\n\n\n\n\nA simple first query\n\n\nIf you create a new query, it defaults to getting a raw data record. This query returns immediately even though the maximum duration is set to 20s because the \nQuick Start topology\n produces about 200 records/s and we are looking for one record with no filters.\n\n\nResults\n\n\nSince the entire record was asked to be returned instead of particular fields, the result defaults to a JSON view of the data. You can click the Show as Table button to switch the mode. In this mode, you can click on each cell to get a popover showing the data formatted.\n\n\nYou can also download the results in JSON, CSV or flattened CSV (fields inside maps and lists are exploded). Any metadata returned for the query is collapsed by default. Any relevant metadata for the query \nas configured\n is shown here. As always the help icons display help messages for each section.\n\n\nExample: Picking a random record from the stream\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\nFiltering and projecting data\n\n\nThe Filters section in the UI features a querybuilder (a modified version of the \njQuery-QueryBuilder\n) that you can use to add filters. These allow you to \npick at the slice of data\n from your stream that is relevant to you.\n\n\nThe Output Data section lets you aggregate or choose to see raw data records. You can either get all the data as \nabove\n or you can select a subset of fields (and optionally rename them) that you would like to see.\n\n\nExample: Finding and picking out fields from events that have probability \n 0.5\n\n\n\n  \n\n  Your browser does not support the video tag\n\n\n\n\n\n\nDefault result display\n\n\nIf you choose the Show All Fields selection in the Output Data option, the results will default to the JSON data view. Otherwise, it defaults to the table.\n\n\n\n\nStream Raw Events\n\n\nNote:\n This query is only available in the Bullet UI version 0.5.0 and later.\n\n\nA very simple but useful query is a query with a filter and a \nSliding Window of size 1\n. This query will run for the extent of your duration and stream back events that match your filters as they arrive:\n\n\n\n\n\nBe careful\n when you use this query to ensure that your filter is sufficient to avoid returning TOO many results too fast. If this occurs Bullet will kill your query because of rate limiting (the default rate limit is 500 records per second).\n\n\nTumbling Windows\n\n\nNote:\n This query is only available in the Bullet UI version 0.5.0 and later.\n\n\nTime-Based Tumbling Windows\n will return results every X seconds:\n\n\n\n\n\nThis example groups-by \"type\" and computes a couple metrics for each 2 second window.\n\n\nAdditive Tumbling Windows\n\n\nNote:\n This query is only available in the Bullet UI version 0.5.0 and later.\n\n\nAdditive tumbling windows\n will also return results every X seconds, but the results will contain all the data collected since the beginning of the query:\n\n\n\n\n\nIn this example we compute bucket'ed frequency for the \"probability\" field. You can see from the scale of the Y axis that these computations are accumulated over the life of the query.\n\n\nComplex Filtering\n\n\nThe querybuilder also lets you easily create nested filters. You can add basic relational filters or group a set of basic filters by connecting them with ANDs and ORs. You can also drag and drop filters and groups.\n\n\nThe querybuilder is also type aware. The operations you can perform change based on the type. Numeric fields only allow numeric values. String fields allow you to apply regular expressions to them or specify multiple values at the same time using a \n,\n. Boolean fields only allow you to choose a radio button etc.\n\n\nExample: Finding and picking out the first and second events in each period that also have probability \n 0.5\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nWhat's the .* next to a field?\n\n\nIf you have a map that is not enumerated (the keys are not known upfront), there will be \ntwo\n selections for the field in the dropdowns. If you want to apply operations to the nested keys, you can choose the field with the \n.*\n. This will display a free-form subfield selection input where you can specify the key. If you want to apply operations on the entire map, you will need to choose the field without the \n.*\n\n\n\n\nCount Distinct\n\n\nExact\n\n\nThe settings you had \nconfigured when launching\n the backend determines the number of unique values that Bullet can \ncount exactly\n. The example UI shown here used the default configuration value of \n16384\n that the example provided, so for all Count Distinct queries where the cardinality of the field combination is less than this number, the result is exact. The metadata also reflects this.\n\n\nYou can also optionally rename the result.\n\n\nExample: Counting unique UUIDs for 20s\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nShouldn't the count be slightly more in the last example?\n\n\nShort answer:\n Yes and it's because of the synthetic nature of the data generation.\n\n\nLong answer:\n We should have had \n20000 ms / 101 ms\n or \n198\n periods or \n198 periods * 20 tuples/period\n or \n3960\n tuples with unique values for the\nuuid\n field. The example spout generates data in bursts of 20 at the start of every period (101 ms). However, the delay isn't exactly 101 ms between periods; it's a bit more depending on when Storm decided to run the emission code. As a result, every period will slowly add a delay of a few ms. Eventually, this can lead us to missing an entire period. This increases the longer the query runs. Even a delay of 1 ms every period (a very likely scenario) can add up to 101 ms or 1 period in as short a time as a 101 periods or \n101 periods * 101 ms/period\n or \n~10 s\n. A good rule of thumb is that for every 10 s your query runs, you are missing 20 tuples. You might also miss another 20 tuples at the beginning or the end of the window since the spout is bursty.\n\n\nIn most real streaming scenarios, data should be constantly flowing and there shouldn't delays building like this. Even so, for a distributed, streaming system like Bullet, you should always remember that data can be missed at either end of your query window due to inherent skews and timing issues.\n\n\n\n\n\n\nWhy did the Maximum Records input disappear?\n\n\nMaximum Records as a query stopping criteria only makes sense when you are picking out raw records. While the API still supports using it as a limiting mechanism on the number of records that are returned to you, the UI eschews this and sets it to a value that you can \nconfigure\n. It is also particularly confusing to see a Maximum Records when you are doing a Count Distinct operation, while it makes sense when you are Grouping data. You should ideally set this to the same value as your maximum aggregation size that you configure when launching your backend.\n\n\n\n\nApproximate\n\n\nWhen the result is approximate, it is shown as a decimal value. The Result Metadata section will reflect that the result was estimated and provide you standard deviations for the true value. The errors are derived from \nDataSketches here\n. Note the line for \n16384\n, which was what we configured for the maximum unique values for the Count Distinct operation. In the example below, this means if we want 99.73% confidence for the result, the \n3\n standard deviation entry says that the true count could vary from \n38194\n to \n39590\n.\n\n\nExample: Counting unique UUIDs for 200s\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nSo why is the approximate count what it is?\n\n\nThe backend should have produced \n20 * 200000/101\n or \n39603\n tuples with unique uuids. Due to the synthetic nature of the data generation and the building delays mentioned above, we estimated that we should subtract about 20 tuples for every 10 s the query runs. Since this query ran for \n200 s\n, this makes the actual uuids generated to be at best \n39603 - (200/10) * 20\n or \n39203\n. The result from Bullet was \n39069\n, which is an error of \n~0.3 %\n. The real error is probably less than that because we assumed the delay between periods to be 1 ms to get the \n39203\n number. It's probably slightly larger making the actual uuids generated lower and closer to our estimate.\n\n\n\n\nGroup all\n\n\nWhen choosing the Grouped Data option, you can choose to add fields to group by. If you do not and you add metrics, they will apply to all the data that matches your filters (or the whole data set if you don't have any).\n\n\nExample: Counting, summing and averaging on the whole dataset\n\n\nThe metrics you apply on fields are all numeric presently. If you apply a metric on a non-numeric field, Bullet will try to \ntype-cast\n your field into number and if it's not possible, the result will be \nnull\n. The result will also be \nnull\n if the field was not present or no data matched your filters.\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nErrors when building queries\n\n\nErrors that can be readily displayed are shown immediately. Some errors like the ones in the example above are only shown when you try to run or save the query.\n\n\n\n\n\n\nAre Grouped Data metrics approximate?\n\n\nNo, the results are all exact. See below to see what is approximated when you have too many unique group combinations.\n\n\n\n\nGroup by\n\n\nYou can also choose Group fields and perform metrics per group. If you do not add any Metric fields, you will be \nperforming a distinct operation\n on your group fields.\n\n\nExample: Grouping by tuple_number\n\n\nIn this example, we group by \ntuple_number\n. Recall that this is the number assigned to a tuple within a period. They range from 0 to 19. If we group by this, we expect to have 20 unique groups. In 5s, we have \n5000/101\n or \n49\n periods. Each period has one of each \ntuple_number\n. We expect \n49\n as the count for each group, and this what we see. The building delays mentioned \nin the note above\n has not really started affecting the data yet. Note that the average is also roughly \n0.50\n since the \nprobability\n field is a uniformly distributed value between 0 and 1.\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nWhat happens if I group by uuid?\n\n\nTry it out! Nothing bad should happen. If the number of unique group values exceeds the \nmaximum configured\n (we used 1024 for this example), you will receive a \nuniform sample\n across your unique group values. The results for your metrics however, are \nnot sampled\n. It is the groups that are sampled on. This means that is \nno\n guarantee of order if you were expecting the \nmost popular\n groups or similar. You should use the Top K query in that scenario.\n\n\n\n\n\n\nWhy no Count Distinct after Grouping\n\n\nAt this time, we do not support counting distinct values per field because with the current implementation of Grouping, it would involve storing DataSketches within DataSketches. We are considering this in a future release however.\n\n\n\n\n\n\nAha, sorting by tuple_number didn't sort properly!\n\n\nGood job, eagle eyes! Unfortunately, whenever we group on fields, those fields become strings under the current implementation. Rather than convert them back at the end, we have currently decided to leave it as is. This means that in your results, if you try and sort by a grouped field, it will perform a lexicographical sort even if it was originally a number.\n\n\nHowever, this also means that you can actually group by any field - including non primitives such as maps and lists! The field will be converted to a string and that string will be used as the field's representation for uniqueness and grouping purposes.\n\n\n\n\nDistributions\n\n\nIn this example, we find distributions of the \nduration\n  field. This field is generated randomly from 0 to 10,049, with a tendency to have values that are closer to 0 than 10,049. Let's see if this is true. Note that since this field has random values, the results you see per query are the values generated during that query's duration.\n\n\nThe distribution type of output data requires you to pick a type of distribution: \nQuantiles\n, \nFrequencies\n or \nCumulative Frequencies\n. \nQuantiles\n lets you get various percentiles (e.g. 25th, 99th) of your numeric field. \nFrequencies\n lets you break up the range of values of your field into intervals and get a count of how many values fell into each interval. \nCumulative Frequencies\n does the same as \nFrequencies\n but each interval includes the counts of all the intervals prior to it. Both \nFrequencies\n and \nCumulative Frequencies\n also give you a probability of how likely a value is to fall into the interval.\n\n\nAll the distributions require you to specify some numeric points. For \nQuantiles\n, these points are between 0 and 1 and the value denotes the percentile you are looking for. (0.25 for 25th percentile, 0.99 for 99th etc). For \nFrequencies\n and \nCumulative Frequencies\n, the points are between the minimum and maximum value of your field and every 2 contiguous points create an interval. However, the first interval always starts from \n-\n to the first point and the last interval always starts from your last point to \n+\n.\n\n\nYou can read much more about this in the UI help by clicking the \nNeed more help?\n link.\n\n\nExact\n\n\nExample: Finding the various percentiles of duration\n\n\nThis example shows all 3 values of specifying points and shows \nexact\n distribution results for the \nduration\n field.\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nExample: Finding some frequency counts of duration values in an interval\n\n\nThe last example showed that the 90th percentile of \nduration\n was around 4000. This example gets some frequencies in various intervals.\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\nTry out and see what \nCumulative Frequencies\n does yourself!\n\n\nApproximate\n\n\nThis next example shows how an approximate distribution result looks.\n\n\nExample: Approximate quantile distribution\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nNormalized Rank Error\n\n\nTo understand what this means, refer to the \nexplanation here\n. You can also refer to the help in the Result Metadata section.\n\n\n\n\nTop K\n\n\nTop K lets you get the most \nfrequent items\n or the \nheavy hitters\n for the values in a set of a fields.\n\n\nExact\n\n\nThis example gets the Top 3 most popular \ntype\n values (there are only 6 but this illustrates the idea).\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\nApproximate\n\n\nBy adding \nduration\n into the fields, the number of unique values for \n(type, duration)\n is increased. However, because \nduration\n has a tendency to have low values, we will have some \nfrequent items\n. The counts are now estimated. We ask for the top 300 results but we also say that they should have a count of at least 20. This restricts the overall number of results to 12.\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\n\n\nMaximum Count Error\n\n\nThe \nmaximum_count_error\n value for the query above was \n3\n. This means that the difference between the upper bound and the lower bound of each count estimate is \n3\n. Bullet returns the upper bound as the estimate so subtracting \n3\n from each count gives you the lower bound of the count. Note that some counts are closer to each other than the count error. For instance, \n(quux, 1)\n and \n(baz, 0)\n have counts \n67\n and \n66\n but their true counts could be from \n64 to 67\n and \n63 to 66\n respectively. This means that \n(baz, 0)\n could well be the most frequent item for this query.\n\n\n\n\nCharting\n\n\nBullet UI v0.3.0 and above\n added support for charting and pivoting. This example shows how to get a basic chart on \nBullet UI v0.3.1\n. If you are following the \nQuick Start on Storm\n, then this should be in your UI. The charting and pivoting modes are only enabled for queries that are \nnot\n Count Distinct or Group without Group Fields. This is because these results only have a single row and it does not make sense to graph them. They are enabled for all other queries.\n\n\nThe charting example below shows how to get a quick chart of a \nGroup\n query with 3 metrics.\n\n\n\n  \n\n  Your browser does not support the video tag.\n\n\n\n\nPivoting\n\n\nIf the regular chart option is insufficient for your result (for instance, you have too many groups and metrics or you want to post-aggregate your results or remove outliers etc), then there is a advanced Pivot mode available when you are in the Chart option. The Pivot option provides a drag-and-drop interface to drag fields to breakdown and aggregate by their values. Operations such as finding standard deviations, variance, average, median, sum over sums etc are available as well as easily viewing them as tables and charts. The following example shows a \nGroup\n query with multiple groups and metrics and some interactions with the Pivot table.\n\n\n\n\nRaw data does not seem to have a regular chart mode option\n\n\nThis is deliberate since the Chart option tries to infer your independent and dependent columns. When you fetch raw data, this is prone to errors so only the Pivot option is allowed. You can always graph within the Pivot option if you need to.\n\n\n\n\n\n  \n\n  Your browser does not support the video tag.", 
            "title": "Usage"
        }, 
        {
            "location": "/ui/usage/#navigating-the-ui", 
            "text": "The UI should (hopefully) be self-explanatory. Any particular section that requires additional information has the   icon next to it. Clicking this will display information relevant to that section.  The interactions in this page are running on the topology that was set up in the  Quick Start on Storm .  Recall that that example backend is configured to produce  20 data records every 101 ms. .   NOTE: Some of these videos use an old version of the Bullet UI  We are currently in progress adding new videos with windowing and other new features from the latest UI version etc.", 
            "title": "Navigating the UI"
        }, 
        {
            "location": "/ui/usage/#landing-page", 
            "text": "Loading the UI takes you to a page that shows all the queries and past results. You can edit, delete and copy your existing queries here. You can also view or clear your past results for the queries.  The help links you  configure  for the UI  are shown in the Help menu.", 
            "title": "Landing page"
        }, 
        {
            "location": "/ui/usage/#schema", 
            "text": "The schema you  plug into the UI  is shown here so the users can better understand what the columns mean. Enumerated map fails can be expanded and their nested fields are also described.  Example: The landing and schema pages  \n   \n  Your browser does not support the video tag.   About tables  All tables in the UI, the table for the existing queries, the existing results, the schema or for a query result are all infinite-scroll type tables - they only show a fixed amount but if you scroll to the end, they automatically load more. The tables can all be sorted by clicking on the column name.", 
            "title": "Schema"
        }, 
        {
            "location": "/ui/usage/#a-simple-first-query", 
            "text": "If you create a new query, it defaults to getting a raw data record. This query returns immediately even though the maximum duration is set to 20s because the  Quick Start topology  produces about 200 records/s and we are looking for one record with no filters.", 
            "title": "A simple first query"
        }, 
        {
            "location": "/ui/usage/#results", 
            "text": "Since the entire record was asked to be returned instead of particular fields, the result defaults to a JSON view of the data. You can click the Show as Table button to switch the mode. In this mode, you can click on each cell to get a popover showing the data formatted.  You can also download the results in JSON, CSV or flattened CSV (fields inside maps and lists are exploded). Any metadata returned for the query is collapsed by default. Any relevant metadata for the query  as configured  is shown here. As always the help icons display help messages for each section.  Example: Picking a random record from the stream  \n   \n  Your browser does not support the video tag.", 
            "title": "Results"
        }, 
        {
            "location": "/ui/usage/#filtering-and-projecting-data", 
            "text": "The Filters section in the UI features a querybuilder (a modified version of the  jQuery-QueryBuilder ) that you can use to add filters. These allow you to  pick at the slice of data  from your stream that is relevant to you.  The Output Data section lets you aggregate or choose to see raw data records. You can either get all the data as  above  or you can select a subset of fields (and optionally rename them) that you would like to see.  Example: Finding and picking out fields from events that have probability   0.5  \n   \n  Your browser does not support the video tag   Default result display  If you choose the Show All Fields selection in the Output Data option, the results will default to the JSON data view. Otherwise, it defaults to the table.", 
            "title": "Filtering and projecting data"
        }, 
        {
            "location": "/ui/usage/#stream-raw-events", 
            "text": "Note:  This query is only available in the Bullet UI version 0.5.0 and later.  A very simple but useful query is a query with a filter and a  Sliding Window of size 1 . This query will run for the extent of your duration and stream back events that match your filters as they arrive:   Be careful  when you use this query to ensure that your filter is sufficient to avoid returning TOO many results too fast. If this occurs Bullet will kill your query because of rate limiting (the default rate limit is 500 records per second).", 
            "title": "Stream Raw Events"
        }, 
        {
            "location": "/ui/usage/#tumbling-windows", 
            "text": "Note:  This query is only available in the Bullet UI version 0.5.0 and later.  Time-Based Tumbling Windows  will return results every X seconds:   This example groups-by \"type\" and computes a couple metrics for each 2 second window.", 
            "title": "Tumbling Windows"
        }, 
        {
            "location": "/ui/usage/#additive-tumbling-windows", 
            "text": "Note:  This query is only available in the Bullet UI version 0.5.0 and later.  Additive tumbling windows  will also return results every X seconds, but the results will contain all the data collected since the beginning of the query:   In this example we compute bucket'ed frequency for the \"probability\" field. You can see from the scale of the Y axis that these computations are accumulated over the life of the query.", 
            "title": "Additive Tumbling Windows"
        }, 
        {
            "location": "/ui/usage/#complex-filtering", 
            "text": "The querybuilder also lets you easily create nested filters. You can add basic relational filters or group a set of basic filters by connecting them with ANDs and ORs. You can also drag and drop filters and groups.  The querybuilder is also type aware. The operations you can perform change based on the type. Numeric fields only allow numeric values. String fields allow you to apply regular expressions to them or specify multiple values at the same time using a  , . Boolean fields only allow you to choose a radio button etc.  Example: Finding and picking out the first and second events in each period that also have probability   0.5  \n   \n  Your browser does not support the video tag.   What's the .* next to a field?  If you have a map that is not enumerated (the keys are not known upfront), there will be  two  selections for the field in the dropdowns. If you want to apply operations to the nested keys, you can choose the field with the  .* . This will display a free-form subfield selection input where you can specify the key. If you want to apply operations on the entire map, you will need to choose the field without the  .*", 
            "title": "Complex Filtering"
        }, 
        {
            "location": "/ui/usage/#count-distinct", 
            "text": "", 
            "title": "Count Distinct"
        }, 
        {
            "location": "/ui/usage/#exact", 
            "text": "The settings you had  configured when launching  the backend determines the number of unique values that Bullet can  count exactly . The example UI shown here used the default configuration value of  16384  that the example provided, so for all Count Distinct queries where the cardinality of the field combination is less than this number, the result is exact. The metadata also reflects this.  You can also optionally rename the result.  Example: Counting unique UUIDs for 20s  \n   \n  Your browser does not support the video tag.   Shouldn't the count be slightly more in the last example?  Short answer:  Yes and it's because of the synthetic nature of the data generation.  Long answer:  We should have had  20000 ms / 101 ms  or  198  periods or  198 periods * 20 tuples/period  or  3960  tuples with unique values for the uuid  field. The example spout generates data in bursts of 20 at the start of every period (101 ms). However, the delay isn't exactly 101 ms between periods; it's a bit more depending on when Storm decided to run the emission code. As a result, every period will slowly add a delay of a few ms. Eventually, this can lead us to missing an entire period. This increases the longer the query runs. Even a delay of 1 ms every period (a very likely scenario) can add up to 101 ms or 1 period in as short a time as a 101 periods or  101 periods * 101 ms/period  or  ~10 s . A good rule of thumb is that for every 10 s your query runs, you are missing 20 tuples. You might also miss another 20 tuples at the beginning or the end of the window since the spout is bursty.  In most real streaming scenarios, data should be constantly flowing and there shouldn't delays building like this. Even so, for a distributed, streaming system like Bullet, you should always remember that data can be missed at either end of your query window due to inherent skews and timing issues.    Why did the Maximum Records input disappear?  Maximum Records as a query stopping criteria only makes sense when you are picking out raw records. While the API still supports using it as a limiting mechanism on the number of records that are returned to you, the UI eschews this and sets it to a value that you can  configure . It is also particularly confusing to see a Maximum Records when you are doing a Count Distinct operation, while it makes sense when you are Grouping data. You should ideally set this to the same value as your maximum aggregation size that you configure when launching your backend.", 
            "title": "Exact"
        }, 
        {
            "location": "/ui/usage/#approximate", 
            "text": "When the result is approximate, it is shown as a decimal value. The Result Metadata section will reflect that the result was estimated and provide you standard deviations for the true value. The errors are derived from  DataSketches here . Note the line for  16384 , which was what we configured for the maximum unique values for the Count Distinct operation. In the example below, this means if we want 99.73% confidence for the result, the  3  standard deviation entry says that the true count could vary from  38194  to  39590 .  Example: Counting unique UUIDs for 200s  \n   \n  Your browser does not support the video tag.   So why is the approximate count what it is?  The backend should have produced  20 * 200000/101  or  39603  tuples with unique uuids. Due to the synthetic nature of the data generation and the building delays mentioned above, we estimated that we should subtract about 20 tuples for every 10 s the query runs. Since this query ran for  200 s , this makes the actual uuids generated to be at best  39603 - (200/10) * 20  or  39203 . The result from Bullet was  39069 , which is an error of  ~0.3 % . The real error is probably less than that because we assumed the delay between periods to be 1 ms to get the  39203  number. It's probably slightly larger making the actual uuids generated lower and closer to our estimate.", 
            "title": "Approximate"
        }, 
        {
            "location": "/ui/usage/#group-all", 
            "text": "When choosing the Grouped Data option, you can choose to add fields to group by. If you do not and you add metrics, they will apply to all the data that matches your filters (or the whole data set if you don't have any).  Example: Counting, summing and averaging on the whole dataset  The metrics you apply on fields are all numeric presently. If you apply a metric on a non-numeric field, Bullet will try to  type-cast  your field into number and if it's not possible, the result will be  null . The result will also be  null  if the field was not present or no data matched your filters.  \n   \n  Your browser does not support the video tag.   Errors when building queries  Errors that can be readily displayed are shown immediately. Some errors like the ones in the example above are only shown when you try to run or save the query.    Are Grouped Data metrics approximate?  No, the results are all exact. See below to see what is approximated when you have too many unique group combinations.", 
            "title": "Group all"
        }, 
        {
            "location": "/ui/usage/#group-by", 
            "text": "You can also choose Group fields and perform metrics per group. If you do not add any Metric fields, you will be  performing a distinct operation  on your group fields.  Example: Grouping by tuple_number  In this example, we group by  tuple_number . Recall that this is the number assigned to a tuple within a period. They range from 0 to 19. If we group by this, we expect to have 20 unique groups. In 5s, we have  5000/101  or  49  periods. Each period has one of each  tuple_number . We expect  49  as the count for each group, and this what we see. The building delays mentioned  in the note above  has not really started affecting the data yet. Note that the average is also roughly  0.50  since the  probability  field is a uniformly distributed value between 0 and 1.  \n   \n  Your browser does not support the video tag.   What happens if I group by uuid?  Try it out! Nothing bad should happen. If the number of unique group values exceeds the  maximum configured  (we used 1024 for this example), you will receive a  uniform sample  across your unique group values. The results for your metrics however, are  not sampled . It is the groups that are sampled on. This means that is  no  guarantee of order if you were expecting the  most popular  groups or similar. You should use the Top K query in that scenario.    Why no Count Distinct after Grouping  At this time, we do not support counting distinct values per field because with the current implementation of Grouping, it would involve storing DataSketches within DataSketches. We are considering this in a future release however.    Aha, sorting by tuple_number didn't sort properly!  Good job, eagle eyes! Unfortunately, whenever we group on fields, those fields become strings under the current implementation. Rather than convert them back at the end, we have currently decided to leave it as is. This means that in your results, if you try and sort by a grouped field, it will perform a lexicographical sort even if it was originally a number.  However, this also means that you can actually group by any field - including non primitives such as maps and lists! The field will be converted to a string and that string will be used as the field's representation for uniqueness and grouping purposes.", 
            "title": "Group by"
        }, 
        {
            "location": "/ui/usage/#distributions", 
            "text": "In this example, we find distributions of the  duration   field. This field is generated randomly from 0 to 10,049, with a tendency to have values that are closer to 0 than 10,049. Let's see if this is true. Note that since this field has random values, the results you see per query are the values generated during that query's duration.  The distribution type of output data requires you to pick a type of distribution:  Quantiles ,  Frequencies  or  Cumulative Frequencies .  Quantiles  lets you get various percentiles (e.g. 25th, 99th) of your numeric field.  Frequencies  lets you break up the range of values of your field into intervals and get a count of how many values fell into each interval.  Cumulative Frequencies  does the same as  Frequencies  but each interval includes the counts of all the intervals prior to it. Both  Frequencies  and  Cumulative Frequencies  also give you a probability of how likely a value is to fall into the interval.  All the distributions require you to specify some numeric points. For  Quantiles , these points are between 0 and 1 and the value denotes the percentile you are looking for. (0.25 for 25th percentile, 0.99 for 99th etc). For  Frequencies  and  Cumulative Frequencies , the points are between the minimum and maximum value of your field and every 2 contiguous points create an interval. However, the first interval always starts from  -  to the first point and the last interval always starts from your last point to  + .  You can read much more about this in the UI help by clicking the  Need more help?  link.", 
            "title": "Distributions"
        }, 
        {
            "location": "/ui/usage/#exact_1", 
            "text": "Example: Finding the various percentiles of duration  This example shows all 3 values of specifying points and shows  exact  distribution results for the  duration  field.  \n   \n  Your browser does not support the video tag.   Example: Finding some frequency counts of duration values in an interval  The last example showed that the 90th percentile of  duration  was around 4000. This example gets some frequencies in various intervals.  \n   \n  Your browser does not support the video tag.  Try out and see what  Cumulative Frequencies  does yourself!", 
            "title": "Exact"
        }, 
        {
            "location": "/ui/usage/#approximate_1", 
            "text": "This next example shows how an approximate distribution result looks.  Example: Approximate quantile distribution  \n   \n  Your browser does not support the video tag.   Normalized Rank Error  To understand what this means, refer to the  explanation here . You can also refer to the help in the Result Metadata section.", 
            "title": "Approximate"
        }, 
        {
            "location": "/ui/usage/#top-k", 
            "text": "Top K lets you get the most  frequent items  or the  heavy hitters  for the values in a set of a fields.", 
            "title": "Top K"
        }, 
        {
            "location": "/ui/usage/#exact_2", 
            "text": "This example gets the Top 3 most popular  type  values (there are only 6 but this illustrates the idea).  \n   \n  Your browser does not support the video tag.", 
            "title": "Exact"
        }, 
        {
            "location": "/ui/usage/#approximate_2", 
            "text": "By adding  duration  into the fields, the number of unique values for  (type, duration)  is increased. However, because  duration  has a tendency to have low values, we will have some  frequent items . The counts are now estimated. We ask for the top 300 results but we also say that they should have a count of at least 20. This restricts the overall number of results to 12.  \n   \n  Your browser does not support the video tag.   Maximum Count Error  The  maximum_count_error  value for the query above was  3 . This means that the difference between the upper bound and the lower bound of each count estimate is  3 . Bullet returns the upper bound as the estimate so subtracting  3  from each count gives you the lower bound of the count. Note that some counts are closer to each other than the count error. For instance,  (quux, 1)  and  (baz, 0)  have counts  67  and  66  but their true counts could be from  64 to 67  and  63 to 66  respectively. This means that  (baz, 0)  could well be the most frequent item for this query.", 
            "title": "Approximate"
        }, 
        {
            "location": "/ui/usage/#charting", 
            "text": "Bullet UI v0.3.0 and above  added support for charting and pivoting. This example shows how to get a basic chart on  Bullet UI v0.3.1 . If you are following the  Quick Start on Storm , then this should be in your UI. The charting and pivoting modes are only enabled for queries that are  not  Count Distinct or Group without Group Fields. This is because these results only have a single row and it does not make sense to graph them. They are enabled for all other queries.  The charting example below shows how to get a quick chart of a  Group  query with 3 metrics.  \n   \n  Your browser does not support the video tag.", 
            "title": "Charting"
        }, 
        {
            "location": "/ui/usage/#pivoting", 
            "text": "If the regular chart option is insufficient for your result (for instance, you have too many groups and metrics or you want to post-aggregate your results or remove outliers etc), then there is a advanced Pivot mode available when you are in the Chart option. The Pivot option provides a drag-and-drop interface to drag fields to breakdown and aggregate by their values. Operations such as finding standard deviations, variance, average, median, sum over sums etc are available as well as easily viewing them as tables and charts. The following example shows a  Group  query with multiple groups and metrics and some interactions with the Pivot table.   Raw data does not seem to have a regular chart mode option  This is deliberate since the Chart option tries to infer your independent and dependent columns. When you fetch raw data, this is prone to errors so only the Pivot option is allowed. You can always graph within the Pivot option if you need to.   \n   \n  Your browser does not support the video tag.", 
            "title": "Pivoting"
        }, 
        {
            "location": "/releases/", 
            "text": "Releases\n\n\nThis sections gathers all the relevant releases of the components of Bullet that we maintain in one place. It may not include the very few initial releases of these components if they were largely irrelevant. Full release notes can be found by clicking on the actual releases.\n\n\nBullet is still in active development. We welcome all contributions. Feel free to raise any issues/questions/bugs and whatever else on the relevant issues section for each component. Please include as many details as you can.\n\n\nDownload\n\n\nFor downloading any artifact listed below manually, you should preferably use the \nJCenter mirror here\n. For resolving artifacts in your build tool, follow the directions in each of the components' Package Manager Setup sections.\n\n\n\n\nBullet Core\n\n\nThe core Bullet logic (a library) that can be used to implement Bullet on different Stream Processors (like Flink, Storm, Kafka Streams etc.). This core library can also be reused in other Bullet components that wish to depend on core Bullet concepts. This actually lived inside the \nBullet Storm\n package prior to version \n0.5.0\n. Starting with 0.5.0, Bullet Storm only includes the logic to implement Bullet on Storm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-core\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-core/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2018-06-26\n\n\n0.4.2\n\n\nFixes a bug with unclosed connections in the RESTPubSub\n\n\n\n\n\n\n2018-06-22\n\n\n0.4.1\n\n\nAdded RESTPublisher HTTP Timeout Setting\n\n\n\n\n\n\n2018-06-18\n\n\n0.4.0\n\n\nAdded support for Integer and Float data types, and configurable BulletRecordProvider class used to instantiate BulletRecords in bullet-core\n\n\n\n\n\n\n2018-04-11\n\n\n0.3.4\n\n\nPre-Start delaying and Buffering changes - queries are now buffered at the start of a query instead of start of each window\n\n\n\n\n\n\n2018-03-30\n\n\n0.3.3\n\n\nBug fix for com.yahoo.bullet.core.querying.Querier#isClosedForPartition\n\n\n\n\n\n\n2018-03-20\n\n\n0.3.2\n\n\nAdded headers to RESTPubSub http requests\n\n\n\n\n\n\n2018-03-16\n\n\n0.3.1\n\n\nAdded RESTPubSub implementation\n\n\n\n\n\n\n2018-02-22\n\n\n0.3.0\n\n\nSupports windowing / incremental updates\n\n\n\n\n\n\n2017-10-04\n\n\n0.2.5\n\n\nSupports an in-memory BufferingSubscriber implementation for reliable subscribing\n\n\n\n\n\n\n2017-10-03\n\n\n0.2.4\n\n\nHelpers added to Config, PubSubMessage, Metadata and JSONFormatter. FAIL signal in Metadata. PubSubMessage is JSON serializable\n\n\n\n\n\n\n2017-09-20\n\n\n0.2.3\n\n\nPubSub is no longer required to be Serializable. Makes PubSubMessage fully serializable. Utility classes and checked exceptions for PubSub\n\n\n\n\n\n\n2017-08-30\n\n\n0.2.2\n\n\nHelper methods to PubSubMessage and Config\n\n\n\n\n\n\n2017-08-23\n\n\n0.2.1\n\n\nRemoves PubSubConfig, adds defaults methods to Publisher/Subscriber interfaces and improves PubSubException\n\n\n\n\n\n\n2017-08-16\n\n\n0.2.0\n\n\nPubSub interfaces and classes to implement custom communication between API and backend\n\n\n\n\n\n\n2017-06-27\n\n\n0.1.2\n\n\nChanges to the BulletConfig interface previously used in Bullet Storm. Users now use BulletStormConfig instead but YAML config is the same\n\n\n\n\n\n\n2017-06-27\n\n\n0.1.1\n\n\nFirst stable release containing the core of Bullet as a library including parsing, implementing queries, creating results, DataSketches etc\n\n\n\n\n\n\n\n\nBullet Storm\n\n\nThe implementation of Bullet on Storm. Due to major API changes between Storm \n= 0.10 and Storm 1.0, Bullet Storm \nbuilds two artifacts\n. The \nartifactId\n changes from \nbullet-storm\n (for 1.0+) to \nbullet-storm-0.10\n. All releases include migration and testing of the code on \nboth\n versions. Both versions are built simultaneously. Feature parity depends on what was new in Storm 1.0. For example, the Resource Aware Scheduler or RAS, is only present in Storm 1.0+. So, bullet-storm-0.10 removes certain CPU and memory related settings specific to RAS in its configuration. There are also minor changes to the Metrics API in Storm. In terms of Bullet itself, there should be no differences.\n\n\n\n\nStorm DRPC PubSub \n\n\nThe DRPC PubSub is part of this artifact and is fully released and available for use starting with versions 0.6.2 and above. It is only meant to be used if you're using Storm as your Backend.\n\n\n\n\n\n\nFuture support\n\n\nWe will support Storm 0.10 for a bit longer till Storm 2.0 is up and stable. Storm versions 1.0+ have a lot of performance fixes and features that you should be running with.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorm-1.0+ Repository\n\n\nhttps://github.com/bullet-db/bullet-storm\n\n\n\n\n\n\nStorm-0.10- Repository\n\n\nhttps://github.com/bullet-db/bullet-storm/tree/storm-0.10\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-storm/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nStorm 1.0\n\n\nStorm 0.10\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2018-06-18\n\n\n0.8.3\n\n\n0.8.3\n\n\nUsing new bullet-record and bullet-core supporting Integer and Float data types\n\n\n\n\n\n\n2018-04-12\n\n\n0.8.2\n\n\n0.8.2\n\n\nDelaying query start in Join Bolt\n\n\n\n\n\n\n2018-04-04\n\n\n0.8.1\n\n\n0.8.1\n\n\nFixed bug in JoinBolt\n\n\n\n\n\n\n2018-03-30\n\n\n0.8.0\n\n\n0.8.0\n\n\nSupports windowing / incremental updates\n\n\n\n\n\n\n2017-11-07\n\n\n0.7.0\n\n\n0.7.0\n\n\nMerge Query and Metadata Streams\n\n\n\n\n\n\n2017-10-24\n\n\n0.6.2\n\n\n0.6.2\n\n\nAdds a fat jar for using the DRPC PubSub in the Web Service\n\n\n\n\n\n\n2017-10-18\n\n\n0.6.1\n\n\n0.6.1\n\n\nDRPC PubSub\n\n\n\n\n\n\n2017-08-30\n\n\n0.6.0\n\n\n0.6.0\n\n\nNew PubSub architecture, removes DRPC components and settings\n\n\n\n\n\n\n2017-06-27\n\n\n0.5.0\n\n\n0.5.0\n\n\nPulled out Bullet Core. BulletConfig to BulletStormConfig\n\n\n\n\n\n\n2017-06-09\n\n\n0.4.3\n\n\n0.4.3\n\n\nAdding rounding for DISTRIBUTION. Latency metric\n\n\n\n\n\n\n2017-04-28\n\n\n0.4.2\n\n\n0.4.2\n\n\nStrict JSON output and fix for no data distributions\n\n\n\n\n\n\n2017-04-26\n\n\n0.4.1\n\n\n0.4.1\n\n\nResult Metadata Concept name mismatch fix\n\n\n\n\n\n\n2017-04-21\n\n\n0.4.0\n\n\n0.4.0\n\n\nDISTRIBUTION and TOP K release. Configuration renames.\n\n\n\n\n\n\n2017-03-13\n\n\n0.3.1\n\n\n0.3.1\n\n\nExtra records accepted after query expiry bug fix\n\n\n\n\n\n\n2017-02-27\n\n\n0.3.0\n\n\n0.3.0\n\n\nMetrics interface, config namespace, NPE bug fix\n\n\n\n\n\n\n2017-02-15\n\n\n0.2.1\n\n\n0.2.1\n\n\nAcking support, Max size and other bug fixes\n\n\n\n\n\n\n2017-01-26\n\n\n0.2.0\n\n\n0.2.0\n\n\nGROUP (DISTINCT, SUM, COUNT, MIN, MAX, AVG)\n\n\n\n\n\n\n2017-01-09\n\n\n0.1.0\n\n\n0.1.0\n\n\nCOUNT DISTINCT and micro-batching\n\n\n\n\n\n\n\n\nBullet Spark\n\n\nThe implementation of Bullet on Spark Streaming.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-spark\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-spark/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2018-06-18\n\n\n0.1.2\n\n\nUses SimpleBulletRecord to avoid some Spark serialization issues with Avro\n\n\n\n\n\n\n2018-06-08\n\n\n0.1.1\n\n\nAdds a command flag to pass custom setting file\n\n\n\n\n\n\n2018-05-25\n\n\n0.1.0\n\n\nThe first release\n\n\n\n\n\n\n\n\nBullet Web Service\n\n\nThe Web Service implementation that can serve a static schema from a file and talk to the backend using the PubSub.\n\n\n\n\nWAR to JAR\n\n\nStarting with 0.1.1 and above, this artifact no longer produces a WAR file that is meant to be run in a servlet container and instead switches to an executable Java application using Spring Boot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-service\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-service/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2018-07-17\n\n\n0.4.0\n\n\nEnhanced Web Service to support BQL queries\n\n\n\n\n\n\n2018-06-25\n\n\n0.3.0\n\n\nUpgrades to Netty-less Bullet Core for the RESTPubsub\n\n\n\n\n\n\n2018-06-14\n\n\n0.2.2\n\n\nAdding settings to configure Websocket\n\n\n\n\n\n\n2018-04-02\n\n\n0.2.1\n\n\nMoved and renamed settings\n\n\n\n\n\n\n2018-03-30\n\n\n0.2.0\n\n\nSupporting windowing / incremental updates\n\n\n\n\n\n\n2017-10-19\n\n\n0.1.1\n\n\nNew PubSub architecture. Switching to Spring Boot and executable JAR instead of WAR\n\n\n\n\n\n\n2016-12-16\n\n\n0.0.1\n\n\nThe first release with support for DRPC and the file-based schema\n\n\n\n\n\n\n\n\n\n\nWant to directly download jars?\n\n\nHead over to the JCenter download page to \ndirectly download all Bullet Storm, Core, Service, Record artifacts\n.\n\n\n\n\nBullet UI\n\n\nThe Bullet UI that lets you build, run, save and visualize results from Bullet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-ui\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-ui/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2018-07-20\n\n\n0.6.0\n\n\nSupports adding a full default starting query\n\n\n\n\n\n\n2018-06-18\n\n\n0.5.0\n\n\nSupports windowing, uses IndexedDB and Ember 3!\n\n\n\n\n\n\n2017-08-22\n\n\n0.4.0\n\n\nQuery sharing, collapsible Raw view, and unsaved/error indicators. Settings rename and other bug fixes\n\n\n\n\n\n\n2017-05-22\n\n\n0.3.2\n\n\nExporting to TSV in Pivot table. Fixes unselectability bug in Raw view\n\n\n\n\n\n\n2017-05-15\n\n\n0.3.1\n\n\nAdds styles to the Pivot table. Fixes some minor UI interactions\n\n\n\n\n\n\n2017-05-10\n\n\n0.3.0\n\n\nAdds Charting and Pivoting support. Migrations enhanced. Support for overriding nested default settings\n\n\n\n\n\n\n2017-05-03\n\n\n0.2.2\n\n\nFixes maxlength of the input for points\n\n\n\n\n\n\n2017-05-02\n\n\n0.2.1\n\n\nFixes a bug with a dependency that broke sorting the Filters\n\n\n\n\n\n\n2017-05-01\n\n\n0.2.0\n\n\nRelease for Top K and Distribution. Supports Bullet Storm 0.4.2+\n\n\n\n\n\n\n2017-02-21\n\n\n0.1.0\n\n\nThe first release with support for all features included in Bullet Storm 0.2.1+\n\n\n\n\n\n\n\n\nBullet Record\n\n\nThe AVRO container that you need to convert your data into to be consumed by Bullet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-record\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-record/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2018-06-14\n\n\n0.2.0\n\n\nMakes BulletRecord pluggable, adds simple record and avro record implementations\n\n\n\n\n\n\n2017-05-19\n\n\n0.1.2\n\n\nReduces the memory footprint needed to serialize itself by a factor of 128 for small records\n\n\n\n\n\n\n2017-04-17\n\n\n0.1.1\n\n\nHelper methods to remove, rename, check presence and count fields in the Record\n\n\n\n\n\n\n2017-02-09\n\n\n0.1.0\n\n\nMap constructor\n\n\n\n\n\n\n\n\nBullet Kafka\n\n\nA PubSub implementation using Kafka as the backing PubSub. Can be used with any Bullet Backend.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-kafka\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-kafka/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2018-02-27\n\n\n0.3.0\n\n\nUses bullet-core-0.3.0 - windows / incremental updates\n\n\n\n\n\n\n2017-10-19\n\n\n0.2.0\n\n\nRefactors and re-releases. Pass-through settings to Kafka. Manual offset committing bug fix\n\n\n\n\n\n\n2017-09-27\n\n\n0.1.2\n\n\nFixes a bug with config loading\n\n\n\n\n\n\n2017-09-22\n\n\n0.1.1\n\n\nFirst release using the PubSub interfaces\n\n\n\n\n\n\n\n\nBullet BQL\n\n\nA library facilitating the conversion from Bullet BQL queries to Bullet JSON queries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nhttps://github.com/bullet-db/bullet-bql\n\n\n\n\n\n\nIssues\n\n\nhttps://github.com/bullet-db/bullet-bql/issues\n\n\n\n\n\n\nLast Tag\n\n\n\n\n\n\n\n\nLatest Artifact\n\n\n\n\n\n\n\n\nPackage Manager Setup\n\n\nSetup for Maven, Gradle etc\n\n\n\n\n\n\n\n\nReleases\n\n\n\n\n\n\n\n\nDate\n\n\nRelease\n\n\nHighlights\n\n\n\n\n\n\n\n\n\n\n2018-07-17\n\n\n0.1.1\n\n\nStops publishing fat jar and marks slf4j dependency provided\n\n\n\n\n\n\n2018-07-05\n\n\n0.1.0\n\n\nFirst release", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#releases", 
            "text": "This sections gathers all the relevant releases of the components of Bullet that we maintain in one place. It may not include the very few initial releases of these components if they were largely irrelevant. Full release notes can be found by clicking on the actual releases.  Bullet is still in active development. We welcome all contributions. Feel free to raise any issues/questions/bugs and whatever else on the relevant issues section for each component. Please include as many details as you can.", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#download", 
            "text": "For downloading any artifact listed below manually, you should preferably use the  JCenter mirror here . For resolving artifacts in your build tool, follow the directions in each of the components' Package Manager Setup sections.", 
            "title": "Download"
        }, 
        {
            "location": "/releases/#bullet-core", 
            "text": "The core Bullet logic (a library) that can be used to implement Bullet on different Stream Processors (like Flink, Storm, Kafka Streams etc.). This core library can also be reused in other Bullet components that wish to depend on core Bullet concepts. This actually lived inside the  Bullet Storm  package prior to version  0.5.0 . Starting with 0.5.0, Bullet Storm only includes the logic to implement Bullet on Storm.           Repository  https://github.com/bullet-db/bullet-core    Issues  https://github.com/bullet-db/bullet-core/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Core"
        }, 
        {
            "location": "/releases/#releases_1", 
            "text": "Date  Release  Highlights      2018-06-26  0.4.2  Fixes a bug with unclosed connections in the RESTPubSub    2018-06-22  0.4.1  Added RESTPublisher HTTP Timeout Setting    2018-06-18  0.4.0  Added support for Integer and Float data types, and configurable BulletRecordProvider class used to instantiate BulletRecords in bullet-core    2018-04-11  0.3.4  Pre-Start delaying and Buffering changes - queries are now buffered at the start of a query instead of start of each window    2018-03-30  0.3.3  Bug fix for com.yahoo.bullet.core.querying.Querier#isClosedForPartition    2018-03-20  0.3.2  Added headers to RESTPubSub http requests    2018-03-16  0.3.1  Added RESTPubSub implementation    2018-02-22  0.3.0  Supports windowing / incremental updates    2017-10-04  0.2.5  Supports an in-memory BufferingSubscriber implementation for reliable subscribing    2017-10-03  0.2.4  Helpers added to Config, PubSubMessage, Metadata and JSONFormatter. FAIL signal in Metadata. PubSubMessage is JSON serializable    2017-09-20  0.2.3  PubSub is no longer required to be Serializable. Makes PubSubMessage fully serializable. Utility classes and checked exceptions for PubSub    2017-08-30  0.2.2  Helper methods to PubSubMessage and Config    2017-08-23  0.2.1  Removes PubSubConfig, adds defaults methods to Publisher/Subscriber interfaces and improves PubSubException    2017-08-16  0.2.0  PubSub interfaces and classes to implement custom communication between API and backend    2017-06-27  0.1.2  Changes to the BulletConfig interface previously used in Bullet Storm. Users now use BulletStormConfig instead but YAML config is the same    2017-06-27  0.1.1  First stable release containing the core of Bullet as a library including parsing, implementing queries, creating results, DataSketches etc", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-storm", 
            "text": "The implementation of Bullet on Storm. Due to major API changes between Storm  = 0.10 and Storm 1.0, Bullet Storm  builds two artifacts . The  artifactId  changes from  bullet-storm  (for 1.0+) to  bullet-storm-0.10 . All releases include migration and testing of the code on  both  versions. Both versions are built simultaneously. Feature parity depends on what was new in Storm 1.0. For example, the Resource Aware Scheduler or RAS, is only present in Storm 1.0+. So, bullet-storm-0.10 removes certain CPU and memory related settings specific to RAS in its configuration. There are also minor changes to the Metrics API in Storm. In terms of Bullet itself, there should be no differences.   Storm DRPC PubSub   The DRPC PubSub is part of this artifact and is fully released and available for use starting with versions 0.6.2 and above. It is only meant to be used if you're using Storm as your Backend.    Future support  We will support Storm 0.10 for a bit longer till Storm 2.0 is up and stable. Storm versions 1.0+ have a lot of performance fixes and features that you should be running with.            Storm-1.0+ Repository  https://github.com/bullet-db/bullet-storm    Storm-0.10- Repository  https://github.com/bullet-db/bullet-storm/tree/storm-0.10    Issues  https://github.com/bullet-db/bullet-storm/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Storm"
        }, 
        {
            "location": "/releases/#releases_2", 
            "text": "Date  Storm 1.0  Storm 0.10  Highlights      2018-06-18  0.8.3  0.8.3  Using new bullet-record and bullet-core supporting Integer and Float data types    2018-04-12  0.8.2  0.8.2  Delaying query start in Join Bolt    2018-04-04  0.8.1  0.8.1  Fixed bug in JoinBolt    2018-03-30  0.8.0  0.8.0  Supports windowing / incremental updates    2017-11-07  0.7.0  0.7.0  Merge Query and Metadata Streams    2017-10-24  0.6.2  0.6.2  Adds a fat jar for using the DRPC PubSub in the Web Service    2017-10-18  0.6.1  0.6.1  DRPC PubSub    2017-08-30  0.6.0  0.6.0  New PubSub architecture, removes DRPC components and settings    2017-06-27  0.5.0  0.5.0  Pulled out Bullet Core. BulletConfig to BulletStormConfig    2017-06-09  0.4.3  0.4.3  Adding rounding for DISTRIBUTION. Latency metric    2017-04-28  0.4.2  0.4.2  Strict JSON output and fix for no data distributions    2017-04-26  0.4.1  0.4.1  Result Metadata Concept name mismatch fix    2017-04-21  0.4.0  0.4.0  DISTRIBUTION and TOP K release. Configuration renames.    2017-03-13  0.3.1  0.3.1  Extra records accepted after query expiry bug fix    2017-02-27  0.3.0  0.3.0  Metrics interface, config namespace, NPE bug fix    2017-02-15  0.2.1  0.2.1  Acking support, Max size and other bug fixes    2017-01-26  0.2.0  0.2.0  GROUP (DISTINCT, SUM, COUNT, MIN, MAX, AVG)    2017-01-09  0.1.0  0.1.0  COUNT DISTINCT and micro-batching", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-spark", 
            "text": "The implementation of Bullet on Spark Streaming.           Repository  https://github.com/bullet-db/bullet-spark    Issues  https://github.com/bullet-db/bullet-spark/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Spark"
        }, 
        {
            "location": "/releases/#releases_3", 
            "text": "Date  Release  Highlights      2018-06-18  0.1.2  Uses SimpleBulletRecord to avoid some Spark serialization issues with Avro    2018-06-08  0.1.1  Adds a command flag to pass custom setting file    2018-05-25  0.1.0  The first release", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-web-service", 
            "text": "The Web Service implementation that can serve a static schema from a file and talk to the backend using the PubSub.   WAR to JAR  Starting with 0.1.1 and above, this artifact no longer produces a WAR file that is meant to be run in a servlet container and instead switches to an executable Java application using Spring Boot.            Repository  https://github.com/bullet-db/bullet-service    Issues  https://github.com/bullet-db/bullet-service/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Web Service"
        }, 
        {
            "location": "/releases/#releases_4", 
            "text": "Date  Release  Highlights      2018-07-17  0.4.0  Enhanced Web Service to support BQL queries    2018-06-25  0.3.0  Upgrades to Netty-less Bullet Core for the RESTPubsub    2018-06-14  0.2.2  Adding settings to configure Websocket    2018-04-02  0.2.1  Moved and renamed settings    2018-03-30  0.2.0  Supporting windowing / incremental updates    2017-10-19  0.1.1  New PubSub architecture. Switching to Spring Boot and executable JAR instead of WAR    2016-12-16  0.0.1  The first release with support for DRPC and the file-based schema      Want to directly download jars?  Head over to the JCenter download page to  directly download all Bullet Storm, Core, Service, Record artifacts .", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-ui", 
            "text": "The Bullet UI that lets you build, run, save and visualize results from Bullet.           Repository  https://github.com/bullet-db/bullet-ui    Issues  https://github.com/bullet-db/bullet-ui/issues    Last Tag     Latest Artifact", 
            "title": "Bullet UI"
        }, 
        {
            "location": "/releases/#releases_5", 
            "text": "Date  Release  Highlights      2018-07-20  0.6.0  Supports adding a full default starting query    2018-06-18  0.5.0  Supports windowing, uses IndexedDB and Ember 3!    2017-08-22  0.4.0  Query sharing, collapsible Raw view, and unsaved/error indicators. Settings rename and other bug fixes    2017-05-22  0.3.2  Exporting to TSV in Pivot table. Fixes unselectability bug in Raw view    2017-05-15  0.3.1  Adds styles to the Pivot table. Fixes some minor UI interactions    2017-05-10  0.3.0  Adds Charting and Pivoting support. Migrations enhanced. Support for overriding nested default settings    2017-05-03  0.2.2  Fixes maxlength of the input for points    2017-05-02  0.2.1  Fixes a bug with a dependency that broke sorting the Filters    2017-05-01  0.2.0  Release for Top K and Distribution. Supports Bullet Storm 0.4.2+    2017-02-21  0.1.0  The first release with support for all features included in Bullet Storm 0.2.1+", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-record", 
            "text": "The AVRO container that you need to convert your data into to be consumed by Bullet.           Repository  https://github.com/bullet-db/bullet-record    Issues  https://github.com/bullet-db/bullet-record/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Record"
        }, 
        {
            "location": "/releases/#releases_6", 
            "text": "Date  Release  Highlights      2018-06-14  0.2.0  Makes BulletRecord pluggable, adds simple record and avro record implementations    2017-05-19  0.1.2  Reduces the memory footprint needed to serialize itself by a factor of 128 for small records    2017-04-17  0.1.1  Helper methods to remove, rename, check presence and count fields in the Record    2017-02-09  0.1.0  Map constructor", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-kafka", 
            "text": "A PubSub implementation using Kafka as the backing PubSub. Can be used with any Bullet Backend.           Repository  https://github.com/bullet-db/bullet-kafka    Issues  https://github.com/bullet-db/bullet-kafka/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet Kafka"
        }, 
        {
            "location": "/releases/#releases_7", 
            "text": "Date  Release  Highlights      2018-02-27  0.3.0  Uses bullet-core-0.3.0 - windows / incremental updates    2017-10-19  0.2.0  Refactors and re-releases. Pass-through settings to Kafka. Manual offset committing bug fix    2017-09-27  0.1.2  Fixes a bug with config loading    2017-09-22  0.1.1  First release using the PubSub interfaces", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#bullet-bql", 
            "text": "A library facilitating the conversion from Bullet BQL queries to Bullet JSON queries           Repository  https://github.com/bullet-db/bullet-bql    Issues  https://github.com/bullet-db/bullet-bql/issues    Last Tag     Latest Artifact     Package Manager Setup  Setup for Maven, Gradle etc", 
            "title": "Bullet BQL"
        }, 
        {
            "location": "/releases/#releases_8", 
            "text": "Date  Release  Highlights      2018-07-17  0.1.1  Stops publishing fat jar and marks slf4j dependency provided    2018-07-05  0.1.0  First release", 
            "title": "Releases"
        }, 
        {
            "location": "/about/contributing/", 
            "text": "Contributing\n\n\nWe welcome all contributions! We also welcome all usage experiences, stories, annoyances and whatever else you want to say. Head on over to our \nContact Us page\n and let us know!\n\n\nContributor License Agreement (CLA)\n\n\nBullet is hosted under the \nBullet Github Organization\n, a subsidiary of the \nYahoo Github Organization\n. In order to contribute to any Yahoo project, you will need to submit a CLA. When you submit a Pull Request to any Bullet repository, a CLABot will ask  you to sign the CLA if you haven't signed one already. Read the \nhuman-readable summary\n of the CLA.\n\n\nFuture plans\n\n\nHere is a selected list of features we are currently considering/working on. Feel free to \ncontact us\n with any ideas/suggestions/PRs for features mentioned here or anything else you think about!\n\n\nThis list is neither comprehensive nor in any particular order.\n\n\n\n\n\n\n\n\nFeature\n\n\nComponents\n\n\nDescription\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\nBullet on X\n\n\nBE\n\n\nWith the pub/sub feature, Bullet can be implemented on other Stream Processors like Flink, Kafka Streaming, Samza etc\n\n\nOpen\n\n\n\n\n\n\nSQL API\n\n\nBE, WS\n\n\nWS supports an endpoint that converts a SQL-like query into Bullet queries\n\n\nIn Progress\n\n\n\n\n\n\nMore Windows\n\n\nBE\n\n\nWe have implemented a few of the windows we wanted to support initially but there are still more we can add\n\n\nOpen\n\n\n\n\n\n\nMore Aggregations\n\n\nBE, UI\n\n\nWe can add more aggregations like Group By Count Distinct etc\n\n\nOpen\n\n\n\n\n\n\nPost Aggregations\n\n\nBE, UI\n\n\nPost aggregations once the aggregations are done is useful\n\n\nOpen\n\n\n\n\n\n\nBullet on Beam\n\n\nBE\n\n\nBullet can be implemented on \nApache Beam\n as an alternative to implementing it on various Stream Processors\n\n\nOpen\n\n\n\n\n\n\nSecurity\n\n\nWS, UI\n\n\nThe obvious enterprise security for locking down access to the data and the instance of Bullet. Considering SSL, Kerberos, LDAP etc. Ideally, without a database\n\n\nPlanning\n\n\n\n\n\n\nPackaging\n\n\nUI, BE, WS\n\n\nGithub releases and building from source are the only two options for the UI. Docker images or the like for quick setup and to mix and match various pluggable components would be really useful\n\n\nOpen", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#contributing", 
            "text": "We welcome all contributions! We also welcome all usage experiences, stories, annoyances and whatever else you want to say. Head on over to our  Contact Us page  and let us know!", 
            "title": "Contributing"
        }, 
        {
            "location": "/about/contributing/#contributor-license-agreement-cla", 
            "text": "Bullet is hosted under the  Bullet Github Organization , a subsidiary of the  Yahoo Github Organization . In order to contribute to any Yahoo project, you will need to submit a CLA. When you submit a Pull Request to any Bullet repository, a CLABot will ask  you to sign the CLA if you haven't signed one already. Read the  human-readable summary  of the CLA.", 
            "title": "Contributor License Agreement (CLA)"
        }, 
        {
            "location": "/about/contributing/#future-plans", 
            "text": "Here is a selected list of features we are currently considering/working on. Feel free to  contact us  with any ideas/suggestions/PRs for features mentioned here or anything else you think about!  This list is neither comprehensive nor in any particular order.     Feature  Components  Description  Status      Bullet on X  BE  With the pub/sub feature, Bullet can be implemented on other Stream Processors like Flink, Kafka Streaming, Samza etc  Open    SQL API  BE, WS  WS supports an endpoint that converts a SQL-like query into Bullet queries  In Progress    More Windows  BE  We have implemented a few of the windows we wanted to support initially but there are still more we can add  Open    More Aggregations  BE, UI  We can add more aggregations like Group By Count Distinct etc  Open    Post Aggregations  BE, UI  Post aggregations once the aggregations are done is useful  Open    Bullet on Beam  BE  Bullet can be implemented on  Apache Beam  as an alternative to implementing it on various Stream Processors  Open    Security  WS, UI  The obvious enterprise security for locking down access to the data and the instance of Bullet. Considering SSL, Kerberos, LDAP etc. Ideally, without a database  Planning    Packaging  UI, BE, WS  Github releases and building from source are the only two options for the UI. Docker images or the like for quick setup and to mix and match various pluggable components would be really useful  Open", 
            "title": "Future plans"
        }, 
        {
            "location": "/about/contact/", 
            "text": "Contact Us\n\n\nIssues\n\n\nIf you have any issues with any of the particular Bullet sub-components, feel free to create issues.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStorm\n\n\nhttps://github.com/bullet-db/bullet-storm/issues\n\n\n\n\n\n\nSpark\n\n\nhttps://github.com/bullet-db/bullet-spark/issues\n\n\n\n\n\n\nWeb Service\n\n\nhttps://github.com/bullet-db/bullet-service/issues\n\n\n\n\n\n\nUI\n\n\nhttps://github.com/bullet-db/bullet-ui/issues\n\n\n\n\n\n\nRecord\n\n\nhttps://github.com/bullet-db/bullet-record/issues\n\n\n\n\n\n\nCore\n\n\nhttps://github.com/bullet-db/bullet-core/issues\n\n\n\n\n\n\nKafka PubSub\n\n\nhttps://github.com/bullet-db/bullet-kafka/issues\n\n\n\n\n\n\nDocumentation\n\n\nhttps://github.com/bullet-db/bullet-docs/issues\n\n\n\n\n\n\n\n\nMailing Lists\n\n\nIf you have a general question, comment, or observation meant for general visibility, reach out to the Users list. If you want to keep it to just the developers, reach to that list instead.\n\n\n\n\n\n\n\n\n\n\nMail\n\n\nForum\n\n\n\n\n\n\n\n\n\n\nUsers\n\n\nbullet-users@googlegroups.com\n\n\nhttps://groups.google.com/d/forum/bullet-users\n\n\n\n\n\n\nDevelopers\n\n\nbullet-dev@googlegroups.com\n\n\nhttps://groups.google.com/d/forum/bullet-dev", 
            "title": "Contact Us"
        }, 
        {
            "location": "/about/contact/#contact-us", 
            "text": "", 
            "title": "Contact Us"
        }, 
        {
            "location": "/about/contact/#issues", 
            "text": "If you have any issues with any of the particular Bullet sub-components, feel free to create issues.           Storm  https://github.com/bullet-db/bullet-storm/issues    Spark  https://github.com/bullet-db/bullet-spark/issues    Web Service  https://github.com/bullet-db/bullet-service/issues    UI  https://github.com/bullet-db/bullet-ui/issues    Record  https://github.com/bullet-db/bullet-record/issues    Core  https://github.com/bullet-db/bullet-core/issues    Kafka PubSub  https://github.com/bullet-db/bullet-kafka/issues    Documentation  https://github.com/bullet-db/bullet-docs/issues", 
            "title": "Issues"
        }, 
        {
            "location": "/about/contact/#mailing-lists", 
            "text": "If you have a general question, comment, or observation meant for general visibility, reach out to the Users list. If you want to keep it to just the developers, reach to that list instead.      Mail  Forum      Users  bullet-users@googlegroups.com  https://groups.google.com/d/forum/bullet-users    Developers  bullet-dev@googlegroups.com  https://groups.google.com/d/forum/bullet-dev", 
            "title": "Mailing Lists"
        }
    ]
}